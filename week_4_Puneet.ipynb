{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 10)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "A CNN trained on CIFAR-10 achieves 95 % training accuracy but only 65 % validation accuracy. The researcher tries the following remedies:\n",
        "\n",
        "1. Adding more convolutional layers\n",
        "2. Increasing kernel size in each convolutional layer\n",
        "3. Using dropout and batch normalization\n",
        "4. Performing extensive data augmentation\n",
        "\n",
        "### 1. Which of the following will most likely reduce overfitting?\n",
        "\n",
        "- [ ] A. 1 and 2\n",
        "- [ ] B. 1 and 3\n",
        "- [x] C. 3 and 4\n",
        "- [ ] D. 1 and 4\n",
        "\n",
        "Ans: Adding layers or larger kernels (1 and 2) may further overfit; dropout and batch norm regularize activations, while augmentation expands the dataset distribution, combating overfitting."
      ],
      "metadata": {
        "id": "FF2pCUklocMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 2. Consider a 2-D convolution with a 5×5 kernel, stride = 2, and padding = 1 on a 64×64×3 input. How many learnable parameters (including) exist in this single convolution layer with 32 filters?\n",
        "\n",
        "Ans: 2432\n",
        "\n",
        "Each filter has 5×5×3=75 weights + 1 bias = 76 parameters.\n",
        "Total = 32 × 76 = 2 432 learnable parameters.\n",
        "Stride and padding affect output size, not parameter count."
      ],
      "metadata": {
        "id": "4AuXUEP5o3vQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 3. Which of the following statements about Batch Normalization (BN) in CNNs are correct?\n",
        "\n",
        "- [x] A. BN mitigates internal covariate shift by stabilizing feature distribution.\n",
        "- [x] B. BN allows higher learning rates, accelerating convergence.\n",
        "- [ ] C. BN completely removes the need for dropout.\n",
        "- [x] D. BN normalizes activations across spatial dimensions and batch samples.\n",
        "\n",
        "Ans: BN standardizes intermediate activations and enables faster, more stable training.\n",
        "Dropout may still be needed for regularization; BN does not replace it entirely."
      ],
      "metadata": {
        "id": "MEHwPePgpWgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. In a CNN for CIFAR-10, early convolutional filters tend to capture edges and textures, while deeper filters respond to object parts. This hierarchy emerges primarily due to —\n",
        "\n",
        "- [ ] A. Weight sharing across spatial locations\n",
        "- [x] B. Local receptive fields expanding through layer stacking\n",
        "- [ ] C. ReLU introducing sparsity in activations\n",
        "- [ ] D. Pooling enforcing translation invariance\n",
        "\n",
        "Ans: As layers stack, each neuron’s receptive field covers a larger portion of the input, allowing progressively complex feature compositions—from edges → shapes → semantic parts."
      ],
      "metadata": {
        "id": "9_uOJAb0xGpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "A CNN has three convolutional layers, each with kernel 3, stride 1, and padding 1, followed by one max-pooling layer of size 2 × 2 and stride 2.\n",
        "\n",
        "### 5. What is the receptive field (in input pixels) of one neuron in the final feature map?\n",
        "\n",
        "- [ ] A. 5 x 5\n",
        "- [ ] B. 7 x 7\n",
        "- [x] C. 10 x 10\n",
        "- [ ] D. 11 x 11\n",
        "\n",
        "Ans: Receptive field after three 3×3 stride-1 layers: 3+2+2=7.\n",
        "Pooling doubles it: 7+(2−1)×2=10.\n",
        "Hence, each final-layer neuron “sees” 10×10 pixels of the input."
      ],
      "metadata": {
        "id": "BXFCgEZcxo6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. While training a CNN from scratch on MNIST, accuracy stagnates at 90 % despite sufficient data and no overfitting. Which is the most plausible underlying reason?\n",
        "\n",
        "- [x] A. Vanishing gradients due to sigmoid activations\n",
        "- [ ] B. Exploding gradients due to large initialization\n",
        "- [ ] C. Improper shuffling of mini-batches\n",
        "- [ ] D. Over-regularization by dropout\n",
        "\n",
        "Ans: Sigmoids saturate for large inputs, producing near-zero derivatives that halt weight updates.\n",
        "ReLU or Leaky ReLU alleviate this by maintaining gradient flow."
      ],
      "metadata": {
        "id": "RAlAJgSmyKKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "Consider two CNN architectures trained on identical data:\n",
        "1. Model A: no pooling, but stride 2 in convolutions\n",
        "2. Model B: uses pooling layers for down-sampling\n",
        "\n",
        "### 7. Which statements are true?\n",
        "\n",
        "- [x] A. Model A learns down-sampling through convolutional weights.\n",
        "- [x] B. Model B’s pooling provides parameter-free translation invariance.\n",
        "- [x] C. Model A may retain more task-specific spatial detail.\n",
        "- [ ] D. Model B will always outperform Model A in generalization.\n",
        "\n",
        "Ans: Strided convolutions subsample via learnable kernels (1), pooling adds invariance without parameters (2), and convolutional down-sampling can preserve fine spatial cues (3).\n",
        "Performance superiority (4) is not guaranteed; it depends on data and regularization."
      ],
      "metadata": {
        "id": "bFbwo-YByifL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 8. Which of the following is a key reason that deeper CNNs like ResNet outperform shallow ones on large-scale image tasks?\n",
        "\n",
        "- [ ] A. Deeper CNNs always have fewer parameters.\n",
        "- [ ] B. Residual connections mitigate vanishing gradients and preserve representational richness.\n",
        "- [ ] C. Batch normalization replaces all non-linearities.\n",
        "- [ ] D. Deeper CNNs avoid overfitting by having more layers.\n",
        "\n",
        "Ans: ResNets introduce skip (identity) connections that ease gradient flow and enable learning of residual mappings, solving degradation in very deep nets."
      ],
      "metadata": {
        "id": "EcrCQzNazBd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "An image of size 128×128 is processed by a sequence of layers:\n",
        "- Conv1: 3×3 kernel, stride 1, padding 1\n",
        "- MaxPool: 2×2, stride 2\n",
        "- Conv2: 3×3 kernel, stride 1, padding 1\n",
        "- MaxPool: 2×2, stride 2\n",
        "\n",
        "### 9. What is the final spatial size (H×W) of the feature map (ignore channels)? Give the value of H.\n",
        "\n",
        "Ans: 32\n",
        "\n",
        "Each pooling halves resolution: 128→64→32. Convs preserve it due to padding 1."
      ],
      "metadata": {
        "id": "eujGEXeAz_iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. During CNN training, you observe the validation accuracy stagnates while training accuracy continues to improve. Which of the following remedies are conceptually justified?\n",
        "\n",
        "- [x] A. Adding dropout layers between fully connected blocks.\n",
        "- [ ] B. Increasing the learning rate.\n",
        "- [x] C. Using data augmentation such as rotations and flips.\n",
        "- [x] D. Reducing the number of convolutional filters.\n",
        "\n",
        "Ans: Classic overfitting symptom. Regularization (dropout), augmentation, or reduced capacity can help.\n",
        "Raising the learning rate generally worsens convergence and may destabilize optimization."
      ],
      "metadata": {
        "id": "d3hz-a510i1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding questions (Total: 12)"
      ],
      "metadata": {
        "id": "Ce5Pqo_sMWCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=2)\n",
        "x = torch.randn(8, 3, 32, 32)\n",
        "y = conv(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 1. What will be the output shape of y?\n",
        "\n",
        "- [ ] A. (8, 6, 28, 28)\n",
        "- [ ] B. (8, 6, 30, 30)\n",
        "- [x] C. (8, 6, 32, 32)\n",
        "- [ ] D. (8, 6, 34, 34)\n",
        "\n",
        "Ans: Padding = 2 and stride = 1 keep spatial dimensions same:\n",
        "out=⌊(32+2×2−5)/1+1⌋=32.\n",
        "Hence (8, 6, 32, 32)."
      ],
      "metadata": {
        "id": "skKLLrX444LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "### 2. Which statements are true about the above network?\n",
        "\n",
        "- [x] A. After pooling, the spatial size becomes half in each dimension.\n",
        "- [x] B. The receptive field of each neuron in the output covers 5×5 pixels of input.\n",
        "- [x] C. Removing padding would shrink the output feature map even before pooling.\n",
        "- [ ] D. Pooling increases the number of parameters.\n",
        "\n",
        "Ans: Padding keeps intermediate sizes same; pooling halves them.\n",
        "Two stacked 3×3 kernels ⇒ receptive field = 5×5.\n",
        "Pooling has no learnable parameters.\n"
      ],
      "metadata": {
        "id": "2gW-SRJt5guN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.tensor([[[-1.0, 2.0], [3.0, -4.0]]])\n",
        "y = F.relu(x)\n",
        "print(y)\n",
        "```\n",
        "\n",
        "### 3. What will be printed as the output?\n",
        "\n",
        "- [x] A. `[[[0., 2.], [3., 0.]]]`\n",
        "- [ ] B. `[[[-1., 2.], [3., -4.]]]`\n",
        "- [ ] C. `[[[1., 2.], [3., 4.]]]`\n",
        "- [ ] D. Parsing Error\n",
        "\n",
        "Ans: ReLU sets negatives to 0; positives unchanged."
      ],
      "metadata": {
        "id": "o992kT3v6GU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 3, 3, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(3)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "### 4. During training, which are true about BatchNorm2d here?\n",
        "\n",
        "- [ ] A. Maintains running estimates of mean and variance per channel.\n",
        "- [ ] B. Has trainable γ (scale) and β (shift).\n",
        "- [ ] C. Normalizes across batch and spatial dimensions.\n",
        "- [ ] D. Has no effect during inference.\n",
        "\n",
        "Ans: BN learns per-channel affine params (γ, β) and uses batch stats while training; during inference, uses running means."
      ],
      "metadata": {
        "id": "fSyyRvlC6e6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "model = SmallCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "for epoch in range(10):\n",
        "    outputs = model(torch.randn(4, 3, 32, 32))\n",
        "    loss = criterion(outputs.view(4, -1), torch.randint(0, 10, (4,)))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "### 5. Which statements are correct?\n",
        "\n",
        "- [x] A. The .zero_grad() prevents gradient accumulation.\n",
        "- [x] B. .backward() computes gradients using automatic differentiation.\n",
        "- [x] C. .step() updates parameters using gradients stored in each tensor.\n",
        "- [ ] D. Momentum = 0.9 reduces overfitting directly.\n",
        "\n",
        "Ans: Momentum = 0.9 reduces overfitting directly."
      ],
      "metadata": {
        "id": "7SlqQKAS6zu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torchvision.transforms as T\n",
        "transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(15),\n",
        "    T.ToTensor()\n",
        "])\n",
        "```\n",
        "\n",
        "### 6. Select true statements regarding this transform pipeline.\n",
        "\n",
        "- [x] A. Each image has 50 % chance to be flipped horizontally.\n",
        "- [ ] B. Rotation preserves label semantics for classification tasks like CIFAR-10.\n",
        "- [x] C. ToTensor() converts PIL Image → [0, 1] tensor.\n",
        "- [x] D. It can reduce overfitting by enriching training diversity.\n",
        "\n",
        "Ans: Rotation may distort label semantics for asymmetric objects (e.g., digits 6/9). Flipping + augmentation combats overfitting."
      ],
      "metadata": {
        "id": "rBh9Be5i7xpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "def receptive_field(kernel_sizes, strides):\n",
        "    rf, jump = 1, 1\n",
        "    for k, s in zip(kernel_sizes, strides):\n",
        "        rf = rf + (k - 1) * jump\n",
        "        jump *= s\n",
        "    return rf\n",
        "\n",
        "print(receptive_field([3,3,3], [1,1,2]))\n",
        "```\n",
        "\n",
        "### 7. What is the receptive field size of the final layer?\n",
        "\n",
        "Ans: 9\n",
        "\n",
        "After first two 3×3 stride-1 layers → rf = 5, jump = 1.\n",
        "Third layer (stride 2): rf = 5 + (3−1)*1 = 7 before jump update → jump = 2.\n",
        "Actually final rf = 7 + (3−1)*1 = 9 pixels."
      ],
      "metadata": {
        "id": "UzBNxViw8FLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=2, padding=2)\n",
        "        self.bn1   = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1   = nn.Linear(16 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "model = TinyCNN()\n",
        "inp = torch.randn(4, 3, 32, 32)\n",
        "out = model(inp)\n",
        "```\n",
        "\n",
        "### 8. Which of the following are true?\n",
        "\n",
        "- [x] A. The feature map size before flattening is 16×8×8.\n",
        "- [x] B. BatchNorm introduces extra learnable parameters (γ, β).\n",
        "- [ ] C. The receptive field of the output neuron covers the whole image.\n",
        "- [x] D. max_pool2d halves spatial dimensions.\n",
        "\n",
        "Ans: Stride 2 in conv1 → 16×16; pool 2×2 → 8×8. BN adds 2×C parameters. Receptive field < entire 32×32 yet."
      ],
      "metadata": {
        "id": "MOtcyYdI8bvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.conv2(F.relu(self.conv1(x))) + x)\n",
        "```\n",
        "\n",
        "### 9. For this residual block:\n",
        "\n",
        "- [x] A. Shapes of input and output must match.\n",
        "- [x] B. Skip connection helps gradient flow.\n",
        "- [ ] C. Number of learnable parameters decreases compared to two independent conv layers.\n",
        "- [x] D. The block can model identity mapping easily.\n",
        "\n",
        "Ans: Residual adds input to output → same shape; skip improves training and permits identity."
      ],
      "metadata": {
        "id": "gGytLY5e9GAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class NormedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 6, 3)\n",
        "        self.bn   = nn.BatchNorm2d(6)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.training:\n",
        "            x = self.bn(x)\n",
        "        return F.relu(x)\n",
        "```\n",
        "\n",
        "### 10. Which statements hold true?\n",
        "\n",
        "- [x] A. During evaluation mode, BN uses running mean/var.\n",
        "- [x] B. self.training flag must be False for inference.\n",
        "- [x] C. BN normalizes over spatial + batch dims per channel.\n",
        "- [ ] D. Turning off BN has no effect on output distribution.\n",
        "\n",
        "Ans: model.eval() sets training=False; BN uses stored stats; normalization is per-channel."
      ],
      "metadata": {
        "id": "pO167rY59wHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class GlobalPoolNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.gap  = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc   = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "```\n",
        "\n",
        "### 11. Choose the correct option.\n",
        "\n",
        "- [x] A. Adaptive pooling outputs size (1×1) regardless of input dimension.\n",
        "- [ ] B. This architecture is translation invariant to object position.\n",
        "- [x] C. fc expects 64 inputs for each sample.\n",
        "- [x] D. Removing gap requires changing fc input size.\n",
        "\n",
        "Ans: GAP → 1×1 maps; fc needs 64 inputs; no strict translation invariance (so B false)."
      ],
      "metadata": {
        "id": "r6xXTXaM-FSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x): return self.layers(x)\n",
        "\n",
        "m = DeeperCNN()\n",
        "x = torch.randn(1,3,32,32)\n",
        "y = m(x)\n",
        "```\n",
        "\n",
        "### 12. Which statements are correct?\n",
        "\n",
        "- [x] A. Depth increases effective receptive field.\n",
        "- [x] B. All conv layers preserve spatial size.\n",
        "- [x] C. Gradient vanishing is more likely than in a 2-layer CNN.\n",
        "- [ ] D. Each layer’s activation map has 128 channels.\n",
        "\n",
        "Ans: Padding 1 keeps 32×32; RF grows with depth; final map has 128 channels only at end, not each."
      ],
      "metadata": {
        "id": "z9Nq210q-jQI"
      }
    }
  ]
}