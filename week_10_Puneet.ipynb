{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5ggp7DNnMOLn",
        "Ce5Pqo_sMWCd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 11)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. Consider a scenario, where you have a task at hand for a NN to understand the syntactic structure of sentences. The sentences consists of words that has shifting grammatical roles (\"can\" acts a noun and sometimes as a verb). Which of the following is important for dynamic contextual recognition?\n",
        "\n",
        "- [ ] A. Static word embeddings\n",
        "- [ ] B. Sequential RNN with limited context window\n",
        "- [x] C. Certain process that takes in account for each token with reference to other tokens in parallel.\n",
        "- [ ] D. Feature-level dropouts during the pre-training phase\n",
        "\n",
        "Ans: To capture context-sensitive roles, the model must allow each token to flexibly reference any other token in the input, not just neighboring or static meanings. Static embeddings and limited recurrence miss global context; parallel referencing enables dynamic role resolution."
      ],
      "metadata": {
        "id": "H7vJlfdT3j3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. Consider a model trained for next word prediction and performs the sentiment analysis task. Which strategies can be used to leverage the knowledge between these two tasks?\n",
        "\n",
        "- [x] A. Pre-training on massive, unorganized general text and then followed by fine-tuning on sentiment data.\n",
        "- [ ] B. Training two seperate models for prediction and classification respectively.\n",
        "- [x] C. Sharing the same encoder architecture for both tasks.\n",
        "- [x] D. Apply MLM (masked language modeling) during the pre-training phase.\n",
        "\n",
        "Ans: Pre-training provides foundational representations transferable across tasks; sharing the encoder lets both tasks use the same learned features; masked language modeling builds deep language understanding. Training isolated models doesn’t share knowledge."
      ],
      "metadata": {
        "id": "cRD2rsUM4TOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 3. What is the major limitation that arises by stacking several layers in deep neural architectures for natural language processing?\n",
        "\n",
        "- [ ] A. The model loses its ability to generalize across different tasks.\n",
        "- [x] B. There is a high possibility that the original input signal gets lost without performing any architectural considerations.\n",
        "- [ ] C. The capacity of the model for long-term dependencies increases.\n",
        "- [ ] D. Language representation becomes totally permutation invariant.\n",
        "\n",
        "Ans: As depth increases, repeated nonlinear transformations can cause the original signal to vanish or explode, especially without architectural remedies like residual connections, leading to loss of meaningful features and making learning difficult."
      ],
      "metadata": {
        "id": "XcKx3PMv4-HW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. What is the major point of difference for layer normalization with respect to batch normalization in deep sequence models?\n",
        "\n",
        "- [ ] A. It normalizes activations across batch dimensions, that leads to increase in the training speed.\n",
        "- [ ] B. It normalizes only across sequence positions, thus, ignoring the feature dimensions.\n",
        "- [x] C. It computes the statistics independently for each example using the current feature activations.\n",
        "- [ ] D. It uses dropout method instead of scaling and shifting operations.\n",
        "\n",
        "Ans: Layer normalization uses the hidden features of each example (not over a batch) to compute statistics, making it suitable when batch statistics might not be meaningful, such as in sequence models."
      ],
      "metadata": {
        "id": "N0sx5MqQ5os2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. How can we improve the performance of transfer learning approach for a language model that is fine-tuned for a domain-specific dataset?\n",
        "\n",
        "- [x] A. Curated and filtered task-specific data\n",
        "- [ ] B. Training from scratch for each domain\n",
        "- [x] C. Initial pre-training on a massive but diverse, raw text\n",
        "- [x] D. Fine-tune on task-specific objective functions\n",
        "\n",
        "Ans: Transfer learning works best when models start with broad pre-training, are then fine-tuned with relevant, high-quality data and objectives. Training from scratch loses all transfer benefits."
      ],
      "metadata": {
        "id": "bEZRNSpL6blD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. In which of the following scenarios, the vanishing gradient problem may primarily arise?\n",
        "\n",
        "- [ ] A. Shallow models with single-layer architecture\n",
        "- [x] B. When gradients becomes 0 quickly as they flow back through several layers.\n",
        "- [ ] C. While using models with complete parallelism and no recurrent nature.\n",
        "- [ ] D. When applying layer norm at every sub-layer\n",
        "\n",
        "Ans: As gradients propagate backward through deep stacks, their magnitude can shrink exponentially—especially with activation functions like sigmoid/tanh—causing early layers to stop updating, thus stalling learning."
      ],
      "metadata": {
        "id": "WFPDfFaU67bS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 7. Which of the following architectural changes led to massive scale in data and parameters in the modern large language models?\n",
        "\n",
        "- [x] A. Causal self-attention with unidirectional masking\n",
        "- [x] B. Removing the recurrence in the computation graph\n",
        "- [ ] C. Exclusive use of static word level embeddings  \n",
        "- [x] D. Appropriate utilization of matrix operations for parallel sequence processing\n",
        "\n",
        "Ans: Scaling up required moving past recurrence to direct, parallelizable architectures—like self-attention and massive matrix ops—and leveraging causality for generation. Static word-only embeddings limit flexibility, especially across languages and domains."
      ],
      "metadata": {
        "id": "vAVXhliuE_sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 8. In models like BERT, how the bidirectional context help -\n",
        "\n",
        "- [ ] A. Predicting the next word in autoregressive manner.\n",
        "- [x] B. Understanding the tokens by attending to both previous and future words\n",
        "- [ ] C. Predicting masked tokens at the end of the sequence\n",
        "- [ ] D. Enforcing causality during output generation\n",
        "\n",
        "Ans: Bidirectional models use both left and right context during training, allowing nuanced understanding that isn’t possible with unidirectional, causal models."
      ],
      "metadata": {
        "id": "WxdyP699FfHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. Which of the following key observation led to the current \"LLM revolution\" in NLP?\n",
        "\n",
        "- [ ] A. Increasing the depth always makes the model perform worse during generation.\n",
        "- [x] B. Performance increases predictably by increasing more parameters, data, and compute resources.\n",
        "- [ ] C. Small models and large models are comparable with appropriate number of epochs it is trained on.\n",
        "- [ ] D. Pre-training objective must always remain as next-word prediction.\n",
        "\n",
        "Ans: As models are scaled up and trained on more data, performance keeps improving in often unexpected, nonlinear ways. This scaling law underlies why LLMs have become so dominant."
      ],
      "metadata": {
        "id": "R_EU4n9sF2Oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Which of the following architecture principles are essential in AI models which support multi-modal (text, vision, and audio together) outputs?\n",
        "\n",
        "- [x] A. Flexible embeddings for every modality.\n",
        "- [x] B. Parallel computation over sequence positions.\n",
        "- [x] C. Use of modality-specific tokenization strategy.\n",
        "- [ ] D. Recurrence-based computation to presever the order\n",
        "\n",
        "Ans: Handling multiple modalities requires flexible, modular embeddings, often modality-specific tokenization, and parallel computations to synthesize information efficiently. Recurrence-only architectures are too limiting for large-scale fusion."
      ],
      "metadata": {
        "id": "WhVm8_uqGYW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. Why most of the deep models have skip connections in every layer?\n",
        "\n",
        "- [ ] A. To enforce layer-wise dropout for regularization.\n",
        "- [x] B. To maintain the original shape that distorts from repeated non-linear transformation.\n",
        "- [ ] C. To implement cross-attention between input and output embeddings.\n",
        "- [ ] D. To compute output probabilities without the hidden states' information.\n",
        "\n",
        "Ans: Skip connections (residuals) make it easier for gradients and signals to propagate, alleviating vanishing/exploding gradients and reducing distortion of the original data, especially as depth increases."
      ],
      "metadata": {
        "id": "G_YzYh7kG7fN"
      }
    }
  ]
}