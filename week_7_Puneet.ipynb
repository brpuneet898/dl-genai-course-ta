{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5ggp7DNnMOLn",
        "Ce5Pqo_sMWCd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. Consider a vanilla RNN processing variable-length sequences for next-token prediction in language modeling. Which of the following most accurately describes the relationship between the hidden state at a given time step and the probability estimation of the current token, especially regarding long-term dependencies and memory limitations?\n",
        "\n",
        "- [ ] A) The hidden state captures all previous context regardless of sequence length, ensuring perfect modeling of dependencies.\n",
        "- [x] B) The hidden state dynamically summarizes past tokens, but its effectiveness in modeling long-term dependencies diminishes rapidly with increased sequence length.\n",
        "- [ ] C) The hidden state is only influenced by the last token input, making it suitable for short-term dependencies.\n",
        "- [ ] D) The hidden state guarantees that gradient propagation remains stable across very long sequences.\n",
        "\n",
        "Ans: The hidden state summarizes prior context, but as sequence length increases, its ability to capture long-term dependencies weakens. This is due to vanishing gradients and limited memory, leading to loss of old information over long sequences, a key limitation of vanilla RNNs."
      ],
      "metadata": {
        "id": "UR35Bs2jqkw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 2. Someone applies an RNN to time-series data where each observation depends on all previous ones, but computational resources are limited. Which modeling strategy related to state dependencies and training method would most likely result in loss of critical information for prediction?\n",
        "\n",
        "- [x] A) Utilizing a windowed approach with fixed-length input sequences using only the most recent tokens\n",
        "- [ ] B) Encoding all historical observations in a compressed feature vector before input to the RNN\n",
        "- [ ] C) Using a vanilla RNN with random token sampling and non-overlapping sequence partitioning during training\n",
        "- [ ] D) Applying backpropagation through time across the full historical data sequence\n",
        "\n",
        "Ans: Using a fixed-length window (windowed approach) discards older context, making it impossible for the model to access dependencies beyond the window. This loss is critical in time series where earlier observations influence later ones."
      ],
      "metadata": {
        "id": "nYH4gTsRq8JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 3. Suppose you are tasked with improving an RNN language model’s ability to handle rare long-term dependencies without increasing the number of model parameters. Which approaches and considerations, based on the properties of vanilla RNNs, would best address this issue?\n",
        "\n",
        "- [x] A) Introducing gated mechanisms such as LSTM or GRU units\n",
        "- [ ] B) Increasing the number of hidden state neurons linearly with input sequence length\n",
        "- [x] C) Implementing truncated backpropagation through time with an optimally chosen window size\n",
        "- [x] D) Applying gradient clipping to stabilize parameter updates in long sequences\n",
        "\n",
        "Ans: Gated units help RNNs retain long-term dependencies; truncated BPTT can help learn over a manageable window; and gradient clipping can prevent instability—these strategies mitigate RNN weaknesses without adding parameters. Merely scaling hidden units (B) does not solve the core problem."
      ],
      "metadata": {
        "id": "OLvSRynmrNoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Someone wishes to quantify a language model's effectiveness using perplexity. Which property of perplexity, for token-level evaluation, best explains its utility as a metric when comparing models trained on the same corpus?\n",
        "\n",
        "- [ ] A) Perplexity directly measures the model’s vocabulary size\n",
        "- [x] B) Lower perplexity implies the model better predicts upcoming tokens, indicating stronger language modeling performance\n",
        "- [ ] C) High perplexity always corresponds to overfitting the training data\n",
        "- [ ] D) Perplexity cannot be used to compare models; it only reports randomness present in the token sequences\n",
        "\n",
        "Ans: Lower perplexity means the model assigns higher probability to real next tokens in the dataset. It reflects the model’s predictive accuracy in sequence modeling and is the standard for evaluating language models."
      ],
      "metadata": {
        "id": "6fdPdK0MrdDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. When utilizing RNNs for sequence modeling tasks in domains like DNA analysis or video activity recognition, what challenges arise due to the inherent structure of such data and the architecture of vanilla RNNs?\n",
        "\n",
        "- [x] A) Memory limitations for capturing dependencies across spatially separated sequence elements\n",
        "- [x] B) Potential for gradient vanishing or explosion during training of long sequences\n",
        "- [ ] C) Inability to process variable-length sequences without substantial preprocessing\n",
        "- [ ] D) Loss of key sequential information due to parameter sharing across time steps\n",
        "\n",
        "Ans: Vanilla RNNs struggle with long-range dependencies (memory limitations) and are prone to vanishing/exploding gradients during training. Parameter sharing and variable-length handling are strengths, not weaknesses of RNNs.​​\n",
        "\n"
      ],
      "metadata": {
        "id": "uFMA4mGhrwXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. You apply random sampling and sequential partitioning to create training/validation splits for an RNN language model on a large text corpus. What is the primary risk of using random sampling if token order is disrupted?\n",
        "\n",
        "- [ ] A) Overrepresentation of frequent tokens in each split\n",
        "- [x] B) Loss of meaningful sequential context, harming the model’s ability to capture dependencies\n",
        "- [ ] C) Increase in computational cost due to redundant training examples\n",
        "- [ ] D) Imbalance in class labels for prediction tasks\n",
        "\n",
        "Ans: Random sampling can disrupt the sequence order, causing the model to lose crucial context and dependencies, which harms learning for sequential tasks where order matters."
      ],
      "metadata": {
        "id": "XUlw4EXIr_ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. During backpropagation through time for an RNN trained on variable-length sequences, why might truncating the unrolled computational graph to a fixed number of steps be both beneficial and detrimental?\n",
        "\n",
        "- [x] A) It reduces computation time but risks forgetting dependencies beyond the truncation length\n",
        "- [ ] B) It prevents exploding gradients but always reduces model expressivity\n",
        "- [ ] C) It optimizes memory use and increases effective parameter sharing\n",
        "- [ ] D) It eliminates vanishing gradients for all sequences regardless of length\n",
        "\n",
        "Ans: Truncating BPTT makes training computationally feasible, but can cause the network to forget influences from steps beyond the truncation point, losing long-term dependencies."
      ],
      "metadata": {
        "id": "yjdsPzJtsUzm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 8. In the context of vanilla RNNs applied to stock market prediction, which technical design and data handling choices most strongly influence the model’s ability to generalize across unseen time periods?\n",
        "\n",
        "- [x] A) Lookback period selection and train-test split strategy\n",
        "- [ ] B) Choice of tokenization method for numerical financial data\n",
        "- [x] C) Activation function type used for hidden state computation\n",
        "- [ ] D) Use of one-hot encoding for input sequences\n",
        "\n",
        "Ans: Properly selecting the lookback period and thoughtful train/test splits make the model more generalizable. Activation function selection also impacts what patterns can be captured, especially nonlinearities important for stock prediction. One-hot encoding and tokenization are less critical vs. architectural/data handling choices.​\n",
        "\n"
      ],
      "metadata": {
        "id": "dyinkmeTslRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. Why are feedforward neural networks and CNNs generally not ideal for modeling sequential data such as language or time series without major architectural modifications?\n",
        "\n",
        "- [x] A) Lack of ability to share parameters across time steps and insufficient memory for retaining sequence context\n",
        "- [ ] B) Limited to processing only fixed-length feature vectors\n",
        "- [ ] C) Unable to apply nonlinear transformations to input data\n",
        "- [ ] D) Restricted to image processing tasks only\n",
        "\n",
        "Ans: Feedforward neural nets and standard CNNs cannot share parameters across time or remember sequence context, making them ill-suited for sequential data without major changes."
      ],
      "metadata": {
        "id": "_dc8leXPtEtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 10. If an RNN language model achieves a perplexity of 20 on a test set, what is the model’s implied level of uncertainty regarding the next token selection?\n",
        "\n",
        "Ans: 20\n",
        "\n",
        "A perplexity of 20 means the model's uncertainty is equivalent to randomly selecting among 20 tokens—lower is better for prediction certainty."
      ],
      "metadata": {
        "id": "IFwyArs9tf2Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. Which step comes first in preparing text data for input into an RNN-based language model according to standard practice?\n",
        "\n",
        "- [ ] A) Building the vocabulary dictionary\n",
        "- [x] B) Tokenizing the raw text into smaller sequence units\n",
        "- [ ] C) Converting each token to a unique integer index\n",
        "- [ ] D) Calculating the model’s perplexity\n",
        "\n",
        "Ans: Tokenization—the process of splitting raw text into words, characters, or other units—is always the first required step before vocabulary building in language modeling workflows."
      ],
      "metadata": {
        "id": "tEi4CVFgtqpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 12. What is the primary role of the hidden state in a vanilla RNN when modeling sequences?\n",
        "\n",
        "- [ ] A) It stores the network’s parameters for training\n",
        "- [x] B) It serves as a summary of all information processed so far in the sequence\n",
        "- [ ] C) It generates the output prediction for each time step independently\n",
        "- [ ] D) It maintains the model’s hyperparameter settings\n",
        "\n",
        "Ans: The hidden state in a vanilla RNN continuously encodes a summary of all information seen so far in the sequence, making it fundamental for sequence learning."
      ],
      "metadata": {
        "id": "-sO1-MZvt84i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Questions (Total: 12)"
      ],
      "metadata": {
        "id": "tZxXjvihqhkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TestRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(10, 8, 2, batch_first=True)\n",
        "        self.fc = nn.Linear(8, 1)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.randn(2, x.size(0), 8)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "x = torch.randn(16, 20, 10)\n",
        "model = TestRNN()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 1. What is the shape of `out` printed above?\n",
        "\n",
        "- [ ] A) (16, 20, 1)\n",
        "- [ ] B) (16, 8)\n",
        "- [x] C) (16, 1)\n",
        "- [ ] D) (16, 2)\n",
        "\n",
        "Ans: The final fully connected layer outputs one value per batch item, so the output shape is (batch_size, 1) = (16, 1)."
      ],
      "metadata": {
        "id": "Upx3naQLusEO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(4, 6, 1)\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(1), 6)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        return out, hn\n",
        "\n",
        "seq = torch.randn(7, 5, 4)\n",
        "model = MyRNN()\n",
        "out, hn = model(seq)\n",
        "print(out.shape, hn.shape)\n",
        "```\n",
        "\n",
        "### 2. What will be printed for `out.shape` and `hn.shape`?\n",
        "\n",
        "- [ ] A) torch.Size([3, 5, 4]) torch.Size([2, 5, 2])\n",
        "- [x] B) torch.Size([7, 5, 6]) torch.Size([1, 5, 6])\n",
        "- [ ] C) torch.Size([3, 5, 4]) torch.Size([1, 5, 6])\n",
        "- [ ] D) torch.Size([7, 5, 6]) torch.Size([2, 5, 2])\n",
        "\n",
        "Ans: PyTorch RNNs default to (seq_len, batch, input_size) input/output shapes; out has shape (7, 5, 6) and hn is (1, 5, 6)."
      ],
      "metadata": {
        "id": "UU8Wt6ybu7jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "rnn = nn.RNN(6, 5, 3, bidirectional=True)\n",
        "x = torch.randn(9, 2, 6)\n",
        "h0 = torch.zeros(3 * 2, 2, 5)\n",
        "out, hn = rnn(x, h0)\n",
        "```\n",
        "\n",
        "### 3. Which are true about out and hn?\n",
        "\n",
        "- [x] A) `out` has shape (9, 2, 10)\n",
        "- [x] B) `hn` has shape (6, 2, 5)\n",
        "- [x] C) The number 10 in output shape comes from doubling the hidden size due to bidirectionality\n",
        "- [ ] D) Initial hidden state must always be zeros\n",
        "\n",
        "Ans: Bidirectional RNNs double the number of features; so out is (seq_len, batch, num_directionshidden_size)=(9,2,10). hn has (num_layersnum_directions, batch, hidden_size) = (6,2,5)."
      ],
      "metadata": {
        "id": "Yk4Je7u8vrjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LoopRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(3, 4, 1, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        h = torch.zeros(1, x.size(0), 4)\n",
        "        for t in range(x.size(1)):\n",
        "            out, h = self.rnn(x[:, t:t+1, :], h)\n",
        "            outputs.append(out)\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "x = torch.randn(5, 8, 3)\n",
        "model = LoopRNN()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 4. What is the shape of out?\n",
        "\n",
        "- [ ] A) (5, 8, 3)\n",
        "- [x] B) (5, 8, 4)\n",
        "- [ ] C) (8, 5, 4)\n",
        "- [ ] D) (5, 1, 4)\n",
        "\n",
        "Ans: Iterates over time dimension, outputting the hidden state at each step; output is (batch, seq_len, hidden_size)."
      ],
      "metadata": {
        "id": "LQvEZuCnv9-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomRNNCell(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNNCell(4, 2)\n",
        "    def forward(self, x):\n",
        "        h = torch.zeros(x.size(0), 2)\n",
        "        out_seq = []\n",
        "        for i in range(x.size(1)):\n",
        "            h = self.rnn(x[:, i, :], h)\n",
        "            out_seq.append(h)\n",
        "        return torch.stack(out_seq, dim=1)\n",
        "\n",
        "x = torch.randn(10, 7, 4)\n",
        "model = CustomRNNCell()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 5. Which statements are correct about the above code snippet?\n",
        "\n",
        "- [x] A) Shape of output is (10, 7, 2)\n",
        "- [x] B) The model processes the input one time step at a time (for loop)\n",
        "- [x] C) Parameter sharing occurs across all time steps\n",
        "- [ ] D) This code supports variable-length sequences without modification\n",
        "\n",
        "Ans: Each time-step uses the same RNN cell (shared parameters); output is stacked for each sequence (batch, seq_len, hidden_size). The for-loop processes sequentially, but length must be known (so D is false)."
      ],
      "metadata": {
        "id": "pSWvYiaYwOpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn1 = nn.RNN(2, 6, 1, batch_first=True)\n",
        "        self.rnn2 = nn.RNN(6, 3, 1, batch_first=True)\n",
        "    def forward(self, x):\n",
        "        o1, _ = self.rnn1(x)\n",
        "        o2, _ = self.rnn2(o1)\n",
        "        return o2\n",
        "\n",
        "x = torch.randn(12, 10, 2)\n",
        "model = MultiRNN()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 6. What is the dimension of model output?\n",
        "\n",
        "- [ ] A) (12, 10, 2)\n",
        "- [ ] B) (12, 10, 6)\n",
        "- [x] C) (12, 10, 3)\n",
        "- [ ] D) (10, 12, 6)\n",
        "\n",
        "Ans: Final layer output shape is (batch, seq_len, hidden_size of last RNN)."
      ],
      "metadata": {
        "id": "q-ZkqyBzwg0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "x = torch.tensor([[1.,2.],[3.,4.]], requires_grad=True)\n",
        "y = x * 2\n",
        "z = y.sum()\n",
        "z.backward()\n",
        "```\n",
        "\n",
        "### 7. Which statements are true?\n",
        "\n",
        "- [x] A) The gradient of x will be all 2s\n",
        "- [x] B) x.grad will have shape (2,2)\n",
        "- [ ] C) z.backward() will throw an error since y is not a leaf tensor\n",
        "- [x] D) The computation graph supports backward calls on scalar outputs\n",
        "\n",
        "Ans: The gradient w.r.t x is 2 everywhere; x.grad matches x's shape; backward on scalar outputs works; y not being leaf is okay."
      ],
      "metadata": {
        "id": "UEqVfWfxwy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TinyRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(1, 1, 1, batch_first=True)\n",
        "    def forward(self, x, h):\n",
        "        out, h_n = self.rnn(x, h)\n",
        "        return out, h_n\n",
        "\n",
        "x = torch.ones(1, 3, 1)\n",
        "h = torch.zeros(1, 1, 1)\n",
        "model = TinyRNN()\n",
        "model.rnn.bias_ih_l0.data.fill_(0.0)\n",
        "model.rnn.bias_hh_l0.data.fill_(0.0)\n",
        "with torch.no_grad():\n",
        "    model.rnn.weight_ih_l0.data.fill_(1.0)\n",
        "    model.rnn.weight_hh_l0.data.fill_(0.0)\n",
        "out, h_n = model(x, h)\n",
        "print(out.squeeze().tolist())\n",
        "```\n",
        "\n",
        "### 8. What value is printed?\n",
        "\n",
        "- [x] A) [1.0, 1.0, 1.0]\n",
        "- [ ] B) [1.0, 2.0, 3.0]\n",
        "- [ ] C) [1.0, 1.0, -1.0]\n",
        "- [ ] D) [0.0, 0.0, 0.0]\n",
        "\n",
        "Ans: With all weights 1, hh=0, input is always 1 and previous hidden doesn't contribute—so output is always 1."
      ],
      "metadata": {
        "id": "meZPdZpLxBJy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "lstm = nn.LSTM(2, 4, batch_first=True)\n",
        "x = torch.randn(3, 5, 2)\n",
        "h0 = torch.zeros(1, 3, 4)\n",
        "c0 = torch.zeros(1, 3, 4)\n",
        "y, (hn, cn) = lstm(x, (h0, c0))\n",
        "print(y.shape, hn.shape, cn.shape)\n",
        "```\n",
        "\n",
        "### 9. What are the shapes of y, hn, cn?\n",
        "\n",
        "- [ ] A) (3, 5, 2), (1, 3, 4), (1, 3, 4)\n",
        "- [x] B) (3, 5, 4), (1, 3, 4), (1, 3, 4)\n",
        "- [ ] C) (5, 3, 4), (1, 3, 4), (1, 3, 4)\n",
        "- [ ] D) (3, 2, 4), (1, 5, 4), (1, 3, 4)\n",
        "\n",
        "Ans: y is (batch, seq_len, hidden_size); hn/cn are (num_layers, batch, hidden_size)."
      ],
      "metadata": {
        "id": "Rf7TqW14xTWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomEncoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(10, 3)\n",
        "        self.rnn = nn.RNN(3, 5, 1, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        seq_out, _ = self.rnn(emb)\n",
        "        return seq_out\n",
        "\n",
        "x = torch.tensor([[1,9,3],[2,2,6]])\n",
        "model = CustomEncoder()\n",
        "y = model(x)\n",
        "```\n",
        "\n",
        "### 10. Which are true?\n",
        "\n",
        "- [x] A) The embedding layer converts indices into 3D vectors\n",
        "- [x] B) Output shape of model is (2, 3, 5)\n",
        "- [ ] C) RNN input is of type torch.long\n",
        "- [x] D) Model would fail if x contained an index >= 10\n",
        "\n",
        "Ans: Embedding transforms long indices to float vectors; model output matches (batch, seq_len, hidden_size); index out-of-bounds triggers runtime error."
      ],
      "metadata": {
        "id": "WQNvZiggxhhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "rnn = nn.RNN(1, 2, 1)\n",
        "x = torch.randn(7, 3, 1)\n",
        "out, hn = rnn(x)\n",
        "print(hn.shape)\n",
        "```\n",
        "\n",
        "### 11. What is the shape of hn?\n",
        "\n",
        "- [x] A) (1, 3, 2)\n",
        "- [ ] B) (3, 2, 7)\n",
        "- [ ] C) (7, 3, 2)\n",
        "- [ ] D) (1, 2, 3)\n",
        "\n",
        "Ans: Shape is (num_layers, batch_size, hidden_size)."
      ],
      "metadata": {
        "id": "ijeu78aWxvc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "rnn = nn.RNN(5, 7, 1, batch_first=True)\n",
        "x = torch.randn(4, 13, 5)\n",
        "out, hn = rnn(x)\n",
        "print(out.shape[2])\n",
        "```\n",
        "\n",
        "### 12. What integer is printed?\n",
        "\n",
        "Ans: 7\n",
        "\n",
        "The third dimension of output corresponds to the hidden_size passed to RNN."
      ],
      "metadata": {
        "id": "5cjFmO8wx-Iq"
      }
    }
  ]
}