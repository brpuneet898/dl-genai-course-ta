{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Theory Questions (Total: 7)"],"metadata":{"id":"5ggp7DNnMOLn"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 1. We train a fully connected PyTorch network with 3 hidden layers of ReLU activations. During training, it is observed that:\n","- The first layer‚Äôs gradients are nearly zero.\n","- The later layers still update normally.\n","\n","Which of the following could be the reasons for this -\n","\n","- [x] A. Improper weight initialization (e.g., large negative values) causing dead ReLUs in early layers.\n","- [ ] B. Exploding gradients in deeper layers cancel out earlier layer updates.\n","- [ ] C. The non-linearity of ReLU creates a non-symmetric optimization landscape.\n","- [x] D. Backpropagated gradients shrink as they pass through multiple layers with zeros from ReLU.\n","\n","Ans:\n","\n","If early-layer pre-activations are pushed negative and you use ReLU, many units output 0, giving zero local derivative and near-zero upstream gradient ‚Äî dead/near-dead units.\n","Gradients are multiplied by gates (0/1 for ReLU) and weight matrices; many zeros in early layers shrink backpropagated signal."],"metadata":{"id":"0R1orU2GJsm3"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 2. Suppose we have a binary classification task using sigmoid output with BCELoss. It is observed that the model predicts values very close to 0 or 1 even in the first few epochs. What does this suggest?\n","\n","- [ ] A. The learning rate is too low.\n","- [x] B. The weights are initialized with too large magnitudes.\n","- [ ] C. The optimizer is not updating parameters.\n","- [ ] D. The loss function is unsuitable for sigmoid outputs.\n","\n","Very large initial weights push sigmoid into saturation. In saturation, gradients = 0, yet predictions look ‚Äúconfident‚Äù early. Low LR (A) causes tiny updates but doesn‚Äôt create saturated outputs. (C) would keep outputs near their random initial values. (D) is false ‚Äî BCE + sigmoid is standard."],"metadata":{"id":"s90DCltIKWry"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","In PyTorch, we have a following model -\n","\n","```\n","model = nn.Sequential(\n","    nn.Linear(100, 50),\n","    nn.ReLU(),\n","    nn.Linear(50, 1),\n","    nn.Sigmoid()\n",")\n","```\n","\n","### 3. Suppose input features are normalized and mean-centered. Which of the following would happen if you replaced ReLU with Sigmoid in the hidden layer?\n","\n","- [x] A. Risk of vanishing gradients increases.\n","- [x] B. Effective capacity of the network decreases.\n","- [x] C. The model may still fit training data but require significantly more epochs.\n","- [ ] D. Exploding gradients become more common.\n","\n","Ans: Hidden sigmoid reintroduces saturation, thus, vanishing gradients more likely. A network with saturating hidden units behaves more linearly around the operating point, thus, lower effective capacity. It can still fit, but needs more epochs due to slower gradient flow. Sigmoid does not make exploding gradients more common (it more like depends on either ReLU or bad initialization or LSTM gate issue)."],"metadata":{"id":"859Nz-oLKcZd"}},{"cell_type":"markdown","source":["[NAT]\n","\n","### 4. You train a 2-layer MLP with tanh activations on normalized data. During training, you observe that the gradients of the first layer‚Äôs weights are consistently around 10^-5. What is the most likely long-term effect on model convergence?\n","\n","Ans: Stagnation\n","\n","If first-layer gradients sit around this value, parameter updates are negligible. This will lead to stall of training process unless you change activation/init/LR or add normalization/residuals."],"metadata":{"id":"iiQDKC_OC3lW"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 5. Is this statement true or false, using torch.nn.Softmax with CrossEntropyLoss is redundant because CrossEntropyLoss already applies log-softmax internally.\n","\n","- [x] A. True, because PyTorch expects raw logits.\n","- [ ] B. False, because CrossEntropyLoss expects probabilities as input.\n","\n","Ans: CrossEntropyLoss expects raw logits and internally applies LogSoftmax + NLL. Passing already-softmaxed probabilities harms training (and can be numerically unstable).\n"],"metadata":{"id":"00HyWeqwDSmD"}},{"cell_type":"markdown","source":["[NAT]\n","\n","Suppose you have a 2-layer network:\n","\n","- Layer 1: Linear(2,2) with weights = identity matrix, no bias, activation = ReLU\n","- Layer 2: Linear(2,1) with weights = [1,1], bias = 0, activation = Sigmoid\n","\n","With,\n","\n","Input: ùë•=[1,‚àí2].\n","\n","### 6. What is the final predicted output (rounded to 3 decimals)?\n","\n","Ans: 0.731\n","\n","- Layer1: z1=x=[1,‚àí2], ReLU ‚Üí a1=[1,0].\n","- Layer2 pre-act: 1‚ãÖ1+1‚ãÖ0=1.\n","- Sigmoid(1) = 0.731"],"metadata":{"id":"qIHCfxaGvS1h"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 7. You experiment with a custom optimizer that skips gradient updates every alternate step. What issues could arise?\n","\n","- [X] A. Convergence slows down compared to normal optimizers.\n","- [X] B. Gradient information may decay before being used.\n","- [X] C. Loss curve oscillations may increase.\n","- [ ] D. The model is mathematically guaranteed to diverge.\n","\n","Skipping updates halves effective step frequency, leading to slower convergence, some gradient signals become stale, and the loss curve can oscillate. No guarantee of divergence."],"metadata":{"id":"H5jCpXJ5wolr"}},{"cell_type":"markdown","source":["# Coding questions (Total: 6)"],"metadata":{"id":"Ce5Pqo_sMWCd"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","```python\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","X = torch.randn(32, 10)\n","y = torch.randint(0, 2, (32,1)).float()\n","\n","model = nn.Sequential(\n","    nn.Linear(10, 16),\n","    nn.ReLU(),\n","    nn.Linear(16, 1),\n","    nn.Sigmoid()\n",")\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01)\n","\n","for epoch in range(5):\n","    for i in range(0, 32, 8):\n","        xb, yb = X[i:i+8], y[i:i+8]\n","        output = model(xb)\n","        loss = criterion(output, yb)\n","        loss.backward()\n","        optimizer.step()\n","```\n","\n","### 1. Which of the following are true?\n","\n","- [ ] A. The final parameter updates will be equivalent to full-batch training.  \n","- [x] B. Gradients will accumulate across mini-batches, making updates unstable.\n","- [x] C. The model may still train but with unpredictable dynamics.\n","- [x] D. Adding optimizer.zero_grad() fixes this training loop.\n","\n","Ans: Adding the term fixes this loop, as otherwise, gradient accumulation happens across the mini-batches which leads to unstable updates. The training might happen but dynamics are not predictable now."],"metadata":{"id":"SgGDJTSBMb54"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n","\n","lrs = []\n","for epoch in range(12):\n","    lrs.append(optimizer.param_groups[0]['lr'])\n","    scheduler.step()\n","\n","```\n","\n","### 2. What is the learning rate after epoch 10? (Round to 4 decimal places)\n","\n","Ans: 0.0025\n","\n","We start at 0.01, at every 5 steps it gets multiplied by 0.5.\n","- Epochs 0‚Äì4: 0.01 ‚Üí after e=4 step LR=0.005\n","- Epochs 5‚Äì9: 0.005 ‚Üí after e=9 step LR=0.0025\n","- Epoch 10 (logged before step): 0.0025."],"metadata":{"id":"Ol59Tf45Mb56"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","```python\n","model = nn.Sequential(\n","    nn.Linear(20, 3),\n","    nn.Softmax(dim=1)\n",")\n","criterion = nn.CrossEntropyLoss()\n","\n","X = torch.randn(4,20)\n","y = torch.tensor([0,1,2,1])\n","\n","output = model(X)\n","loss = criterion(output,y)\n","\n","```\n","\n","### 3. What is wrong with this code?\n","\n","- [x] A. CrossEntropyLoss expects raw logits, not softmax probabilities.\n","- [ ] B. The target y must be one-hot encoded for CrossEntropyLoss.\n","- [ ] C. The number of classes in y does not match the model output.\n","- [ ] D. Nothing is wrong, it should work fine.\n","\n","Ans: CrossEntropyLoss expects logits. Passing softmax probabilities loses the log-sum-exp stabilization and double-softmaxes the signal."],"metadata":{"id":"q2Xc6yQHMb57"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","torch.manual_seed(0)\n","\n","X = torch.tensor([[2., -3.]])\n","layer = nn.Linear(2,1)\n","with torch.no_grad():\n","    layer.weight[:] = torch.tensor([[1.,-2.]])\n","    layer.bias[:] = torch.tensor([0.5])\n","\n","output = layer(X)\n","print(output.item())\n","```\n","\n","### 4. What is the numerical output value printed?\n","\n","Ans: 8.5\n","\n","```text\n","w1*x1 + w2*x2 + b\n","2*1 + -3*-2 + 0.5\n","```"],"metadata":{"id":"yZvm-HSl2Ggs"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","```python\n","model = nn.Sequential(\n","    nn.Linear(10, 10),\n","    nn.BatchNorm1d(10),\n","    nn.ReLU(),\n","    nn.Linear(10,1)\n",")\n","\n","model.eval()\n","X = torch.randn(4,10)\n","print(model(X))\n","```\n","\n","### 5. What happens when the model is in .eval() mode here?\n","\n","- [x] A. BatchNorm uses running statistics instead of batch statistics.\n","- [ ] B. Gradients will not be computed for BatchNorm parameters.\n","- [ ] C. The output is identical to training mode in all cases.\n","- [x] D. Dropout, if present, would also behave differently in eval mode.\n","\n","\n","Ans: In .eval(), BatchNorm uses running mean/var, not batch stats. Gradients can still be computed if the .backward() function is called. Output differs from the train mode.  "],"metadata":{"id":"R8GOC-HV2ig1"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","model = nn.Sequential(\n","    nn.Linear(10, 20),\n","    nn.ReLU(),\n","    nn.Linear(20, 5),\n","    nn.ReLU(),\n","    nn.Linear(5, 1)\n",")\n","```\n","\n","### 6. How many trainable parameters (including bias) does this model have in total?\n","\n","Ans: 336\n","\n","- Layer 1 - 10*20 + 20\n","- Layer 2 - 20*5 + 5\n","- Layer 3 - 5*1 + 1"],"metadata":{"id":"xXG3hT74216F"}}]}