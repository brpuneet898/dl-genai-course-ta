{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 1. You train two models A and B on the same dataset. Model A uses an optimizer that reaches lower empirical risk faster but exhibits a sharp minimum around its final parameters. Model B converges more slowly to a slightly higher training loss but ends in a wider, flatter basin. Assume identical architectures, data, and regularization. Which statements are most consistent with an expert understanding of optimization strategies and generalization?\n",
        "\n",
        "- [ ] A. Model A is guaranteed to generalize better because its training loss is lower.\n",
        "- [x] B. The wider basin around Model B’s solution can correlate with better generalization due to robustness to small parameter perturbations.\n",
        "- [x] C. Minimizing empirical risk efficiently is not the same as minimizing true risk; optimization is a means to the generalization end.\n",
        "- [ ] D. If two solutions achieve similar validation loss, the solution with the sharper Hessian spectrum is typically preferred.\n",
        "\n",
        "Ans: Optimization targets *empirical risk*; deep learning cares about *true risk*. Flatter minima (wider basins, smaller leading Hessian eigenvalues) often correlate with better generalization, whereas sharp minima can be brittle. Lower training loss alone does not guarantee better generalization.\n"
      ],
      "metadata": {
        "id": "FF2pCUklocMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 2. Consider high-dimensional nonconvex loss landscapes typical in deep learning. At stationary points $ (∇f(x)=0) $, which is most likely to occur as dimensionality grows?\n",
        "\n",
        "- [ ] A. Local minima dominate because second derivatives vanish.\n",
        "- [x] B. Saddle points proliferate since mixed-sign Hessian eigenvalues are statistically more common than all-positive spectra.\n",
        "- [ ] C. Global minima dominate because of the concentration of measure.\n",
        "- [ ] D. Points with negative semidefinite Hessians dominate due to overparameterization.\n",
        "\n",
        "Ans: In high dimensions, random Hessian spectra are more likely to have mixed signs (saddles) than be strictly positive definite (local minima), making saddle points far more common and a practical obstacle for first-order methods.\n"
      ],
      "metadata": {
        "id": "3t67-qBjHzRA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 3. For a differentiable convex objective f over a convex domain, which statement is most accurate?\n",
        "\n",
        "- [ ] A. A global minimum must have strictly positive eigenvalues for the Hessian.\n",
        "- [x] B. Any local minimum is also a global minimum.\n",
        "- [ ] C. A local maximum is also a global maximum.\n",
        "- [ ] D. If $ ∇²f(x) $ has a single negative eigenvalue at a stationary point, it remains a global minimum by continuity.\n",
        "\n",
        "Ans: Convexity guarantees that any local minimum is global. Positive semidefiniteness (not strictly positive) of the Hessian is the differential test for convexity; any negative eigenvalue contradicts convexity.\n"
      ],
      "metadata": {
        "id": "TlGLGzIcICdj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Suppose f is differentiable and you take a small step $ xₜ₊₁ = xₜ − η ∇f(xₜ) $. Using a first-order Taylor approximation, what is the approximate change $ Δf = f(xₜ₊₁) − f(xₜ)) $ expressed in terms of $ η $ and the gradient norm at $ xₜ $? (Give the leading-order term; ignore higher-order terms.)\n",
        "\n",
        "- [ ] $ \\eta \\nabla f(x_t)^2 $\n",
        "- [x] $ -\\eta \\|\\nabla f(x_t)\\|^2 $\n",
        "- [ ] $ \\eta \\|\\nabla f(x_t)\\|^2 $\n",
        "- [ ] $ -\\eta \\nabla f(x_t)^2 $\n",
        "\n",
        "Ans: First-order expansion: $ f(x+\\epsilon)\\approx f(x)+\\epsilon^\\top \\nabla f(x) $. With $ \\epsilon=-\\eta \\nabla f(x) $, we get $ Δf \\approx -\\eta \\|\\nabla f(x)\\|^2 $.\n"
      ],
      "metadata": {
        "id": "XzUfXV6rIPvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. After optimizing a network with saturating nonlinearities and observe training stalls far from any known minima. Which interventions directly address vanishing gradients from an optimization perspective?\n",
        "\n",
        "- [x] A. Re-parameterize or initialize so pre-activations fall in non-saturating regions (e.g., careful init, normalization).\n",
        "- [x] B. Use learning-rate schedules or warmup to traverse flatter regions with controlled steps.\n",
        "- [ ] C. Increase model size to reduce training loss variance.\n",
        "- [x] D. Switch to non-saturating activations where appropriate to enlarge gradient-carrying regions.\n",
        "\n",
        "Ans: When derivatives approach zero (e.g., saturating tanh/sigmoid regimes), gradient signals vanish. Better initialization/normalization, suitable LR schedules, and non-saturating activations help maintain gradient flow and progress.\n"
      ],
      "metadata": {
        "id": "fPrP-41pJdoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 6. You must minimize a convex $ f(x) $ subject to convex inequality constraints $ c_i(x) ≤ 0 $. Which methods are mathematically standard approaches to respect constraints during optimization?\n",
        "\n",
        "- [x] A. Form a Lagrangian $ L(x,\\alpha)=f(x)+\\sum_i \\alpha_i c_i(x) $ with $ \\alpha_i\\ge 0 $ and solve the saddle-point problem.\n",
        "- [x] B. Add penalties $ \\sum_i \\alpha_i c_i(x) $ to the objective (carefully tuned) to discourage violations.\n",
        "- [x] C. After each unconstrained step, project back to the feasible set via $  \\mathrm{Proj}_X(x) $.\n",
        "- [ ] D. Replace f with any strongly concave surrogate; constraints then become irrelevant.\n",
        "\n",
        "Ans: Lagrangian duality, penalty methods, and projected updates are canonical strategies for convex constrained optimization; option D is incorrect.\n"
      ],
      "metadata": {
        "id": "IkTV10M_Jspb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. Let f be convex and X a random variable. Which statement is correct?\n",
        "\n",
        "- [ ] A. $ \\mathbb{E}[f(X)] \\le f(\\mathbb{E}[X]) $\n",
        "- [x] B. $ \\mathbb{E}[f(X)] \\ge f(\\mathbb{E}[X]) $\n",
        "- [ ] C. The inequality direction depends on the variance of X.\n",
        "- [ ] D. The inequality holds only if f is twice differentiable and strongly convex.\n",
        "\n",
        "Ans: Jensen’s inequality states that for convex f, the expectation of f(X) is at least f of the expectation. Differentiability or strong convexity is not required.\n"
      ],
      "metadata": {
        "id": "cv0W7VyJKGrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 8. There are multiple optimization methods like - constant LR, step decay, cosine decay, and warmup, and between vanilla SGD, momentum/Nesterov, and adaptive methods (e.g., RMSProp/Adam). Which selections best reflect deep optimization understanding for large-scale training?\n",
        "\n",
        "- [x] A. Warmup can stabilize early training when gradients are noisy or scales are poorly calibrated.\n",
        "- [x] B. Cosine or step decays can help escape plateaus and improve late-stage convergence compared to a fixed LR.\n",
        "- [x] C. Momentum/Nesterov can mitigate curvature-induced zig-zagging and accelerate along low-curvature valleys.\n",
        "- [ ] D. Adaptive methods strictly dominate SGD with momentum for final generalization across all tasks.\n",
        "\n",
        "Ans: Schedules shape training dynamics across regimes; momentum-like methods accelerate and smooth updates. No method dominates universally; adaptives sometimes underperform in final generalization compared to tuned SGD+m.\n"
      ],
      "metadata": {
        "id": "s3ytMZwcKVQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. Which statement about batch size and noise in updates is most accurate?\n",
        "\n",
        "- [x] A. Smaller batches introduce gradient noise that can act as an implicit regularizer and may help avoid sharp minima.\n",
        "- [ ] B. Full-batch GD is always better because it uses the exact gradient.\n",
        "- [ ] C. Mini-batch noise guarantees convergence to the global minimum in nonconvex problems.\n",
        "- [ ] D. Larger batches always generalize better due to higher signal-to-noise ratio.\n",
        "\n",
        "Ans: Stochasticity can help exploration and avoid sharp or poor basins; there is no universal guarantee of global optimality in nonconvex settings, and larger batches do not always generalize better.\n"
      ],
      "metadata": {
        "id": "je5eYs1nKxvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Training loss keeps decreasing; validation loss decreases initially but then rises and stays high. Which interventions are aligned with core optimization?\n",
        "\n",
        "- [x] A. Increase data augmentation and/or apply dropout/weight decay to narrow the generalization gap.\n",
        "- [ ] B. Increase the learning rate late in training to force convergence to a better basin.\n",
        "- [x] C. Use LR decay to refine solutions after initial progress, complementing regularization.\n",
        "- [ ] D. Remove all regularization to let the optimizer fully minimize training loss.\n",
        "\n",
        "Ans: The pattern indicates overfitting; regularization and appropriate LR schedules help generalization. Raising LR late or removing regularization typically worsens generalization.\n"
      ],
      "metadata": {
        "id": "WV5AFatdK7en"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. For gradient descent on a differentiable scalar objective f, what vector direction (in terms of $ ∇f(x) $) yields the steepest first-order decrease in f at x? (Give the unit vector direction.)\n",
        "\n",
        "- [ ] $ -\\frac{\\|\\nabla f(x)\\|}{\\nabla f(x)} $\n",
        "- [ ] $ \\frac{\\|\\nabla f(x)\\|}{\\nabla f(x)} $\n",
        "- [x] $ -\\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|} $\n",
        "- [ ] $ \\frac{\\nabla f(x)}{\\|\\nabla f(x)\\|} $\n",
        "\n",
        "Ans: The inner product $ \\epsilon^\\top \\nabla f(x) $ is minimized by moving opposite to the gradient with unit norm, giving steepest descent in first-order approximation.\n"
      ],
      "metadata": {
        "id": "EdSqcR3PLKkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 12. Which statement is most reasonable for guiding optimizer choice, based on curvature?\n",
        "\n",
        "- [x] A. If updates oscillate across narrow valleys, momentum/Nesterov or smaller LR can reduce zig-zagging due to anisotropic curvature.\n",
        "- [ ] B. If gradients are small and flat everywhere, increasing LR indefinitely is best.\n",
        "- [ ] C. If the landscape is flat, switching to saturating activations will improve gradients.\n",
        "- [ ] D. If the Hessian is indefinite, full-batch gradients remove saddle points.\n",
        "\n",
        "Ans: Zig-zagging indicates strong curvature anisotropy; momentum and careful LR help. Large LR in flat regions risks instability; saturating activations worsen gradients; full-batch does not remove saddles.\n"
      ],
      "metadata": {
        "id": "5iOvZ60dLtXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding questions (Total: 12)"
      ],
      "metadata": {
        "id": "Ce5Pqo_sMWCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, d_out):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_in, d_hidden, bias=True),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.LayerNorm(d_hidden),\n",
        "            nn.Linear(d_hidden, d_out, bias=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model1 = Block(32, 128, 10)\n",
        "model2 = Block(32, 128, 10)\n",
        "\n",
        "X = torch.randn(512, 32)\n",
        "y = torch.randint(0, 10, (512,))\n",
        "\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "opt_sgd = optim.SGD(model1.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4, nesterov=True)\n",
        "opt_adamw = optim.AdamW(model2.parameters(), lr=3e-3, betas=(0.9, 0.999), weight_decay=1e-2)\n",
        "\n",
        "def train_step(model, opt):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    logits = model(X)\n",
        "    loss = crit(logits, y)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "    opt.step()\n",
        "    return loss.item()\n",
        "\n",
        "losses_sgd, losses_adamw = [], []\n",
        "for t in range(20):\n",
        "    losses_sgd.append(train_step(model1, opt_sgd))\n",
        "    losses_adamw.append(train_step(model2, opt_adamw))\n",
        "\n",
        "print(f\"SGD last 3 losses: {losses_sgd[-3:]}\")\n",
        "print(f\"AdamW last 3 losses: {losses_adamw[-3:]}\")\n",
        "```\n",
        "\n",
        "### 1. Considering the code and standard optimizer behaviors, which statements are most accurate?\n",
        "\n",
        "- [x] A. AdamW’s weight decay is decoupled from the gradient of the loss, unlike classic L2 penalty implemented via `weight_decay` in SGD.\n",
        "- [x] B. With Nesterov momentum, the update uses the projected “lookahead” gradient, often reducing zig-zagging across narrow valleys.\n",
        "- [ ] C. For equal effective step sizes, SGD with momentum is guaranteed to achieve a lower training loss than AdamW after the same number of steps.\n",
        "- [x] D. Gradient clipping by global norm can mitigate occasional instability for both optimizers but does not replace a good learning-rate schedule.\n",
        "\n",
        "Ans: AdamW applies decay in a decoupled manner; Nesterov uses a lookahead gradient; clipping stabilizes but doesn’t replace LR scheduling. No general guarantee that SGD+m dominates AdamW in steps-to-loss."
      ],
      "metadata": {
        "id": "skKLLrX444LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "w = torch.tensor([[1.0, -2.0],[3.0, -4.0]], requires_grad=True)\n",
        "x = torch.tensor([[2.0, -1.0]], requires_grad=False)\n",
        "target = torch.tensor([[1.0, -3.0]], requires_grad=False)\n",
        "\n",
        "y = x @ w\n",
        "loss = ((y - target)**2).sum()\n",
        "loss.backward()\n",
        "\n",
        "eta = 0.05\n",
        "with torch.no_grad():\n",
        "    grad_copy = w.grad.clone()\n",
        "    w_new = w - eta * grad_copy\n",
        "\n",
        "first_order_delta = (-eta * (grad_copy * grad_copy)).sum().item()\n",
        "print(\"grad:\\n\", grad_copy)\n",
        "print(\"first_order_delta:\", first_order_delta)\n",
        "```\n",
        "\n",
        "### 2. If we interpret `first_order_delta` as a first-order Taylor approximation of Δloss due to the step, which statement is most correct?\n",
        "\n",
        "- [ ] A. It should be positive because we subtract a positive step times positive gradients.\n",
        "- [x] B. It should be non-positive, approximately −η∥∇ℓ∥^2, reflecting a local decrease to first order.\n",
        "- [ ] C. Its sign is undefined at first order; second-order terms dominate the sign.\n",
        "- [ ] D. It equals exactly the true change in loss regardless of step size.\n",
        "\n",
        "Ans: First-order approximation for a step −η∇ℓ is −η∥∇ℓ∥^2 (non-positive). Exact equality doesn’t hold except in special linear/quadratic cases with tiny steps.\n"
      ],
      "metadata": {
        "id": "2gW-SRJt5guN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "import math\n",
        "\n",
        "base_lr = 0.1\n",
        "min_lr  = 0.0\n",
        "warmup_steps = 5\n",
        "total_steps  = 25\n",
        "\n",
        "def lr_at(step):\n",
        "    if step < warmup_steps:\n",
        "        return base_lr * (step + 1) / warmup_steps\n",
        "    t = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "    return min_lr + 0.5 * (base_lr - min_lr) * (1 + math.cos(math.pi * t))\n",
        "\n",
        "lr_12 = lr_at(12)\n",
        "print(\"LR at step 12:\", lr_12)\n",
        "```\n",
        "\n",
        "### 3. What is the exact value of the learning rate at step = 12 printed by the code above? (write the decimal up to 3 places)\n",
        "\n",
        "Ans: 0.077. After warmup (steps 0–4), cosine decay applies. With step=12, t=(12-5)/(25-5)=7/20=0.35. LR =0.5∗0.1∗(1+cos(π∗0.35))≈0.076604."
      ],
      "metadata": {
        "id": "o992kT3v6GU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "class SmallNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(16, 64)\n",
        "        self.act = nn.GELU()\n",
        "        self.l2 = nn.Linear(64, 4)\n",
        "    def forward(self, x):\n",
        "        return self.l2(self.act(self.l1(x)))\n",
        "\n",
        "net = SmallNet()\n",
        "opt = optim.SGD(net.parameters(), lr=0.05, momentum=0.9)\n",
        "\n",
        "x = torch.randn(32, 16)\n",
        "y = torch.randint(0, 4, (32,))\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "opt.zero_grad(set_to_none=True)\n",
        "out = net(x)\n",
        "loss = loss_fn(out, y)\n",
        "loss.backward()\n",
        "\n",
        "gnorm = torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=1.0)\n",
        "\n",
        "for p in net.parameters():\n",
        "    if p.grad is not None:\n",
        "        p.grad.mul_(0.5)  \n",
        "\n",
        "opt.step()\n",
        "\n",
        "print(\"Global grad-norm observed before clamp (reported):\", float(gnorm))\n",
        "```\n",
        "\n",
        "### 4. Which statements about the above gradient handling are correct?\n",
        "\n",
        "- [x] A. `clip_grad_norm_` rescales all parameter gradients by a common factor if the global norm exceeds `max_norm`.\n",
        "- [x] B. Post-clipping rescaling (multiplying by 0.5) reduces the effective step further and can change the intended clipping magnitude.\n",
        "- [ ] C. `clip_grad_norm_` normalizes each parameter tensor independently to have unit norm.\n",
        "- [x] D. The reported value from `clip_grad_norm_` is the norm before clipping was applied.\n",
        "\n",
        "Ans: Global-norm clipping applies one scale factor; extra rescaling changes effective update; the API returns the pre-clipping global norm."
      ],
      "metadata": {
        "id": "fSyyRvlC6e6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "beta1, beta2, eps = 0.9, 0.999, 1e-8\n",
        "g1, g2 = 0.3, -0.1  \n",
        "m = 0.0\n",
        "v = 0.0\n",
        "\n",
        "# t = 1\n",
        "m = beta1*m + (1-beta1)*g1            # m1\n",
        "v = beta2*v + (1-beta2)*(g1*g1)       # v1\n",
        "\n",
        "m_hat1 = m/(1-beta1**1)\n",
        "v_hat1 = v/(1-beta2**1)\n",
        "\n",
        "# t = 2\n",
        "m = beta1*m + (1-beta1)*g2            # m2\n",
        "v = beta2*v + (1-beta2)*(g2*g2)       # v2\n",
        "\n",
        "m_hat2 = m/(1-beta1**2)\n",
        "v_hat2 = v/(1-beta2**2)\n",
        "\n",
        "print(\"m_hat1, v_hat1:\", m_hat1, v_hat1)\n",
        "print(\"m_hat2, v_hat2:\", m_hat2, v_hat2)\n",
        "```\n",
        "\n",
        "### 5. Which statement best captures Adam’s bias-correction in the code?\n",
        "\n",
        "- [ ] A. m_hat equals the raw exponential moving average m because correction cancels when beta1=0.9.\n",
        "- [x] B. m_hat and v_hat divide by `(1 - beta^t)` to correct initialization bias toward zero at early steps.\n",
        "- [ ] C. Bias correction increases the decay of m and v, making steps smaller than using raw m and v.\n",
        "- [ ] D. Bias correction only affects v but not m.\n",
        "\n",
        "Ans: Adam corrects the zero-initialization bias via division by 1−β^t for both first and second moments."
      ],
      "metadata": {
        "id": "7SlqQKAS6zu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "net = nn.Sequential(nn.Linear(64, 128), nn.ReLU(), nn.Linear(128, 2))\n",
        "X = torch.randn(256, 64)\n",
        "y = torch.randint(0, 2, (256,))\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "opt = optim.SGD(net.parameters(), lr=0.2, momentum=0.9, weight_decay=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n",
        "\n",
        "def train_epoch(use_explicit_l2=False, l2_lambda=0.0):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    logits = net(X)\n",
        "    base_loss = crit(logits, y)\n",
        "    loss = base_loss\n",
        "    if use_explicit_l2:\n",
        "        l2 = sum((p**2).sum() for p in net.parameters())\n",
        "        loss = loss + l2_lambda * l2\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "for epoch in range(10):\n",
        "    if epoch < 5:\n",
        "        train_epoch(use_explicit_l2=False)\n",
        "    else:\n",
        "        train_epoch(use_explicit_l2=True, l2_lambda=1e-4)\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"Final LR:\", scheduler.get_last_lr()[0])\n",
        "```\n",
        "\n",
        "### 7. Which statements best reflect the code’s regularization and scheduling?\n",
        "\n",
        "- [x] A. weight_decay in SGD applies L2-like shrinkage coupled to the gradient step; adding explicit l2_lambda * ||θ||² augments loss-based gradients in later epochs.\n",
        "- [x] B. Using both decay and explicit L2 increases total shrinkage pressure on parameters in the second phase.\n",
        "- [ ] C. StepLR modifies opt.param_groups before opt.step(), so the first five epochs already use a decayed LR.\n",
        "- [x] D. The learning rate halves at epochs 5 and 10 boundaries (given step_size=5), affecting step magnitudes post-update calls.\n",
        "\n",
        "Ans: Weight decay is coupled to the update; explicit L2 adds to loss gradient; StepLR.step() is called after opt.step() here, so LR changes take effect from the next epoch boundary."
      ],
      "metadata": {
        "id": "UzBNxViw8FLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([3.0, 4.0, 0.0])  \n",
        "g = torch.tensor([1.0, -2.0, 2.0])  \n",
        "eta = 1.0\n",
        "R = 4.0\n",
        "\n",
        "x_new = x - eta * g\n",
        "norm = torch.linalg.vector_norm(x_new)\n",
        "if norm > R:\n",
        "    x_proj = (R / norm) * x_new\n",
        "else:\n",
        "    x_proj = x_new\n",
        "\n",
        "print(\"x_new:\", x_new.tolist())\n",
        "print(\"x_proj:\", x_proj.tolist())\n",
        "```\n",
        "\n",
        "### 8. Which is the correct x_proj after the projection?\n",
        "\n",
        "- [ ] A. `[2.0,6.0,−2.0]`\n",
        "- [x] B. `(4/(44)^1/2)[2,6,−2]`\n",
        "- [ ] C. `(5/(44)^1/2)[2,6,−2]`\n",
        "- [ ] D. `[3.2,9.6,−3.2]`\n",
        "\n",
        "Ans: Since ∥xnew​∥=(44)^1/2 > 4, project by scaling to radius 4 along the same direction."
      ],
      "metadata": {
        "id": "MOtcyYdI8bvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "net = nn.Sequential(nn.Linear(32, 64), nn.ReLU(), nn.Linear(64, 2))\n",
        "crit = nn.CrossEntropyLoss()\n",
        "opt = optim.SGD(net.parameters(), lr=0.05)\n",
        "\n",
        "X = torch.randn(2048, 32)\n",
        "y = torch.randint(0, 2, (2048,))\n",
        "\n",
        "def step(batch_size):\n",
        "    perm = torch.randperm(X.size(0))\n",
        "    bidx = perm[:batch_size]\n",
        "    xb, yb = X[bidx], y[bidx]\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    logits = net(xb)\n",
        "    loss = crit(logits, yb)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    return float(loss)\n",
        "\n",
        "loss_16  = step(16)\n",
        "loss_256 = step(256)\n",
        "\n",
        "print(loss_16, loss_256)\n",
        "```\n",
        "\n",
        "### 9. Which choice most likely reduces stochastic gradient variance in this setup (all else equal)?\n",
        "\n",
        "- [ ] A. Use smaller batch sizes, as that always reduces variance.\n",
        "- [x] B. Use larger batch sizes, although it may affect generalization dynamics.\n",
        "- [ ] C. Keep batch size constant but double the learning rate.\n",
        "- [ ] D. Replace SGD with momentum while keeping batch size tiny; variance strictly vanishes.\n",
        "\n",
        "Ans: Larger batches reduce minibatch noise variance, though this can interact with generalization and may require LR retuning."
      ],
      "metadata": {
        "id": "gGytLY5e9GAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "net = nn.Sequential(nn.Linear(16, 32), nn.ReLU(), nn.Linear(32, 4))\n",
        "opt = optim.Adam(net.parameters(), lr=1e-3)\n",
        "crit = nn.CrossEntropyLoss()\n",
        "\n",
        "Xtr = torch.randn(256, 16); ytr = torch.randint(0, 4, (256,))\n",
        "Xva = torch.randn(128, 16); yva = torch.randint(0, 4, (128,))\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "patience, bad = 3, 0\n",
        "\n",
        "for epoch in range(20):\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "    logits = net(Xtr)\n",
        "    loss = crit(logits, ytr)\n",
        "    loss.backward(); opt.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        vloss = crit(net(Xva), yva).item()\n",
        "\n",
        "    if vloss < best_val - 1e-4:\n",
        "        best_val, bad = vloss, 0\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            break\n",
        "\n",
        "print(\"Stopped at epoch:\", epoch)\n",
        "print(\"Best val:\", best_val)\n",
        "```\n",
        "\n",
        "### 10. Which statements about the stopping logic are correct? (choose all that apply)\n",
        "\n",
        "- [x] A. Validation loss must improve by at least 1e-4 to reset patience; otherwise bad increases.\n",
        "- [x] B. Training continues while bad < patience; once bad reaches patience, loop breaks.\n",
        "- [ ] C. This guarantees stopping at a global minimum of validation loss.\n",
        "- [x] D. A smaller tolerance (like 1e-6) would make improvements harder to register, potentially increasing early stops.\n",
        "\n",
        "Ans: The tolerance gate controls what counts as improvement; patience counts non-improving epochs; no guarantee of global minima."
      ],
      "metadata": {
        "id": "pO167rY59wHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class F(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=1, padding=2)\n",
        "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x).relu()\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x).relu()\n",
        "        return x\n",
        "\n",
        "net = F()\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "z = net(x)\n",
        "flat_dim = z[0].numel()  \n",
        "print(\"Per-sample flattened dim:\", flat_dim)\n",
        "```\n",
        "\n",
        "### 11. What exact integer does `Per-sample flattened dim` print?\n",
        "\n",
        "Ans: 4096. conv1 keeps 32x32 → pool halves to 16x16 → conv2 keeps 16x16 with 16 channels ⇒ 16∗16∗16=4096."
      ],
      "metadata": {
        "id": "r6xXTXaM-FSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "# We compute deterministic scalar updates: w is 1D parameter.\n",
        "# SGD with momentum (classic):\n",
        "#   v_{t+1} = mu * v_t + g_t\n",
        "#   w_{t+1} = w_t - lr * v_{t+1}\n",
        "\n",
        "w0 = 1.0\n",
        "lr = 0.1\n",
        "mu = 0.9\n",
        "\n",
        "# Gradients at two consecutive steps (given deterministically)\n",
        "g1 = 2.0\n",
        "g2 = -1.0\n",
        "\n",
        "v0 = 0.0\n",
        "\n",
        "# Step 1\n",
        "v1 = mu * v0 + g1      \n",
        "w1 = w0 - lr * v1      \n",
        "\n",
        "# Step 2\n",
        "v2 = mu * v1 + g2      \n",
        "w2 = w1 - lr * v2      \n",
        "\n",
        "print(\"w2_exact:\", w2)\n",
        "```\n",
        "\n",
        "### 12. What exact numeric value is printed for w2_exact?\n",
        "\n",
        "Ans: 0.72. v0 = 0; w1 = 1 - 0.1*2 = 0.8, v2 = 0.9 * 2 - 1 = 0.8, w2 = 0.8 - 0.1 * 0.8 = 0.72"
      ],
      "metadata": {
        "id": "z9Nq210q-jQI"
      }
    }
  ]
}