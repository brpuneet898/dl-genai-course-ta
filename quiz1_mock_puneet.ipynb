{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5Xe9IfakXrPJVV7XgP6V2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Quiz 1 - Mock Test Questions"],"metadata":{"id":"5vYixuXAzJmS"}},{"cell_type":"markdown","source":["## Theory (10)"],"metadata":{"id":"dTnL6VBnzDMJ"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 1. A single-layer perceptron with a hard step activation is trained on a dataset that is not linearly separable. Which of the following is the most accurate statement about its representational limitation?\n","\n","- [ ] A. It will converge to a solution that minimizes mean squared error on the training set.\n","- [ ] B. It can represent any Boolean function if trained long enough.\n","- [x] C. It cannot represent the XOR function due to linear separability limits.\n","- [ ] D. It can represent XOR only if the learning rate is sufficiently small.\n","\n","Ans: A single-layer perceptron implements a linear decision boundary, which cannot represent XOR. Training time or learning rate does not change the hypothesis class."],"metadata":{"id":"9O6plBlPbnGq"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 2. Consider a 3-layer MLP with ReLU activations trained with cross-entropy loss. Which of the following choices increase the risk of vanishing/exploding gradients during training?\n","\n","- [x] A. Using very deep networks without skip connections.\n","- [x] B. Using sigmoid activations in hidden layers with poor weight initialization.\n","- [ ] C. Using ReLU activations with He (Kaiming) initialization.\n","- [ ] D. Using Batch Normalization between linear and activation layers.\n","\n","Ans: Depth + saturating activations + poor initialization can cause vanishing/exploding gradients. ReLU with He init and BatchNorm mitigate these issues."],"metadata":{"id":"wMVnUPpFb1fx"}},{"cell_type":"markdown","source":["[NAT]\n","\n","You train a classifier with softmax + cross-entropy. For a single sample, the model logits are [2,0,−1] for classes [0,1,2], and the true class is 0.\n","### 3. Compute the cross-entropy loss (natural log) to 4 decimal places.\n","\n","Ans: 0.1691"],"metadata":{"id":"4k_T94O7cIcI"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 4. Which statement best captures the role of an activation function in an MLP?\n","\n","- [ ] A. It guarantees convexity of the loss landscape.\n","- [x] B. It introduces nonlinearity, enabling composition of linear transforms to model complex functions.\n","- [ ] C. It primarily serves as a learning rate schedule.\n","- [ ] D. It regularizes the model by dropping activations.\n","\n","Ans: Nonlinear activations allow stacked linear layers to represent nonlinear mappings. They do not guarantee convexity or directly schedule learning rates."],"metadata":{"id":"v5rVoOwCcY4z"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 5. You apply L2 weight decay, Dropout(p=0.5), and early stopping. Which statements are true about generalization?\n","\n","- [x] A. L2 penalizes large weights, typically smoothing decision boundaries.\n","- [x] B. Dropout behaves like model averaging over sub-networks at train time.\n","- [x] C. Early stopping often selects a point with higher training loss but lower test loss than later epochs.\n","- [ ] D. Dropout increases the effective width at test time.\n","\n","Ans: L2 shrinks weights; Dropout approximates ensemble effects; early stopping is a practical regularizer. Test-time Dropout is off (with scaling), not “increased width.”"],"metadata":{"id":"dHvv2MZ9cocJ"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 6. Batch Normalization primarily helps by:\n","\n","- [ ] A. Eliminating the need for nonlinear activations.\n","- [x] B. Reducing internal covariate shift and stabilizing gradients across layers.\n","- [ ] C. Guaranteeing faster test-time inference.\n","- [ ] D. Replacing the optimizer’s momentum term.\n","\n","Ans: BN normalizes intermediate activations to stabilize training and gradient flow. It doesn’t remove nonlinearities or replace optimizer features."],"metadata":{"id":"p9Ahmh3cc2pl"}},{"cell_type":"markdown","source":["[NAT]\n","\n","### 7. You initialize a fully-connected layer with He normal initialization for ReLU: weights W ~ N(0, 2/fan_in). If fan_in = 50, what is the standard deviation (upto 2 decimal places).\n","\n","Ans: 0.20. Variance = 2/50 = 0.04."],"metadata":{"id":"dhtJAvVOdQm4"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 8. Comparing SGD with momentum vs Adam, which statements are correct in typical deep learning practice?\n","\n","- [x] A. Adam adapts learning rates per-parameter using first and second moment estimates.\n","- [ ] B. Momentum SGD cannot converge without weight decay.\n","- [x] C. Adam is often more robust to poorly scaled gradients at initialization.\n","- [ ] D. Adam always outperforms SGD on final generalization.\n","\n","Ans: Adam uses adaptive moments; momentum SGD can converge without weight decay; Adam can be robust early on; generalization superiority is task-dependent."],"metadata":{"id":"ouDlRtV4dsvP"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 9. For a binary classifier with logits $z$ and sigmoid outputs $ \\sigma(z) $, which loss formulation is numerically stable?\n","\n","- [ ] A. $-y \\log \\sigma(z) - (1 - y) \\log(1 - \\sigma(z))$ computed directly from $\\sigma(z)$.\n","- [x] B. $\\max(z, 0) - z \\cdot y + \\log(1 + e^{-|z|})$.\n","- [ ] C. Mean squared error between $y$ and $\\sigma(z)$.\n","- [ ] D. Hinge loss on $\\sigma(z)$.\n","\n","Ans: Using logits avoids catastrophic cancellation for extreme $z$. Direct sigmoid BCE can underflow/overflow."],"metadata":{"id":"KABTY_JReAXd"}},{"cell_type":"markdown","source":["[NAT]\n","\n","### 10. A two-layer MLP (no bias) is $f(x) = W_2 \\, \\text{ReLU}(W_1 x)$. With  \n","$W_1 = \\begin{bmatrix} 1 & -2 \\\\ -3 & 4 \\end{bmatrix}, \\; W_2 = [2 \\;\\; -1], \\; \\text{and } x = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}.$  \n","Compute $f(x)$.\n","\n","Ans: 0.\n","\n","$W_1 x = [1 \\cdot 2 + (-2) \\cdot 1,\\; -3 \\cdot 2 + 4 \\cdot 1] = [0,\\; -2].$ ReLU $\\rightarrow [0, 0].$ Then $W_2 [0, 0]^T = 0.$\n"],"metadata":{"id":"5RGgBKqpex_n"}},{"cell_type":"markdown","source":["## Coding (10)"],"metadata":{"id":"UIbKBZMhzGJt"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","import torch\n","torch.manual_seed(0)\n","\n","x = torch.tensor([[0.5, -1.0, 2.0]])\n","W = torch.randn(3, 2) * (2/3)**0.5  # He-like for ReLU (fan_in=3)\n","b = torch.zeros(2)\n","h = torch.relu(x @ W + b)\n","\n","V = torch.randn(2, 1) * (2/2)**0.5\n","y = h @ V\n","print(y.item())\n","```\n","\n","### 11. Report the scalar printed (to 4 decimal places).\n","\n","Ans: 0.3822. With fixed seed and shapes, the forward pass is deterministic; He-like scaling keeps activations reasonable. Can be verified by execution."],"metadata":{"id":"onDc0hCdzNDe"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","```python\n","import torch\n","torch.manual_seed(1)\n","logits = torch.tensor([[3.0, -1.0, 0.5]])\n","target = torch.tensor([0])\n","loss = torch.nn.functional.cross_entropy(logits, target)\n","```\n","\n","### 12. Which statement is correct?\n","\n","- [x] A. loss equals `-torch.log_softmax(logits, dim=1)[0,0]`.\n","- [ ] B. loss equals `-torch.log(torch.softmax(logits, dim=0)[0,0])`.\n","- [ ] C. loss is the same as MSE between logits and one-hot vector.\n","- [ ] D. loss depends on temperature scaling by default.\n","\n","Ans: F.cross_entropy = NLLLoss(log_softmax). Softmax must be across dim=1 (classes)."],"metadata":{"id":"mcV02N1MzgXZ"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","import torch\n","torch.manual_seed(2)\n","\n","W = torch.tensor([1.0], requires_grad=True)   # scalar weight\n","x = torch.tensor([2.0])\n","y = torch.tensor([5.0])\n","\n","# model: y_hat = W*x\n","y_hat = W * x\n","loss = ((y_hat - y)**2).mean()   \n","loss.backward()\n","\n","eta = 0.1\n","with torch.no_grad():\n","    W -= eta * W.grad\n","print(W.item())\n","```\n","\n","### 13. Compute the updated weight after one SGD step.\n","\n","Ans: 2.2.\n","\n","Loss gradient wrt W: 2(xW−y)x=2(2W−5)⋅2=8W−20. With W=1, grad =−12. Update W←1−0.1(−12)=2.2."],"metadata":{"id":"5ybAgpCSz0hP"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","```python\n","import torch\n","torch.manual_seed(3)\n","x = torch.randn(64, 100)  \n","lin1 = torch.nn.Linear(100, 100)\n","lin2 = torch.nn.Linear(100, 100)\n","\n","# Case A: Sigmoid activations\n","hA = torch.sigmoid(lin1(x))\n","hA = torch.sigmoid(lin2(hA)).mean().item()\n","\n","# Case B: ReLU activations\n","hB = torch.relu(lin1(x))\n","hB = torch.relu(lin2(hB)).mean().item()\n","```\n","\n","### 14. Which are likely true with default PyTorch init?\n","\n","- [x] A. Case A tends to have smaller mean activations due to saturation around 0–1.\n","- [x] B. Case B activations are sparser but maintain larger variance than Case A.\n","- [x] C. Case A gradients are at higher risk of vanishing.\n","- [ ] D. Case B will necessarily explode.\n","\n","Ans: Sigmoid compresses values; ReLU yields sparse positives, better gradient flow than sigmoid; explosion is not guaranteed."],"metadata":{"id":"XX2TGQPY0TWI"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","import torch\n","logits = torch.tensor([[1.5, 0.2]])\n","target = torch.tensor([1])\n","loss = torch.nn.functional.cross_entropy(logits, target)\n","print(loss.item())\n","```\n","\n","### 15. Fill the numeric value (to 4 decimals).\n","\n","Ans: 1.1166. Softmax over 2 classes with logits [1.5, 0.2]; CE uses logsumexp - z_y."],"metadata":{"id":"3Hh7-P0S0l8b"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","### 16. Suppose you optimize with `torch.optim.SGD(params, lr=0.1, weight_decay=1e-4)`. This weight_decay is equivalent to:\n","\n","- [ ] A. Adding $ λ||W||_1 $ to the loss and differentiating.\n","- [x] B. Adding $ λ||W||_2^2 $ to the loss and differentiating.\n","- [ ] C. Clipping weights by $ λ $ every step.\n","- [ ] D. Decaying gradients by $ λ $.\n","\n","Ans: weight_decay in SGD is classical L2 penalty."],"metadata":{"id":"OwEeLTKA0x4F"}},{"cell_type":"markdown","source":["[MSQ]\n","\n","### 17. With Adam (β₁=0.9, β₂=0.999, ε=1e-8), which are correct?\n","\n","- [x] A. It scales updates inversely with the square root of second moments.\n","- [x] B. Bias correction counteracts initialization of moments at zero.\n","- [x] C. Using too large a learning rate can still destabilize training.\n","- [ ] D. Adam guarantees the global optimum in nonconvex settings.\n","\n","Ans: Adam adapts per-parameter step sizes; bias correction is crucial early; no global optimality guarantee in deep nets."],"metadata":{"id":"JxG_TDM21myu"}},{"cell_type":"markdown","source":["[NAT]\n","\n","```python\n","import torch\n","torch.manual_seed(4)\n","x = torch.randn(1, 3)\n","W1 = torch.randn(3, 2, requires_grad=True)\n","W2 = torch.randn(2, 1, requires_grad=True)\n","\n","h = torch.relu(x @ W1)\n","y = h @ W2\n","loss = (y**2).mean()\n","loss.backward()\n","\n","print(W1.grad.norm().item())\n","```\n","\n","### 18. Give the value of printed scalar (to 4 decimals).\n","\n","Ans: 0.9486. Deterministic with fixed seed. Tests ReLU gates and squared loss propagate gradients."],"metadata":{"id":"cmJ_uH5Z165W"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","```python\n","import torch, torch.nn as nn\n","torch.manual_seed(5)\n","drop = nn.Dropout(p=0.5)\n","x = torch.ones(10, 4)\n","y_train = drop(x)        \n","drop.eval()\n","y_eval  = drop(x)       \n","```\n","\n","### 19. Which statement is correct?\n","\n","- [ ] A. y_train will be all zeros.\n","- [ ] B. y_eval equals x scaled by (1-p).\n","- [x] C. y_train keeps a random 50% of units and scales survivors by 1/(1-p).\n","- [ ] D. y_eval equals exactly x.\n","\n","Ans: In training, units are dropped with prob p and survivors are scaled by 1/(1-p) to keep expectation; in eval, Dropout is an identity mapping."],"metadata":{"id":"XUQk2skh2Laa"}},{"cell_type":"markdown","source":["[MCQ]\n","\n","```python\n","import torch, torch.nn as nn\n","torch.manual_seed(6)\n","bn = nn.BatchNorm1d(3, momentum=0.1, affine=True, track_running_stats=True)\n","\n","bn.train()\n","x1 = torch.randn(4, 3)\n","bn(x1)\n","\n","x2 = torch.randn(4, 3)\n","bn(x2)\n","\n","print(bn.running_mean)\n","```\n","\n","### 20. What will be the running mean printed as?\n","\n","- [ ] A. `Tensor([-0.6533, -0.1270, -0.1000])`\n","- [x] B. `Tensor([-0.0593, -0.1270, -0.0098])`\n","- [ ] C. `Tensor([-0.0593, -0.2000, -0.1008])`\n","- [ ] D. `Tensor([-0.0600, -0.1370, -0.0098])`\n","\n","Ans: BatchNorm updates running statistics with momentum."],"metadata":{"id":"9jYVeChN2k8W"}}]}