{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quiz 1 - Week 1 to 4 (Question Bank)"
      ],
      "metadata": {
        "id": "S68z7I2JHYRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Theory Questions - 61"
      ],
      "metadata": {
        "id": "KfFzgXHkmilv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following best describes an *artificial neuron* in the context of neural networks?  \n",
        "\n",
        "a) A function that maps discrete values to binary states  \n",
        "b) A computational unit that applies a weighted sum of inputs followed by an activation function  \n",
        "c) A mathematical model that only computes derivatives  \n",
        "d) A data structure used for storing neural parameters  \n",
        "\n",
        "**Answer:** b  "
      ],
      "metadata": {
        "id": "UIv2ZI93m9Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following are **roles of activation functions** in a neural network?  \n",
        "\n",
        "a) Introduce non-linearity into the model  \n",
        "b) Prevent weights from updating during training  \n",
        "c) Allow the network to approximate complex functions  \n",
        "d) Help gradient flow during backpropagation  \n",
        "\n",
        "**Answer:** a, c, d  "
      ],
      "metadata": {
        "id": "NFd9iE26m_mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given an artificial neuron with inputs $(x_1 = 2, x_2 = -1)$, weights $(w_1 = 0.5, w_2 = -0.25)$, and bias $(b = 1)$, compute the weighted sum $(z)$.  \n",
        "\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "\n",
        "a) 0.25  \n",
        "b) 1.75  \n",
        "c) 2.25  \n",
        "d) -0.75  \n",
        "\n",
        "**Answer:** c  "
      ],
      "metadata": {
        "id": "vU1Iuw3snFf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following is **NOT a commonly used activation function**?  \n",
        "\n",
        "a) Sigmoid  \n",
        "b) Tanh  \n",
        "c) ReLU  \n",
        "d) Euclidean  \n",
        "\n",
        "**Answer:** d  "
      ],
      "metadata": {
        "id": "4Fx3x4UwnTsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consider the **sigmoid activation function**:  \n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Which of the following statements are true?  \n",
        "\n",
        "a) Its output always lies between 0 and 1  \n",
        "b) It suffers from the vanishing gradient problem for large $(|z|)$  \n",
        "c) It is zero-centered  \n",
        "d) It is differentiable everywhere  \n",
        "\n",
        "**Answer:** a, b, d  "
      ],
      "metadata": {
        "id": "or_d1etXnVlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which loss function is most suitable for **binary classification** problems?  \n",
        "\n",
        "a) Mean Squared Error (MSE)  \n",
        "b) Cross-Entropy Loss  \n",
        "c) Hinge Loss  \n",
        "d) KL Divergence  \n",
        "\n",
        "**Answer:** b  "
      ],
      "metadata": {
        "id": "Vvi5vyqCnYuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why are **non-linear activation functions** essential in deep networks?  \n",
        "\n",
        "a) Without them, multiple layers collapse into a single linear transformation  \n",
        "b) They allow networks to approximate complex non-linear decision boundaries  \n",
        "c) They reduce computational cost compared to linear functions  \n",
        "d) They enable learning of high-dimensional representations  \n",
        "\n",
        "**Answer:** a, b, d  "
      ],
      "metadata": {
        "id": "EYbTLbTjnhSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following challenges can occur when training deep neural networks?  \n",
        "\n",
        "a) Vanishing or exploding gradients  \n",
        "b) Overfitting due to high model capacity  \n",
        "c) Difficulty in optimization due to non-convexity  \n",
        "d) Infinite solutions due to regularization  \n",
        "\n",
        "**Answer:** a, b, c  "
      ],
      "metadata": {
        "id": "AUNOXCzinkTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suppose a single-layer perceptron is trained on a linearly separable dataset using a step activation. Which of the following guarantees convergence?  \n",
        "\n",
        "a) Learning rate â†’ 0  \n",
        "b) Perceptron Convergence Theorem  \n",
        "c) Universal Approximation Theorem  \n",
        "d) Vanishing Gradient Theorem  \n",
        "\n",
        "**Answer:** b"
      ],
      "metadata": {
        "id": "Re6d_Pv_nsUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consider a neuron with input vector $(x = [x_1, x_2])$, weight vector $(w = [w_1, w_2])$, and bias $(b)$. Let the activation be sigmoid:  \n",
        "\n",
        "$$\n",
        "f(z) = \\frac{1}{1 + e^{-z}}, \\quad z = w_1x_1 + w_2x_2 + b\n",
        "$$\n",
        "\n",
        "Which of the following are correct about the **gradient of the output** with respect to $(w_1)$?  \n",
        "\n",
        "a) $(\\frac{\\partial f}{\\partial w_1} = f(z)(1-f(z))x_1)$  \n",
        "b) It always lies between -0.25 and 0.25 times $(x_1)$  \n",
        "c) Gradient vanishes for large $(|z|)$  \n",
        "d) It is independent of bias $(b)$  \n",
        "\n",
        "**Answer:** a, b, c  "
      ],
      "metadata": {
        "id": "wDcVFYVmnw_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following statements about the **Universal Approximation Theorem** is correct?  \n",
        "\n",
        "a) A feedforward neural network with a single hidden layer and finite neurons can approximate any continuous function on compact subsets of $(\\mathbb{R}^n)$.  \n",
        "b) It guarantees efficient training of neural networks.  \n",
        "c) It applies only to recurrent neural networks.  \n",
        "d) It requires infinite hidden layers.  \n",
        "\n",
        "**Answer:** a  "
      ],
      "metadata": {
        "id": "uJq_0uuBn0YW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You design a neural network without any activation functions (only linear transformations). Which statements are true?  \n",
        "\n",
        "a) The network reduces to a single linear transformation regardless of depth  \n",
        "b) Such a model cannot represent XOR function  \n",
        "c) Gradient descent will diverge in training  \n",
        "d) Adding more layers improves representation power  \n",
        "\n",
        "**Answer:** a, b  "
      ],
      "metadata": {
        "id": "Cnjw8TNdn3Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which activation function can output unbounded positive values, is sparse in nature, and widely used in deep networks?  \n",
        "\n",
        "a) Sigmoid  \n",
        "b) ReLU  \n",
        "c) Tanh  \n",
        "d) Softmax  \n",
        "\n",
        "**Answer:** b  "
      ],
      "metadata": {
        "id": "gx9R7WpwoAHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given ReLU activation function $(f(z) = \\max(0, z))$:  \n",
        "\n",
        "Which of the following are true?  \n",
        "\n",
        "a) Its derivative is 1 when $(z > 0)$ and 0 when $(z < 0)$  \n",
        "b) It avoids vanishing gradients in positive domain  \n",
        "c) It is differentiable everywhere including at $(z=0)$  \n",
        "d) It induces sparsity in activations  \n",
        "\n",
        "**Answer:** a, b, d  \n"
      ],
      "metadata": {
        "id": "YpIeXaUeoLg6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### You are training a deep network with sigmoid activations in all layers. Training is extremely slow due to vanishing gradients. Which strategy is **least effective**?  \n",
        "\n",
        "a) Replace sigmoid with ReLU or variants  \n",
        "b) Normalize inputs and use better weight initialization  \n",
        "c) Use residual connections or batch normalization  \n",
        "d) Increase the number of sigmoid layers further  \n",
        "\n",
        "**Answer:** d  "
      ],
      "metadata": {
        "id": "QVtWJR0Goeku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consider a neuron with inputs $(x = [1,2,3])$, weights $(w = [0.2, -0.5, 0.1])$, bias $(b = 0.4)$, and activation `tanh`. Compute the neuron output (approximate to 3 decimal places).  \n",
        "\n",
        "$$\n",
        "f(z) = \\tanh(z)\n",
        "$$\n",
        "\n",
        "**Answer:** -0.100  "
      ],
      "metadata": {
        "id": "bHP30KW4oh21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which of the following are reasons why loss metrics are essential in neural networks?  \n",
        "\n",
        "a) They quantify the mismatch between predictions and ground truth  \n",
        "b) They provide gradients for optimization  \n",
        "c) They guarantee avoidance of overfitting  \n",
        "d) They enable comparison between different models  \n",
        "\n",
        "**Answer:** a, b, d  "
      ],
      "metadata": {
        "id": "GfBKqcp7opfN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 1. You build a 3-layer fully connected neural network in PyTorch using `torch.nn.Sequential`, with ReLU activations in the hidden layers and Sigmoid activation in the output layer. Consider the following statements about training dynamics:\n",
        "\n",
        "- [ ] A. Gradient vanishing is impossible because ReLU completely avoids it.\n",
        "- [x] B. The Sigmoid at the output layer can cause gradients to vanish if the inputs are large in magnitude.\n",
        "- [ ] C. Using ReLU ensures both vanishing and exploding gradients are eliminated.\n",
        "- [x] D. Combining ReLU hidden layers with a Sigmoid output may still result in vanishing gradients in earlier layers."
      ],
      "metadata": {
        "id": "FYhr3PhCreC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. In PyTorch, if you define a `torch.nn.Linear` layer but forget to specify a non-linear activation afterwards:\n",
        "\n",
        "- [ ] A. PyTorch will throw a runtime error.\n",
        "- [x] B. The layer behaves as a purely linear transformation.\n",
        "- [x] C. The overall network may degenerate into a linear function if no other nonlinearities exist.\n",
        "- [x] D. The model still learns, but its expressive capacity is severely limited."
      ],
      "metadata": {
        "id": "CzfeXkasreDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "Consider a **2-layer feedforward neural network** in PyTorch:\n",
        "\n",
        "- Input vector:  \n",
        "  $$\n",
        "  x = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- First layer: `Linear(2,2)` with weights  \n",
        "  $$\n",
        "  W^{[1]} = \\begin{bmatrix} 1 & -2 \\\\ 0 & 1 \\end{bmatrix}, \\quad b^{[1]} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- Activation: **tanh**\n",
        "\n",
        "- Second layer: `Linear(2,1)` with weights  \n",
        "  $$\n",
        "  W^{[2]} = \\begin{bmatrix} 2 & -1 \\end{bmatrix}, \\quad b^{[2]} = [0]\n",
        "  $$\n",
        "\n",
        "- Output activation: **Sigmoid**\n",
        "\n",
        "- Target: $ y = 1 $, loss = **binary cross-entropy**.\n",
        "\n",
        "### 3. Compute the gradient of the loss with respect to the weights of the second layer, $\\frac{\\partial L}{\\partial W^{[2]}}$. Finally, report the sum of all elements of this gradient vector.\n",
        "\n",
        "Ans: âˆ’0.238"
      ],
      "metadata": {
        "id": "Fdo_hPOareDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Which PyTorch optimizer is generally the least sensitive to learning rate tuning?\n",
        "\n",
        "- [ ] A. `torch.optim.SGD`\n",
        "- [ ] B. `torch.optim.RMSprop`\n",
        "- [x] C. `torch.optim.Adam`\n",
        "- [ ] D. `torch.optim.Adagrad`"
      ],
      "metadata": {
        "id": "Zohp0K5xreDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. Consider two models trained on the same dataset: one with `torch.optim.SGD`, another with `torch.optim.Adam`. Which statements are correct?\n",
        "\n",
        "- [x] A. Adam adapts the learning rate for each parameter based on historical gradients.\n",
        "- [x] B. SGD with momentum can approximate Adamâ€™s performance in some tasks.\n",
        "- [ ] C. Adam always converges faster than SGD in all scenarios.\n",
        "- [x] D. SGD may generalize better than Adam in large-scale classification tasks."
      ],
      "metadata": {
        "id": "dpEFgq7YreDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. You experiment with different hidden-layer activations in PyTorch. Which is most prone to dead neurons?\n",
        "\n",
        "- [ ] A. `Sigmoid`\n",
        "- [ ] B. `Tanh`\n",
        "- [x] C. `ReLU`\n",
        "- [ ] D. `LeakyReLU`"
      ],
      "metadata": {
        "id": "ycGXj6CKDmRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. Suppose you implement a classifier with `torch.nn.Softmax(dim=1)` at the output. Which of the following are true?\n",
        "\n",
        "- [x] A. The output probabilities always sum to 1.\n",
        "- [x] B. This is suitable for multi-class single-label problems.\n",
        "- [ ] C. This is suitable for multi-class multi-label problems.\n",
        "- [x] D. The most commonly used loss function for such problems is `torch.nn.CrossEntropyLoss`."
      ],
      "metadata": {
        "id": "AJW-oS85EHYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "Consider a **3-hidden-layer PyTorch network** with width 2 and **sigmoid activation** everywhere.\n",
        "\n",
        "- Input:  \n",
        "  $$\n",
        "  x = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- All weights initialized as identity matrices, biases = 0.\n",
        "\n",
        "- Output: a single linear neuron.\n",
        "\n",
        "- Target: $ y = 3 $, loss = **MSE**.\n",
        "\n",
        "### 8. Compute the gradient of the loss with respect to the input, $ \\frac{\\partial L}{\\partial x} $. Finally, report the sum of all elements of this gradient vector.\n",
        "\n",
        "Ans: 0.0003"
      ],
      "metadata": {
        "id": "1QeDFJLvFPnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 9. Why is proper weight initialization critical for deep PyTorch models?\n",
        "\n",
        "- [x] A. Poor initialization may cause exploding/vanishing gradients.\n",
        "- [ ] B. Initialization only affects convergence speed, not final performance.\n",
        "- [x] C. Xavier/Glorot initialization stabilizes variance of activations across layers.\n",
        "- [ ] D. Random unscaled initialization is sufficient for deep networks."
      ],
      "metadata": {
        "id": "1NVo4MpVF8r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Which of the following directly mitigate vanishing gradients in deep PyTorch networks?\n",
        "\n",
        "- [ ] A. Replace ReLU with Sigmoid.\n",
        "- [x] B. Apply `torch.nn.BatchNorm`.\n",
        "- [x] C. Use Residual Connections (`torch.nn.Sequential`).\n",
        "- [x] D. Use initialization (`torch.nn.init.kaiming_normal_`)."
      ],
      "metadata": {
        "id": "2BNSIiamw5XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. Which of the following do not dynamically adjust the learning rate during PyTorch training?\n",
        "\n",
        "- [ ] A. `torch.optim.lr_scheduler.ReduceLROnPlateau`\n",
        "- [ ] B. `torch.optim.lr_scheduler.StepLR`\n",
        "- [ ] C. `torch.optim.lr_scheduler.ExponentialLR`\n",
        "- [x] D. Fixed `lr` in `torch.optim.Adam`"
      ],
      "metadata": {
        "id": "gM4wD5YQxUVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 12. What is the effect of different activations on optimization geometry?\n",
        "\n",
        "- [x] A. ReLU partitions space into piecewise linear regions, simplifying optimization.\n",
        "- [x] B. Sigmoid compresses outputs into [0,1], often leading to flat gradients.\n",
        "- [x] C. Tanh centers outputs at 0, often helping convergence vs sigmoid.\n",
        "- [ ] D. Softmax in hidden layers improves optimization geometry."
      ],
      "metadata": {
        "id": "Do3OewEpxuGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 13. Two identical PyTorch models are trained, but one uses `torch.nn.BatchNorm1d`. Which is true?\n",
        "\n",
        "- [x] A. BatchNorm reduces internal covariate shift.\n",
        "- [ ] B. BatchNorm eliminates the need for activations.\n",
        "- [x] C. BatchNorm allows training with higher learning rates.\n",
        "- [ ] D. BatchNorm guarantees higher test accuracy."
      ],
      "metadata": {
        "id": "lSJtWScByM30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 14. During training, you notice oscillations in loss values across epochs. Which remedies are valid?\n",
        "\n",
        "- [x] A. Reduce learning rate in the optimizer.\n",
        "- [x] B. Switch from SGD to Adam.\n",
        "- [x] C. Add momentum to SGD.\n",
        "- [ ] D. Remove non-linear activations to simplify optimization."
      ],
      "metadata": {
        "id": "eWWRAGqiyguV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "You train a **3-layer MLP** in PyTorch:\n",
        "\n",
        "- Input dimension = 2\n",
        "- Hidden Layer 1: 2 neurons, activation = ReLU\n",
        "- Hidden Layer 2: 2 neurons, activation = ReLU\n",
        "- Output: 1 neuron, activation = Sigmoid\n",
        "\n",
        "Weights/biases:\n",
        "\n",
        "$$\n",
        "W^{[1]} = \\begin{bmatrix} 1 & -1 \\\\ 2 & 0 \\end{bmatrix}, \\quad b^{[1]} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{[2]} = \\begin{bmatrix} 1 & 0 \\\\ -1 & 2 \\end{bmatrix}, \\quad b^{[2]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{[3]} = \\begin{bmatrix} 1 & -2 \\end{bmatrix}, \\quad b^{[3]} = [0]\n",
        "$$\n",
        "\n",
        "Input:  \n",
        "$$\n",
        "x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\text{Target: } y = 0\n",
        "$$\n",
        "\n",
        "### 15. Compute the final output prediction $\\hat{y}$ of the network after the forward pass.\n",
        "\n",
        "Ans: 0.182"
      ],
      "metadata": {
        "id": "g3-kcnI_y0Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 1. We train a fully connected PyTorch network with 3 hidden layers of ReLU activations. During training, it is observed that:\n",
        "- The first layerâ€™s gradients are nearly zero.\n",
        "- The later layers still update normally.\n",
        "\n",
        "Which of the following could be the reasons for this -\n",
        "\n",
        "- [x] A. Improper weight initialization (e.g., large negative values) causing dead ReLUs in early layers.\n",
        "- [ ] B. Exploding gradients in deeper layers cancel out earlier layer updates.\n",
        "- [ ] C. The non-linearity of ReLU creates a non-symmetric optimization landscape.\n",
        "- [x] D. Backpropagated gradients shrink as they pass through multiple layers with zeros from ReLU.\n",
        "\n",
        "Ans:\n",
        "\n",
        "If early-layer pre-activations are pushed negative and you use ReLU, many units output 0, giving zero local derivative and near-zero upstream gradient â€” dead/near-dead units.\n",
        "Gradients are multiplied by gates (0/1 for ReLU) and weight matrices; many zeros in early layers shrink backpropagated signal."
      ],
      "metadata": {
        "id": "0R1orU2GJsm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 2. Suppose we have a binary classification task using sigmoid output with BCELoss. It is observed that the model predicts values very close to 0 or 1 even in the first few epochs. What does this suggest?\n",
        "\n",
        "- [ ] A. The learning rate is too low.\n",
        "- [x] B. The weights are initialized with too large magnitudes.\n",
        "- [ ] C. The optimizer is not updating parameters.\n",
        "- [ ] D. The loss function is unsuitable for sigmoid outputs.\n",
        "\n",
        "Very large initial weights push sigmoid into saturation. In saturation, gradients = 0, yet predictions look â€œconfidentâ€ early. Low LR (A) causes tiny updates but doesnâ€™t create saturated outputs. (C) would keep outputs near their random initial values. (D) is false â€” BCE + sigmoid is standard."
      ],
      "metadata": {
        "id": "s90DCltIKWry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "In PyTorch, we have a following model -\n",
        "\n",
        "```\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(100, 50),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(50, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "```\n",
        "\n",
        "### 3. Suppose input features are normalized and mean-centered. Which of the following would happen if you replaced ReLU with Sigmoid in the hidden layer?\n",
        "\n",
        "- [x] A. Risk of vanishing gradients increases.\n",
        "- [x] B. Effective capacity of the network decreases.\n",
        "- [x] C. The model may still fit training data but require significantly more epochs.\n",
        "- [ ] D. Exploding gradients become more common.\n",
        "\n",
        "Ans: Hidden sigmoid reintroduces saturation, thus, vanishing gradients more likely. A network with saturating hidden units behaves more linearly around the operating point, thus, lower effective capacity. It can still fit, but needs more epochs due to slower gradient flow. Sigmoid does not make exploding gradients more common (it more like depends on either ReLU or bad initialization or LSTM gate issue)."
      ],
      "metadata": {
        "id": "859Nz-oLKcZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 4. You train a 2-layer MLP with tanh activations on normalized data. During training, you observe that the gradients of the first layerâ€™s weights are consistently around 10^-5. What is the most likely long-term effect on model convergence?\n",
        "\n",
        "Ans: Stagnation\n",
        "\n",
        "If first-layer gradients sit around this value, parameter updates are negligible. This will lead to stall of training process unless you change activation/init/LR or add normalization/residuals."
      ],
      "metadata": {
        "id": "iiQDKC_OC3lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 5. Is this statement true or false, using torch.nn.Softmax with CrossEntropyLoss is redundant because CrossEntropyLoss already applies log-softmax internally.\n",
        "\n",
        "- [x] A. True, because PyTorch expects raw logits.\n",
        "- [ ] B. False, because CrossEntropyLoss expects probabilities as input.\n",
        "\n",
        "Ans: CrossEntropyLoss expects raw logits and internally applies LogSoftmax + NLL. Passing already-softmaxed probabilities harms training (and can be numerically unstable).\n"
      ],
      "metadata": {
        "id": "00HyWeqwDSmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "Suppose you have a 2-layer network:\n",
        "\n",
        "- Layer 1: Linear(2,2) with weights = identity matrix, no bias, activation = ReLU\n",
        "- Layer 2: Linear(2,1) with weights = [1,1], bias = 0, activation = Sigmoid\n",
        "\n",
        "With,\n",
        "\n",
        "Input: ğ‘¥=[1,âˆ’2].\n",
        "\n",
        "### 6. What is the final predicted output (rounded to 3 decimals)?\n",
        "\n",
        "Ans: 0.731\n",
        "\n",
        "- Layer1: z1=x=[1,âˆ’2], ReLU â†’ a1=[1,0].\n",
        "- Layer2 pre-act: 1â‹…1+1â‹…0=1.\n",
        "- Sigmoid(1) = 0.731"
      ],
      "metadata": {
        "id": "qIHCfxaGvS1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 7. You experiment with a custom optimizer that skips gradient updates every alternate step. What issues could arise?\n",
        "\n",
        "- [X] A. Convergence slows down compared to normal optimizers.\n",
        "- [X] B. Gradient information may decay before being used.\n",
        "- [X] C. Loss curve oscillations may increase.\n",
        "- [ ] D. The model is mathematically guaranteed to diverge.\n",
        "\n",
        "Skipping updates halves effective step frequency, leading to slower convergence, some gradient signals become stale, and the loss curve can oscillate. No guarantee of divergence."
      ],
      "metadata": {
        "id": "H5jCpXJ5wolr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. Consider a convolutional neural network where the input image is of size **128Ã—128Ã—3**, and the first convolutional layer uses **64 filters of size 7Ã—7 with stride 2 and padding 3**. Which of the following correctly gives the output dimension of this convolutional layer?\n",
        "\n",
        "- [x] A. 64 Ã— 64 Ã— 64  \n",
        "- [ ] B. 64 Ã— 128 Ã— 128  \n",
        "- [ ] C. 64 Ã— 63 Ã— 63  \n",
        "- [ ] D. 64 Ã— 62 Ã— 62  \n",
        "\n",
        "**Ans:** Output dimension = (128 + 2Ã—3 âˆ’ 7)/2 + 1 = 64. Hence, 64Ã—64 with 64 filters.The convolution formula ensures halving due to stride 2. With padding=3, the spatial resolution becomes 64Ã—64. Depth equals number of filters = 64."
      ],
      "metadata": {
        "id": "Zk-L_QtRp96K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. CNNs exploit spatial locality and parameter sharing. Which of the following statements are correct with respect to these principles?\n",
        "\n",
        "- [x] A. Parameter sharing means the same kernel weights slide across different spatial regions of the input.  \n",
        "- [ ] B. Local connectivity ensures that each neuron is connected to all the input pixels.  \n",
        "- [x] C. Increasing kernel size reduces the degree of spatial locality assumption.  \n",
        "- [x] D. Parameter sharing is the reason CNNs require fewer parameters compared to fully connected networks.  \n",
        "\n",
        "**Ans:** Local connectivity means small local regions, not all pixels. Sharing kernels across the image reduces parameter count. Larger kernels imply a weaker assumption of strict locality."
      ],
      "metadata": {
        "id": "GlzQE7DeXIUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 3. You apply a 3Ã—3 kernel with stride 1 and no padding to a 32Ã—32 image. How many parameters does this convolutional layer have if it uses 16 filters and the input has 3 channels? (Ignore bias terms).\n",
        "\n",
        "**Ans:** 432\n",
        "\n",
        "Each filter has 3Ã—3Ã—3 parameters, and there are 16 such filters. Parameters depend only on kernel size and input depth, not on spatial dimensions."
      ],
      "metadata": {
        "id": "b8s0RVS3XTLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Pooling layers in CNNs are often said to contribute to translation invariance. Which of the following best describes this effect?\n",
        "\n",
        "- [ ] A. Pooling ensures the CNN is completely invariant to all types of affine transformations.  \n",
        "- [x] B. Pooling discards exact positional information but retains dominant activations, so small shifts in the input cause similar outputs.  \n",
        "- [ ] C. Pooling performs normalization across channels to ensure stability.  \n",
        "- [ ] D. Pooling acts as a regularization technique by enforcing sparsity.  \n",
        "\n",
        "**Ans:** Pooling creates robustness to small translations since nearby activations result in similar pooled outputs, but it does not handle all transformations."
      ],
      "metadata": {
        "id": "Nd7JE5T9Xfuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. Consider the receptive field of neurons in deeper CNN layers. Which statements are true?\n",
        "\n",
        "- [x] A. Receptive field always grows with depth.  \n",
        "- [x] B. A neuron in deeper layers â€œseesâ€ a larger portion of the input image.  \n",
        "- [ ] C. If stride is large, receptive field growth is slower.  \n",
        "- [x] D. Stacking multiple small kernels (e.g., 3Ã—3) can achieve the same receptive field as one large kernel (e.g., 7Ã—7) with fewer parameters.  \n",
        "\n",
        "**Ans:** Depth increases effective receptive field. Stride actually increases receptive field growth. Multiple 3Ã—3 kernels approximate larger kernels while using fewer parameters."
      ],
      "metadata": {
        "id": "6sTyiNWoXt5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. Suppose you stack **two convolutional layers** with kernel size 3Ã—3, stride 1, padding 1. What is the effective receptive field size of a neuron in the second layer with respect to the input image?\n",
        "\n",
        "- [ ] A. 3x3\n",
        "- [x] B. 5x5\n",
        "- [ ] C. 7x7\n",
        "- [ ] D. 1x1\n",
        "\n",
        "**Ans:** Each 3Ã—3 covers its own local region. Two stacked layers expand the effective receptive field by two pixels on each side, yielding 5Ã—5."
      ],
      "metadata": {
        "id": "COu2yuLtX1ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. Batch Normalization is frequently applied after convolutional layers. Which of the following is the most accurate reasoning for its utility in CNNs?\n",
        "\n",
        "- [ ] A. It prevents overfitting by reducing model complexity.  \n",
        "- [x] B. It normalizes activations across spatial dimensions, allowing faster training and more stable gradients.  \n",
        "- [ ] C. It increases representational power by introducing more parameters.  \n",
        "- [ ] D. It replaces dropout completely as a regularization technique.  \n",
        "\n",
        "**Ans:** BatchNorm stabilizes gradient flow and training by normalizing activations, making optimization easier and reducing sensitivity to initialization."
      ],
      "metadata": {
        "id": "hSNiwJlxYPQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 8. You must choose among valid, same (zero), and reflect padding for early layers on natural images. Which statement best captures their frequency-domain implications on edge artifacts and bias near borders?\n",
        "\n",
        "- [ ] A. Valid padding preserves stationarity across the field and avoids any boundary bias.\n",
        "- [x] B. Same (zero) padding introduces strong artificial low-frequency bias near edges because zeros act as a dark frame, corrupting local statistics.\n",
        "- [ ] C. Reflect padding better preserves local second-order statistics near borders compared to zero padding.\n",
        "- [x] D. All padding choices are equivalent after sufficient depth.\n",
        "\n",
        "**Ans:** A: valid reduces field size and induces border bias by shrinking coverage; D: artifacts can persist and affect learning."
      ],
      "metadata": {
        "id": "peFVX-JrYgj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 9. You train a CNN for object classification on natural images. The feature extractor is a stack of strided convolutions with ReLU and no padding in early layers, followed by global average pooling (GAP) and a linear classifier. Consider the following statements about translation behavior:\n",
        "\n",
        "- [ ] A single convolution with stride 1 and circular padding is exactly translation equivariant to pixel shifts under the group action.\n",
        "- [x] In practice, boundary handling and pooling/striding break exact translation equivariance; GAP partially recovers translation invariance at the representation level.\n",
        "- [ ] C. Max pooling increases translation equivariance compared to stride-2 convolution with the same receptive field.\n",
        "- [x] Sub-pixel (fractional) translations of the input typically produce aliasing after striding/pooling unless pre-filtering (low-pass) is used.\n",
        "\n",
        "**Ans:** B is correct; equivariance is broken by padding/striding/pooling; GAP promotes invariance in late layers. D is correct due to sampling theory and aliasing. A is only true under circular padding and infinite support assumptions throughout the stack, not typical pipelines; C is falseâ€”max pooling generally reduces exact equivariance similarly to striding."
      ],
      "metadata": {
        "id": "UvQOhm-NZTeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 10. Consider two CNNs: one uses max pooling, the other uses strided convolution for downsampling. Which of the following best compares their roles?\n",
        "\n",
        "- [ ] A. Max pooling introduces additional parameters compared to strided convolution.  \n",
        "- [x] B. Strided convolution learns the downsampling operation, while max pooling is fixed.  \n",
        "- [ ] C. Max pooling retains more information than strided convolution.  \n",
        "- [ ] D. Strided convolution cannot reduce resolution effectively.  \n",
        "\n",
        "**Ans:** Pooling is deterministic, whereas strided convolution learns kernels to perform downsampling, thus being more flexible."
      ],
      "metadata": {
        "id": "w0X0xlPaZ0XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 11. You pass a 224Ã—224Ã—3 image through a CNN with the following layers:  \n",
        "1. Conv: 7Ã—7, stride 2, padding 3  \n",
        "2. Max Pool: 3Ã—3, stride 2, padding 1  \n",
        "\n",
        "What is the output spatial dimension after these two layers (ignore channels)? Please note that the answer will be in the form of `a X a`. Please put the value of `a`.\n",
        "\n",
        "**Ans:** 56. First conv reduces to 112Ã—112. Then pooling reduces further to 56Ã—56. Formula applied step-by-step."
      ],
      "metadata": {
        "id": "Pyf5_odvaEqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "\n",
        "### 12. Which statement best captures why residual connections help very deep CNNs?\n",
        "\n",
        "- [ ] A. They widen the network, increasing representational capacity quadratically in depth.\n",
        "- [x] B. They provide identity shortcuts so the optimization can learn residual functions; the Jacobianâ€™s eigenvalues are biased toward 1, mitigating vanishing/exploding gradients.\n",
        "- [ ] C. They reduce the number of parameters and hence regularize implicitly, the main effect.\n",
        "- [ ] D. They enforce exact invariance to additive noise in inputs.\n",
        "\n",
        "**Ans:** The residual connections were mainly introduced in deep CNNs, so that it will help to mitigate the problem of vanishing or exploding gradients. It is because in very deep layers it might have the tendency to forget the accumulated gradients."
      ],
      "metadata": {
        "id": "IGFj_j5GbHej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "A CNN trained on CIFAR-10 achieves 95 % training accuracy but only 65 % validation accuracy. The researcher tries the following remedies:\n",
        "\n",
        "1. Adding more convolutional layers\n",
        "2. Increasing kernel size in each convolutional layer\n",
        "3. Using dropout and batch normalization\n",
        "4. Performing extensive data augmentation\n",
        "\n",
        "### 1. Which of the following will most likely reduce overfitting?\n",
        "\n",
        "- [ ] A. 1 and 2\n",
        "- [ ] B. 1 and 3\n",
        "- [x] C. 3 and 4\n",
        "- [ ] D. 1 and 4\n",
        "\n",
        "Ans: Adding layers or larger kernels (1 and 2) may further overfit; dropout and batch norm regularize activations, while augmentation expands the dataset distribution, combating overfitting."
      ],
      "metadata": {
        "id": "FF2pCUklocMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 2. Consider a 2-D convolution with a 5Ã—5 kernel, stride = 2, and padding = 1 on a 64Ã—64Ã—3 input. How many learnable parameters (including) exist in this single convolution layer with 32 filters?\n",
        "\n",
        "Ans: 2432\n",
        "\n",
        "Each filter has 5Ã—5Ã—3=75 weights + 1 bias = 76 parameters.\n",
        "Total = 32 Ã— 76 = 2 432 learnable parameters.\n",
        "Stride and padding affect output size, not parameter count."
      ],
      "metadata": {
        "id": "4AuXUEP5o3vQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 3. Which of the following statements about Batch Normalization (BN) in CNNs are correct?\n",
        "\n",
        "- [x] A. BN mitigates internal covariate shift by stabilizing feature distribution.\n",
        "- [x] B. BN allows higher learning rates, accelerating convergence.\n",
        "- [ ] C. BN completely removes the need for dropout.\n",
        "- [x] D. BN normalizes activations across spatial dimensions and batch samples.\n",
        "\n",
        "Ans: BN standardizes intermediate activations and enables faster, more stable training.\n",
        "Dropout may still be needed for regularization; BN does not replace it entirely."
      ],
      "metadata": {
        "id": "MEHwPePgpWgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. In a CNN for CIFAR-10, early convolutional filters tend to capture edges and textures, while deeper filters respond to object parts. This hierarchy emerges primarily due to â€”\n",
        "\n",
        "- [ ] A. Weight sharing across spatial locations\n",
        "- [x] B. Local receptive fields expanding through layer stacking\n",
        "- [ ] C. ReLU introducing sparsity in activations\n",
        "- [ ] D. Pooling enforcing translation invariance\n",
        "\n",
        "Ans: As layers stack, each neuronâ€™s receptive field covers a larger portion of the input, allowing progressively complex feature compositionsâ€”from edges â†’ shapes â†’ semantic parts."
      ],
      "metadata": {
        "id": "9_uOJAb0xGpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "A CNN has three convolutional layers, each with kernel 3, stride 1, and padding 1, followed by one max-pooling layer of size 2 Ã— 2 and stride 2.\n",
        "\n",
        "### 5. What is the receptive field (in input pixels) of one neuron in the final feature map?\n",
        "\n",
        "- [ ] A. 5 x 5\n",
        "- [ ] B. 7 x 7\n",
        "- [x] C. 10 x 10\n",
        "- [ ] D. 11 x 11\n",
        "\n",
        "Ans: Receptive field after three 3Ã—3 stride-1 layers: 3+2+2=7.\n",
        "Pooling doubles it: 7+(2âˆ’1)Ã—2=10.\n",
        "Hence, each final-layer neuron â€œseesâ€ 10Ã—10 pixels of the input."
      ],
      "metadata": {
        "id": "BXFCgEZcxo6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. While training a CNN from scratch on MNIST, accuracy stagnates at 90 % despite sufficient data and no overfitting. Which is the most plausible underlying reason?\n",
        "\n",
        "- [x] A. Vanishing gradients due to sigmoid activations\n",
        "- [ ] B. Exploding gradients due to large initialization\n",
        "- [ ] C. Improper shuffling of mini-batches\n",
        "- [ ] D. Over-regularization by dropout\n",
        "\n",
        "Ans: Sigmoids saturate for large inputs, producing near-zero derivatives that halt weight updates.\n",
        "ReLU or Leaky ReLU alleviate this by maintaining gradient flow."
      ],
      "metadata": {
        "id": "RAlAJgSmyKKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "Consider two CNN architectures trained on identical data:\n",
        "1. Model A: no pooling, but stride 2 in convolutions\n",
        "2. Model B: uses pooling layers for down-sampling\n",
        "\n",
        "### 7. Which statements are true?\n",
        "\n",
        "- [x] A. Model A learns down-sampling through convolutional weights.\n",
        "- [x] B. Model Bâ€™s pooling provides parameter-free translation invariance.\n",
        "- [x] C. Model A may retain more task-specific spatial detail.\n",
        "- [ ] D. Model B will always outperform Model A in generalization.\n",
        "\n",
        "Ans: Strided convolutions subsample via learnable kernels (1), pooling adds invariance without parameters (2), and convolutional down-sampling can preserve fine spatial cues (3).\n",
        "Performance superiority (4) is not guaranteed; it depends on data and regularization."
      ],
      "metadata": {
        "id": "bFbwo-YByifL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 8. Which of the following is a key reason that deeper CNNs like ResNet outperform shallow ones on large-scale image tasks?\n",
        "\n",
        "- [ ] A. Deeper CNNs always have fewer parameters.\n",
        "- [ ] B. Residual connections mitigate vanishing gradients and preserve representational richness.\n",
        "- [ ] C. Batch normalization replaces all non-linearities.\n",
        "- [ ] D. Deeper CNNs avoid overfitting by having more layers.\n",
        "\n",
        "Ans: ResNets introduce skip (identity) connections that ease gradient flow and enable learning of residual mappings, solving degradation in very deep nets."
      ],
      "metadata": {
        "id": "EcrCQzNazBd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "An image of size 128Ã—128 is processed by a sequence of layers:\n",
        "- Conv1: 3Ã—3 kernel, stride 1, padding 1\n",
        "- MaxPool: 2Ã—2, stride 2\n",
        "- Conv2: 3Ã—3 kernel, stride 1, padding 1\n",
        "- MaxPool: 2Ã—2, stride 2\n",
        "\n",
        "### 9. What is the final spatial size (HÃ—W) of the feature map (ignore channels)? Give the value of H.\n",
        "\n",
        "Ans: 32\n",
        "\n",
        "Each pooling halves resolution: 128â†’64â†’32. Convs preserve it due to padding 1."
      ],
      "metadata": {
        "id": "eujGEXeAz_iw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. During CNN training, you observe the validation accuracy stagnates while training accuracy continues to improve. Which of the following remedies are conceptually justified?\n",
        "\n",
        "- [x] A. Adding dropout layers between fully connected blocks.\n",
        "- [ ] B. Increasing the learning rate.\n",
        "- [x] C. Using data augmentation such as rotations and flips.\n",
        "- [x] D. Reducing the number of convolutional filters.\n",
        "\n",
        "Ans: Classic overfitting symptom. Regularization (dropout), augmentation, or reduced capacity can help.\n",
        "Raising the learning rate generally worsens convergence and may destabilize optimization."
      ],
      "metadata": {
        "id": "d3hz-a510i1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding Questions - 39"
      ],
      "metadata": {
        "id": "0inkPrK8mlEt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a **Python function** that computes the output of a neuron given input vector $(x)$, weights $(w)$, bias $(b)$, and activation function $(\\sigma)$. Test it with ReLU activation.  \n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def relu(z):\n",
        "    return max(0, z)\n",
        "\n",
        "def neuron_output(x, w, b, activation):\n",
        "    z = np.dot(w, x) + b\n",
        "    return activation(z)\n",
        "\n",
        "x = np.array([2, -1])\n",
        "w = np.array([0.5, -0.25])\n",
        "b = 1\n",
        "print(neuron_output(x, w, b, relu))\n",
        "```\n",
        "\n",
        "**Answer:** `2.25`"
      ],
      "metadata": {
        "id": "qnzzCzsOnPhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Write a Python function that computes **Mean Squared Error (MSE)** given predicted values $(y_{pred})$ and true values $(y_{true})$. Test it with:  \n",
        "$$\n",
        "y_{true} = [1, 0, 1], \\quad y_{pred} = [0.9, 0.2, 0.8]\n",
        "$$  \n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((np.array(y_true) - np.array(y_pred))**2)\n",
        "\n",
        "y_true = [1, 0, 1]\n",
        "y_pred = [0.9, 0.2, 0.8]\n",
        "print(mse(y_true, y_pred))\n",
        "```\n",
        "\n",
        "**Answer:** 0.0233 (approx).  "
      ],
      "metadata": {
        "id": "3sYU8k_0nbTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement a Python function that computes **binary cross-entropy loss** for given true labels and predicted probabilities. Then compute the loss for:  \n",
        "\n",
        "$$\n",
        "y_{true} = [1, 0, 1], \\quad y_{pred} = [0.9, 0.2, 0.7]\n",
        "$$\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    return -np.mean(y_true*np.log(y_pred) + (1-y_true)*np.log(1-y_pred))\n",
        "\n",
        "y_true = [1, 0, 1]\n",
        "y_pred = [0.9, 0.2, 0.7]\n",
        "print(binary_cross_entropy(y_true, y_pred))  \n",
        "```\n",
        "\n",
        "**Answer:** ~0.228  "
      ],
      "metadata": {
        "id": "KzRGgs9IoDXY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2,4),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(4,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = MLP()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = criterion(output,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "```\n",
        "\n",
        "### 1. Which of the following are true?\n",
        "\n",
        "- [ ] A. Without non-linear activation, this model can solve XOR.  \n",
        "- [x] B. If we replace Tanh with ReLU, convergence might be faster.\n",
        "- [x] C. If we omit `optimizer.zero_grad()`, gradients accumulate and weights diverge.\n",
        "- [ ] D. Sigmoid in the final layer is necessary for mean squared error loss calculation."
      ],
      "metadata": {
        "id": "BxAgTvpYrqEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(42)\n",
        "\n",
        "X = torch.tensor([[1.,-1.]])\n",
        "y = torch.tensor([[1.]])\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters():\n",
        "        p.fill_(1.0)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "output = model(X)\n",
        "loss = criterion(output,y)\n",
        "print(output.item(), loss.item())\n",
        "```\n",
        "\n",
        "### 2. What is the loss value printed? (Round to 3 decimal places)\n",
        "\n",
        "Ans: 0.313"
      ],
      "metadata": {
        "id": "Kej8ZRTKrqEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = torch.randn(5,10)\n",
        "y = torch.randint(0,3,(5,))\n",
        "\n",
        "model = nn.Linear(10,3)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X)\n",
        "    loss = F.cross_entropy(logits,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "### 3. Which of the following statement/s is incorrect?\n",
        "\n",
        "- [ ] A. `cross_entropy` internally applies softmax to logits.\n",
        "- [ ] B. Changing labels `y` to one-hot encoding would break this code.\n",
        "- [ ] C. The final linear layer must have 3 outputs for this setup.\n",
        "- [x] D. Replacing Adam with SGD(lr=0.01) would guarantee identical convergence."
      ],
      "metadata": {
        "id": "bN4jtZ84rqEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(0)\n",
        "\n",
        "X = torch.tensor([[2., -3.]])\n",
        "layer = nn.Linear(2,1)\n",
        "with torch.no_grad():\n",
        "    layer.weight[:] = torch.tensor([[1.,-2.]])\n",
        "    layer.bias[:] = torch.tensor([0.5])\n",
        "\n",
        "output = layer(X)\n",
        "print(output.item())\n",
        "```\n",
        "\n",
        "### 4. What is the numerical output value printed?\n",
        "\n",
        "Ans: 8.5"
      ],
      "metadata": {
        "id": "GzvQ7Pd1rqEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X)\n",
        "    loss = F.cross_entropy(logits,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "```\n",
        "\n",
        "### 5. Which statements are true?\n",
        "\n",
        "- [x] A. Learning rate decreases every 10 epochs by a factor of 0.1.\n",
        "- [ ] B. If base LR = 0.01, then LR at epoch 21 is 0.0001.\n",
        "- [ ] C. Scheduler must be stepped before `optimizer.step()` to function correctly.\n",
        "- [x] D. This helps avoid getting stuck in sharp local minima."
      ],
      "metadata": {
        "id": "qy7eZzAHrqEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(7)\n",
        "\n",
        "X = torch.tensor([[1.,2.,3.]])\n",
        "y = torch.tensor([[1.]])\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "output = model(X)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "\n",
        "grads = []\n",
        "for p in model.parameters():\n",
        "    grads.append(p.grad.sum().item())\n",
        "\n",
        "print(sum(grads))\n",
        "```\n",
        "\n",
        "### 6. What is the sum of all gradient elements printed? (Round to 3 decimal places)\n",
        "\n",
        "Ans: -0.578"
      ],
      "metadata": {
        "id": "9FOyGLwQrqEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X = torch.randn(32, 10)\n",
        "y = torch.randint(0, 2, (32,1)).float()\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(5):\n",
        "    for i in range(0, 32, 8):\n",
        "        xb, yb = X[i:i+8], y[i:i+8]\n",
        "        output = model(xb)\n",
        "        loss = criterion(output, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```\n",
        "\n",
        "### 1. Which of the following are true?\n",
        "\n",
        "- [ ] A. The final parameter updates will be equivalent to full-batch training.  \n",
        "- [x] B. Gradients will accumulate across mini-batches, making updates unstable.\n",
        "- [x] C. The model may still train but with unpredictable dynamics.\n",
        "- [x] D. Adding optimizer.zero_grad() fixes this training loop.\n",
        "\n",
        "Ans: Adding the term fixes this loop, as otherwise, gradient accumulation happens across the mini-batches which leads to unstable updates. The training might happen but dynamics are not predictable now."
      ],
      "metadata": {
        "id": "SgGDJTSBMb54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "lrs = []\n",
        "for epoch in range(12):\n",
        "    lrs.append(optimizer.param_groups[0]['lr'])\n",
        "    scheduler.step()\n",
        "\n",
        "```\n",
        "\n",
        "### 2. What is the learning rate after epoch 10? (Round to 4 decimal places)\n",
        "\n",
        "Ans: 0.0025\n",
        "\n",
        "We start at 0.01, at every 5 steps it gets multiplied by 0.5.\n",
        "- Epochs 0â€“4: 0.01 â†’ after e=4 step LR=0.005\n",
        "- Epochs 5â€“9: 0.005 â†’ after e=9 step LR=0.0025\n",
        "- Epoch 10 (logged before step): 0.0025."
      ],
      "metadata": {
        "id": "Ol59Tf45Mb56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(20, 3),\n",
        "    nn.Softmax(dim=1)\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "X = torch.randn(4,20)\n",
        "y = torch.tensor([0,1,2,1])\n",
        "\n",
        "output = model(X)\n",
        "loss = criterion(output,y)\n",
        "\n",
        "```\n",
        "\n",
        "### 3. What is wrong with this code?\n",
        "\n",
        "- [x] A. CrossEntropyLoss expects raw logits, not softmax probabilities.\n",
        "- [ ] B. The target y must be one-hot encoded for CrossEntropyLoss.\n",
        "- [ ] C. The number of classes in y does not match the model output.\n",
        "- [ ] D. Nothing is wrong, it should work fine.\n",
        "\n",
        "Ans: CrossEntropyLoss expects logits. Passing softmax probabilities loses the log-sum-exp stabilization and double-softmaxes the signal."
      ],
      "metadata": {
        "id": "q2Xc6yQHMb57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(0)\n",
        "\n",
        "X = torch.tensor([[2., -3.]])\n",
        "layer = nn.Linear(2,1)\n",
        "with torch.no_grad():\n",
        "    layer.weight[:] = torch.tensor([[1.,-2.]])\n",
        "    layer.bias[:] = torch.tensor([0.5])\n",
        "\n",
        "output = layer(X)\n",
        "print(output.item())\n",
        "```\n",
        "\n",
        "### 4. What is the numerical output value printed?\n",
        "\n",
        "Ans: 8.5\n",
        "\n",
        "```text\n",
        "w1*x1 + w2*x2 + b\n",
        "2*1 + -3*-2 + 0.5\n",
        "```"
      ],
      "metadata": {
        "id": "yZvm-HSl2Ggs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 10),\n",
        "    nn.BatchNorm1d(10),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(10,1)\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "X = torch.randn(4,10)\n",
        "print(model(X))\n",
        "```\n",
        "\n",
        "### 5. What happens when the model is in .eval() mode here?\n",
        "\n",
        "- [x] A. BatchNorm uses running statistics instead of batch statistics.\n",
        "- [ ] B. Gradients will not be computed for BatchNorm parameters.\n",
        "- [ ] C. The output is identical to training mode in all cases.\n",
        "- [x] D. Dropout, if present, would also behave differently in eval mode.\n",
        "\n",
        "\n",
        "Ans: In .eval(), BatchNorm uses running mean/var, not batch stats. Gradients can still be computed if the .backward() function is called. Output differs from the train mode.  "
      ],
      "metadata": {
        "id": "R8GOC-HV2ig1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 5),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(5, 1)\n",
        ")\n",
        "```\n",
        "\n",
        "### 6. How many trainable parameters (including bias) does this model have in total?\n",
        "\n",
        "Ans: 336\n",
        "\n",
        "- Layer 1 - 10*20 + 20\n",
        "- Layer 2 - 20*5 + 5\n",
        "- Layer 3 - 5*1 + 1"
      ],
      "metadata": {
        "id": "xXG3hT74216F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OddPaddingConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OddPaddingConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n",
        "                               kernel_size=5, stride=2, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.fc = nn.Linear(32*7*7, 10)  # assume input is 3x32x32\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.bn2(out))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "model = OddPaddingConvNet()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 1. Which of the following are true?\n",
        "\n",
        "- [ ] A. The Linear layer input size is incorrectly specified; it will throw an error.\n",
        "- [x] B. After conv2, the feature map size is 7x7.\n",
        "- [ ] C. Zero padding in conv1 makes translation equivariance exact at edges.\n",
        "- [x] D. After conv1, the feature map size is 14x14.\n",
        "\n",
        "Ans: Conv1: (32âˆ’5)/2+1=14. Conv2: same padding, stride=2, yields 7. Linear matches 32âˆ—7âˆ—7. But equivariance is broken at borders."
      ],
      "metadata": {
        "id": "egZh6i4-qU5l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "x = torch.randn(2, 16, 32, 32)\n",
        "block = ResidualBlock()\n",
        "y = block(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 2. Which statements are correct?\n",
        "\n",
        "- [x] A. The residual connection ensures gradient bypass, reducing vanishing gradient risk.\n",
        "- [ ] B. Removing BN would not affect gradient flow stability.\n",
        "- [x] C. y.shape is (2, 16, 32, 32).\n",
        "- [ ] D. Skip connection doubles parameter count.\n",
        "\n",
        "Ans: BN stabilizes gradients, skip aids flow. Params arenâ€™t doubled, since skip adds no learnable weights."
      ],
      "metadata": {
        "id": "_m15WwAtjku3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "bn = nn.BatchNorm2d(8)\n",
        "x_train = torch.randn(10, 8, 32, 32)\n",
        "bn.train()\n",
        "out_train = bn(x_train)\n",
        "bn.eval()\n",
        "x_test = torch.randn(2, 8, 32, 32)\n",
        "out_test = bn(x_test)\n",
        "```\n",
        "\n",
        "### 3. Which are true?\n",
        "\n",
        "- [ ] A. In train mode, running_mean is not updated.\n",
        "- [x] B. In eval mode, normalization uses running_mean/var not current batch.\n",
        "- [x] C. Small batch size at test does not affect outputs.\n",
        "- [ ] D. Train-time always uses stored running stats.\n",
        "\n",
        "Ans: Train: uses batch stats; updates running stats. Eval: uses running stats, small batch irrelevant."
      ],
      "metadata": {
        "id": "32ABZe2JkDn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 16, 3, stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv(x))\n",
        "        pooled = self.gap(out)\n",
        "        return self.fc(pooled.view(out.size(0), -1))\n",
        "\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "model = SmallCNN()\n",
        "print(model(x).shape)\n",
        "```\n",
        "\n",
        "### 4. Which statements are correct?\n",
        "\n",
        "- [x] A. GAP outputs shape (4,16,1,1).\n",
        "- [x] B. GAP encourages translation invariance.\n",
        "- [ ] C. Replacing GAP with Flatten would reduce parameters.\n",
        "- [x] D. GAP avoids dependence on input resolution."
      ],
      "metadata": {
        "id": "pVhvwdyKkYrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "drop = nn.Dropout(p=0.5)\n",
        "x = torch.ones(10, 10)\n",
        "drop.train()\n",
        "y = drop(x)\n",
        "```\n",
        "\n",
        "### 5. Which are correct?\n",
        "\n",
        "- [x] A. About 50% of entries become zero.\n",
        "- [x] B. Remaining entries are scaled by 2.\n",
        "- [ ] C. In eval mode, outputs are half of train mode.\n",
        "- [ ] D. Dropout is applied channel-wise only."
      ],
      "metadata": {
        "id": "ACYA3Ys-kvOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "convT = nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1)\n",
        "x = torch.randn(1, 16, 8, 8)\n",
        "y = convT(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 6. What is the output spatial size? If the answer is in the form `a x a`. Please enter the value of `a`.\n",
        "\n",
        "Ans: 16\n",
        "\n",
        "Formula: (8-1)2 - 2*1 + 4 = 16"
      ],
      "metadata": {
        "id": "xh0X3wUhlM9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "model = nn.Conv2d(3, 8, 3)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(2):\n",
        "    x = torch.randn(4, 3, 32, 32)\n",
        "    y = torch.randint(0, 8, (4,))\n",
        "    opt.zero_grad()\n",
        "    out = model(x).mean(dim=(2,3))\n",
        "    loss = F.cross_entropy(out, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "```\n",
        "\n",
        "### 7. Which statements are correct?\n",
        "\n",
        "- [ ] A. Model trains on raw pixel values without nonlinearities.\n",
        "- [x] B. The loss is computed on spatially averaged logits.\n",
        "- [x] C. `opt.zero_grad()` is necessary to avoid gradient accumulation.\n",
        "- [ ] D. Conv2d auto-applies softmax internally.\n"
      ],
      "metadata": {
        "id": "XTbK29WMl4Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "inp = torch.randn(4, 3, 64, 64)   # batch of 4 images\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 8. What is the correct output shape of `out`?\n",
        "\n",
        "- [ ] A. (4, 32, 16, 16)\n",
        "- [x] B. (4, 32, 16, 16)\n",
        "- [ ] C. (4, 32, 32, 32)\n",
        "- [ ] D. (4, 32, 64, 64)\n",
        "\n",
        "Ans: Conv1 halves size (64â†’32). Conv2 keeps (32â†’32). MaxPool halves again (32â†’16). Channels = 32."
      ],
      "metadata": {
        "id": "5rN7v5rBmW9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "model = DeeperCNN()\n",
        "inp = torch.randn(1, 3, 32, 32)\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 9. Why does the output retain 32Ã—32 spatial size even after 3 convolutions?\n",
        "\n",
        "- [x] A. Because stride=1 with padding=1 preserves spatial dimensions.\n",
        "- [ ] B. Because filters expand size.\n",
        "- [ ] C. Because PyTorch automatically resizes inputs.\n",
        "- [x] D. Because zero-padding compensates the border effect.\n",
        "\n",
        "Ans: With stride 1 and padding = kernel//2, conv is \"same\" size. PyTorch does not auto-resize; explicit padding ensures no shrinkage."
      ],
      "metadata": {
        "id": "NowhZEkQmrH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0, bias=False)\n",
        "print(sum(p.numel() for p in conv.parameters()))\n",
        "```\n",
        "\n",
        "### 10. How many parameters are there (ignore bias)?\n",
        "\n",
        "Ans: 450. Each filter: 3Ã—25 params. 6 filters = 450. Bias is off."
      ],
      "metadata": {
        "id": "0C1qTQlrnKXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class WeirdCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 3, kernel_size=1, stride=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "inp = torch.randn(2, 3, 32, 32)\n",
        "model = WeirdCNN()\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 11. Why does `WeirdCNN` act like a channel-wise linear projection?\n",
        "\n",
        "- [x] A. 1Ã—1 convolution mixes channels without changing spatial size.\n",
        "- [ ] B. 1Ã—1 convolution increases receptive field.\n",
        "- [ ] C. 1Ã—1 convolution discards spatial info completely.\n",
        "- [ ] D. It is identical to pooling."
      ],
      "metadata": {
        "id": "aYqFdYL8nf_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "class MultiConv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv2d(3, 4, 1)\n",
        "        self.branch2 = nn.Conv2d(3, 4, 3, padding=1)\n",
        "        self.branch3 = nn.Conv2d(3, 4, 5, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
        "\n",
        "model = MultiConv()\n",
        "inp = torch.randn(1,3,32,32)\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 12. What is the output channel count?\n",
        "\n",
        "Ans: 12. Each branch outputs 4 channels. Concatenation along dim=1 â†’ 4+4+4=12."
      ],
      "metadata": {
        "id": "sC2z3AsZoA2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=2)\n",
        "x = torch.randn(8, 3, 32, 32)\n",
        "y = conv(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 1. What will be the output shape of y?\n",
        "\n",
        "- [ ] A. (8, 6, 28, 28)\n",
        "- [ ] B. (8, 6, 30, 30)\n",
        "- [x] C. (8, 6, 32, 32)\n",
        "- [ ] D. (8, 6, 34, 34)\n",
        "\n",
        "Ans: Padding = 2 and stride = 1 keep spatial dimensions same:\n",
        "out=âŒŠ(32+2Ã—2âˆ’5)/1+1âŒ‹=32.\n",
        "Hence (8, 6, 32, 32)."
      ],
      "metadata": {
        "id": "skKLLrX444LA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "### 2. Which statements are true about the above network?\n",
        "\n",
        "- [x] A. After pooling, the spatial size becomes half in each dimension.\n",
        "- [x] B. The receptive field of each neuron in the output covers 5Ã—5 pixels of input.\n",
        "- [x] C. Removing padding would shrink the output feature map even before pooling.\n",
        "- [ ] D. Pooling increases the number of parameters.\n",
        "\n",
        "Ans: Padding keeps intermediate sizes same; pooling halves them.\n",
        "Two stacked 3Ã—3 kernels â‡’ receptive field = 5Ã—5.\n",
        "Pooling has no learnable parameters.\n"
      ],
      "metadata": {
        "id": "2gW-SRJt5guN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "x = torch.tensor([[[-1.0, 2.0], [3.0, -4.0]]])\n",
        "y = F.relu(x)\n",
        "print(y)\n",
        "```\n",
        "\n",
        "### 3. What will be printed as the output?\n",
        "\n",
        "- [x] A. `[[[0., 2.], [3., 0.]]]`\n",
        "- [ ] B. `[[[-1., 2.], [3., -4.]]]`\n",
        "- [ ] C. `[[[1., 2.], [3., 4.]]]`\n",
        "- [ ] D. Parsing Error\n",
        "\n",
        "Ans: ReLU sets negatives to 0; positives unchanged."
      ],
      "metadata": {
        "id": "o992kT3v6GU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 3, 3, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(3)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return x\n",
        "```\n",
        "\n",
        "### 4. During training, which are true about BatchNorm2d here?\n",
        "\n",
        "- [ ] A. Maintains running estimates of mean and variance per channel.\n",
        "- [ ] B. Has trainable Î³ (scale) and Î² (shift).\n",
        "- [ ] C. Normalizes across batch and spatial dimensions.\n",
        "- [ ] D. Has no effect during inference.\n",
        "\n",
        "Ans: BN learns per-channel affine params (Î³, Î²) and uses batch stats while training; during inference, uses running means."
      ],
      "metadata": {
        "id": "fSyyRvlC6e6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch.optim as optim\n",
        "\n",
        "model = SmallCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "for epoch in range(10):\n",
        "    outputs = model(torch.randn(4, 3, 32, 32))\n",
        "    loss = criterion(outputs.view(4, -1), torch.randint(0, 10, (4,)))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "### 5. Which statements are correct?\n",
        "\n",
        "- [x] A. The .zero_grad() prevents gradient accumulation.\n",
        "- [x] B. .backward() computes gradients using automatic differentiation.\n",
        "- [x] C. .step() updates parameters using gradients stored in each tensor.\n",
        "- [ ] D. Momentum = 0.9 reduces overfitting directly.\n",
        "\n",
        "Ans: Momentum = 0.9 reduces overfitting directly."
      ],
      "metadata": {
        "id": "7SlqQKAS6zu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torchvision.transforms as T\n",
        "transform = T.Compose([\n",
        "    T.RandomHorizontalFlip(p=0.5),\n",
        "    T.RandomRotation(15),\n",
        "    T.ToTensor()\n",
        "])\n",
        "```\n",
        "\n",
        "### 6. Select true statements regarding this transform pipeline.\n",
        "\n",
        "- [x] A. Each image has 50 % chance to be flipped horizontally.\n",
        "- [ ] B. Rotation preserves label semantics for classification tasks like CIFAR-10.\n",
        "- [x] C. ToTensor() converts PIL Image â†’ [0, 1] tensor.\n",
        "- [x] D. It can reduce overfitting by enriching training diversity.\n",
        "\n",
        "Ans: Rotation may distort label semantics for asymmetric objects (e.g., digits 6/9). Flipping + augmentation combats overfitting."
      ],
      "metadata": {
        "id": "rBh9Be5i7xpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "def receptive_field(kernel_sizes, strides):\n",
        "    rf, jump = 1, 1\n",
        "    for k, s in zip(kernel_sizes, strides):\n",
        "        rf = rf + (k - 1) * jump\n",
        "        jump *= s\n",
        "    return rf\n",
        "\n",
        "print(receptive_field([3,3,3], [1,1,2]))\n",
        "```\n",
        "\n",
        "### 7. What is the receptive field size of the final layer?\n",
        "\n",
        "Ans: 9\n",
        "\n",
        "After first two 3Ã—3 stride-1 layers â†’ rf = 5, jump = 1.\n",
        "Third layer (stride 2): rf = 5 + (3âˆ’1)*1 = 7 before jump update â†’ jump = 2.\n",
        "Actually final rf = 7 + (3âˆ’1)*1 = 9 pixels."
      ],
      "metadata": {
        "id": "UzBNxViw8FLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TinyCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, stride=2, padding=2)\n",
        "        self.bn1   = nn.BatchNorm2d(8)\n",
        "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1   = nn.Linear(16 * 8 * 8, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc1(x)\n",
        "\n",
        "model = TinyCNN()\n",
        "inp = torch.randn(4, 3, 32, 32)\n",
        "out = model(inp)\n",
        "```\n",
        "\n",
        "### 8. Which of the following are true?\n",
        "\n",
        "- [x] A. The feature map size before flattening is 16Ã—8Ã—8.\n",
        "- [x] B. BatchNorm introduces extra learnable parameters (Î³, Î²).\n",
        "- [ ] C. The receptive field of the output neuron covers the whole image.\n",
        "- [x] D. max_pool2d halves spatial dimensions.\n",
        "\n",
        "Ans: Stride 2 in conv1 â†’ 16Ã—16; pool 2Ã—2 â†’ 8Ã—8. BN adds 2Ã—C parameters. Receptive field < entire 32Ã—32 yet."
      ],
      "metadata": {
        "id": "MOtcyYdI8bvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
        "    def forward(self, x):\n",
        "        return F.relu(self.conv2(F.relu(self.conv1(x))) + x)\n",
        "```\n",
        "\n",
        "### 9. For this residual block:\n",
        "\n",
        "- [x] A. Shapes of input and output must match.\n",
        "- [x] B. Skip connection helps gradient flow.\n",
        "- [ ] C. Number of learnable parameters decreases compared to two independent conv layers.\n",
        "- [x] D. The block can model identity mapping easily.\n",
        "\n",
        "Ans: Residual adds input to output â†’ same shape; skip improves training and permits identity."
      ],
      "metadata": {
        "id": "gGytLY5e9GAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class NormedCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 6, 3)\n",
        "        self.bn   = nn.BatchNorm2d(6)\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.training:\n",
        "            x = self.bn(x)\n",
        "        return F.relu(x)\n",
        "```\n",
        "\n",
        "### 10. Which statements hold true?\n",
        "\n",
        "- [x] A. During evaluation mode, BN uses running mean/var.\n",
        "- [x] B. self.training flag must be False for inference.\n",
        "- [x] C. BN normalizes over spatial + batch dims per channel.\n",
        "- [ ] D. Turning off BN has no effect on output distribution.\n",
        "\n",
        "Ans: model.eval() sets training=False; BN uses stored stats; normalization is per-channel."
      ],
      "metadata": {
        "id": "pO167rY59wHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class GlobalPoolNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.gap  = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc   = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        x = self.gap(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.fc(x)\n",
        "```\n",
        "\n",
        "### 11. Choose the correct option.\n",
        "\n",
        "- [x] A. Adaptive pooling outputs size (1Ã—1) regardless of input dimension.\n",
        "- [ ] B. This architecture is translation invariant to object position.\n",
        "- [x] C. fc expects 64 inputs for each sample.\n",
        "- [x] D. Removing gap requires changing fc input size.\n",
        "\n",
        "Ans: GAP â†’ 1Ã—1 maps; fc needs 64 inputs; no strict translation invariance (so B false)."
      ],
      "metadata": {
        "id": "r6xXTXaM-FSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    def forward(self, x): return self.layers(x)\n",
        "\n",
        "m = DeeperCNN()\n",
        "x = torch.randn(1,3,32,32)\n",
        "y = m(x)\n",
        "```\n",
        "\n",
        "### 12. Which statements are correct?\n",
        "\n",
        "- [x] A. Depth increases effective receptive field.\n",
        "- [x] B. All conv layers preserve spatial size.\n",
        "- [x] C. Gradient vanishing is more likely than in a 2-layer CNN.\n",
        "- [ ] D. Each layerâ€™s activation map has 128 channels.\n",
        "\n",
        "Ans: Padding 1 keeps 32Ã—32; RF grows with depth; final map has 128 channels only at end, not each."
      ],
      "metadata": {
        "id": "z9Nq210q-jQI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}