{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. Consider a convolutional neural network where the input image is of size **128×128×3**, and the first convolutional layer uses **64 filters of size 7×7 with stride 2 and padding 3**. Which of the following correctly gives the output dimension of this convolutional layer?\n",
        "\n",
        "- [x] A. 64 × 64 × 64  \n",
        "- [ ] B. 64 × 128 × 128  \n",
        "- [ ] C. 64 × 63 × 63  \n",
        "- [ ] D. 64 × 62 × 62  \n",
        "\n",
        "**Ans:** Output dimension = (128 + 2×3 − 7)/2 + 1 = 64. Hence, 64×64 with 64 filters.The convolution formula ensures halving due to stride 2. With padding=3, the spatial resolution becomes 64×64. Depth equals number of filters = 64."
      ],
      "metadata": {
        "id": "0R1orU2GJsm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. CNNs exploit spatial locality and parameter sharing. Which of the following statements are correct with respect to these principles?\n",
        "\n",
        "- [x] A. Parameter sharing means the same kernel weights slide across different spatial regions of the input.  \n",
        "- [ ] B. Local connectivity ensures that each neuron is connected to all the input pixels.  \n",
        "- [x] C. Increasing kernel size reduces the degree of spatial locality assumption.  \n",
        "- [x] D. Parameter sharing is the reason CNNs require fewer parameters compared to fully connected networks.  \n",
        "\n",
        "**Ans:** Local connectivity means small local regions, not all pixels. Sharing kernels across the image reduces parameter count. Larger kernels imply a weaker assumption of strict locality."
      ],
      "metadata": {
        "id": "GlzQE7DeXIUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 3. You apply a 3×3 kernel with stride 1 and no padding to a 32×32 image. How many parameters does this convolutional layer have if it uses 16 filters and the input has 3 channels? (Ignore bias terms).\n",
        "\n",
        "**Ans:** 432\n",
        "\n",
        "Each filter has 3×3×3 parameters, and there are 16 such filters. Parameters depend only on kernel size and input depth, not on spatial dimensions."
      ],
      "metadata": {
        "id": "b8s0RVS3XTLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Pooling layers in CNNs are often said to contribute to translation invariance. Which of the following best describes this effect?\n",
        "\n",
        "- [ ] A. Pooling ensures the CNN is completely invariant to all types of affine transformations.  \n",
        "- [x] B. Pooling discards exact positional information but retains dominant activations, so small shifts in the input cause similar outputs.  \n",
        "- [ ] C. Pooling performs normalization across channels to ensure stability.  \n",
        "- [ ] D. Pooling acts as a regularization technique by enforcing sparsity.  \n",
        "\n",
        "**Ans:** Pooling creates robustness to small translations since nearby activations result in similar pooled outputs, but it does not handle all transformations."
      ],
      "metadata": {
        "id": "Nd7JE5T9Xfuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. Consider the receptive field of neurons in deeper CNN layers. Which statements are true?\n",
        "\n",
        "- [x] A. Receptive field always grows with depth.  \n",
        "- [x] B. A neuron in deeper layers “sees” a larger portion of the input image.  \n",
        "- [ ] C. If stride is large, receptive field growth is slower.  \n",
        "- [x] D. Stacking multiple small kernels (e.g., 3×3) can achieve the same receptive field as one large kernel (e.g., 7×7) with fewer parameters.  \n",
        "\n",
        "**Ans:** Depth increases effective receptive field. Stride actually increases receptive field growth. Multiple 3×3 kernels approximate larger kernels while using fewer parameters."
      ],
      "metadata": {
        "id": "6sTyiNWoXt5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. Suppose you stack **two convolutional layers** with kernel size 3×3, stride 1, padding 1. What is the effective receptive field size of a neuron in the second layer with respect to the input image?\n",
        "\n",
        "- [ ] A. 3x3\n",
        "- [x] B. 5x5\n",
        "- [ ] C. 7x7\n",
        "- [ ] D. 1x1\n",
        "\n",
        "**Ans:** Each 3×3 covers its own local region. Two stacked layers expand the effective receptive field by two pixels on each side, yielding 5×5."
      ],
      "metadata": {
        "id": "COu2yuLtX1ys"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. Batch Normalization is frequently applied after convolutional layers. Which of the following is the most accurate reasoning for its utility in CNNs?\n",
        "\n",
        "- [ ] A. It prevents overfitting by reducing model complexity.  \n",
        "- [x] B. It normalizes activations across spatial dimensions, allowing faster training and more stable gradients.  \n",
        "- [ ] C. It increases representational power by introducing more parameters.  \n",
        "- [ ] D. It replaces dropout completely as a regularization technique.  \n",
        "\n",
        "**Ans:** BatchNorm stabilizes gradient flow and training by normalizing activations, making optimization easier and reducing sensitivity to initialization."
      ],
      "metadata": {
        "id": "hSNiwJlxYPQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 8. You must choose among valid, same (zero), and reflect padding for early layers on natural images. Which statement best captures their frequency-domain implications on edge artifacts and bias near borders?\n",
        "\n",
        "- [ ] A. Valid padding preserves stationarity across the field and avoids any boundary bias.\n",
        "- [x] B. Same (zero) padding introduces strong artificial low-frequency bias near edges because zeros act as a dark frame, corrupting local statistics.\n",
        "- [ ] C. Reflect padding better preserves local second-order statistics near borders compared to zero padding.\n",
        "- [x] D. All padding choices are equivalent after sufficient depth.\n",
        "\n",
        "**Ans:** A: valid reduces field size and induces border bias by shrinking coverage; D: artifacts can persist and affect learning."
      ],
      "metadata": {
        "id": "peFVX-JrYgj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 9. You train a CNN for object classification on natural images. The feature extractor is a stack of strided convolutions with ReLU and no padding in early layers, followed by global average pooling (GAP) and a linear classifier. Consider the following statements about translation behavior:\n",
        "\n",
        "- [ ] A single convolution with stride 1 and circular padding is exactly translation equivariant to pixel shifts under the group action.\n",
        "- [x] In practice, boundary handling and pooling/striding break exact translation equivariance; GAP partially recovers translation invariance at the representation level.\n",
        "- [ ] C. Max pooling increases translation equivariance compared to stride-2 convolution with the same receptive field.\n",
        "- [x] Sub-pixel (fractional) translations of the input typically produce aliasing after striding/pooling unless pre-filtering (low-pass) is used.\n",
        "\n",
        "**Ans:** B is correct; equivariance is broken by padding/striding/pooling; GAP promotes invariance in late layers. D is correct due to sampling theory and aliasing. A is only true under circular padding and infinite support assumptions throughout the stack, not typical pipelines; C is false—max pooling generally reduces exact equivariance similarly to striding."
      ],
      "metadata": {
        "id": "UvQOhm-NZTeB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 10. Consider two CNNs: one uses max pooling, the other uses strided convolution for downsampling. Which of the following best compares their roles?\n",
        "\n",
        "- [ ] A. Max pooling introduces additional parameters compared to strided convolution.  \n",
        "- [x] B. Strided convolution learns the downsampling operation, while max pooling is fixed.  \n",
        "- [ ] C. Max pooling retains more information than strided convolution.  \n",
        "- [ ] D. Strided convolution cannot reduce resolution effectively.  \n",
        "\n",
        "**Ans:** Pooling is deterministic, whereas strided convolution learns kernels to perform downsampling, thus being more flexible."
      ],
      "metadata": {
        "id": "w0X0xlPaZ0XP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 11. You pass a 224×224×3 image through a CNN with the following layers:  \n",
        "1. Conv: 7×7, stride 2, padding 3  \n",
        "2. Max Pool: 3×3, stride 2, padding 1  \n",
        "\n",
        "What is the output spatial dimension after these two layers (ignore channels)? Please note that the answer will be in the form of `a X a`. Please put the value of `a`.\n",
        "\n",
        "**Ans:** 56. First conv reduces to 112×112. Then pooling reduces further to 56×56. Formula applied step-by-step."
      ],
      "metadata": {
        "id": "Pyf5_odvaEqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "\n",
        "### 12. Which statement best captures why residual connections help very deep CNNs?\n",
        "\n",
        "- [ ] A. They widen the network, increasing representational capacity quadratically in depth.\n",
        "- [x] B. They provide identity shortcuts so the optimization can learn residual functions; the Jacobian’s eigenvalues are biased toward 1, mitigating vanishing/exploding gradients.\n",
        "- [ ] C. They reduce the number of parameters and hence regularize implicitly, the main effect.\n",
        "- [ ] D. They enforce exact invariance to additive noise in inputs.\n",
        "\n",
        "**Ans:** The residual connections were mainly introduced in deep CNNs, so that it will help to mitigate the problem of vanishing or exploding gradients. It is because in very deep layers it might have the tendency to forget the accumulated gradients."
      ],
      "metadata": {
        "id": "IGFj_j5GbHej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding questions (Total: 12)"
      ],
      "metadata": {
        "id": "Ce5Pqo_sMWCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class OddPaddingConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OddPaddingConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16,\n",
        "                               kernel_size=5, stride=2, padding=0)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.fc = nn.Linear(32*7*7, 10)  # assume input is 3x32x32\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = F.relu(self.bn1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.bn2(out))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return self.fc(out)\n",
        "\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "model = OddPaddingConvNet()\n",
        "out = model(x)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 1. Which of the following are true?\n",
        "\n",
        "- [ ] A. The Linear layer input size is incorrectly specified; it will throw an error.\n",
        "- [x] B. After conv2, the feature map size is 7x7.\n",
        "- [ ] C. Zero padding in conv1 makes translation equivariance exact at edges.\n",
        "- [x] D. After conv1, the feature map size is 14x14.\n",
        "\n",
        "Ans: Conv1: (32−5)/2+1=14. Conv2: same padding, stride=2, yields 7. Linear matches 32∗7∗7. But equivariance is broken at borders."
      ],
      "metadata": {
        "id": "SgGDJTSBMb54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return F.relu(out)\n",
        "\n",
        "x = torch.randn(2, 16, 32, 32)\n",
        "block = ResidualBlock()\n",
        "y = block(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 2. Which statements are correct?\n",
        "\n",
        "- [x] A. The residual connection ensures gradient bypass, reducing vanishing gradient risk.\n",
        "- [ ] B. Removing BN would not affect gradient flow stability.\n",
        "- [x] C. y.shape is (2, 16, 32, 32).\n",
        "- [ ] D. Skip connection doubles parameter count.\n",
        "\n",
        "Ans: BN stabilizes gradients, skip aids flow. Params aren’t doubled, since skip adds no learnable weights."
      ],
      "metadata": {
        "id": "_m15WwAtjku3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "bn = nn.BatchNorm2d(8)\n",
        "x_train = torch.randn(10, 8, 32, 32)\n",
        "bn.train()\n",
        "out_train = bn(x_train)\n",
        "bn.eval()\n",
        "x_test = torch.randn(2, 8, 32, 32)\n",
        "out_test = bn(x_test)\n",
        "```\n",
        "\n",
        "### 3. Which are true?\n",
        "\n",
        "- [ ] A. In train mode, running_mean is not updated.\n",
        "- [x] B. In eval mode, normalization uses running_mean/var not current batch.\n",
        "- [x] C. Small batch size at test does not affect outputs.\n",
        "- [ ] D. Train-time always uses stored running stats.\n",
        "\n",
        "Ans: Train: uses batch stats; updates running stats. Eval: uses running stats, small batch irrelevant."
      ],
      "metadata": {
        "id": "32ABZe2JkDn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class SmallCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 16, 3, stride=2)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(16, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv(x))\n",
        "        pooled = self.gap(out)\n",
        "        return self.fc(pooled.view(out.size(0), -1))\n",
        "\n",
        "x = torch.randn(4, 3, 32, 32)\n",
        "model = SmallCNN()\n",
        "print(model(x).shape)\n",
        "```\n",
        "\n",
        "### 4. Which statements are correct?\n",
        "\n",
        "- [x] A. GAP outputs shape (4,16,1,1).\n",
        "- [x] B. GAP encourages translation invariance.\n",
        "- [ ] C. Replacing GAP with Flatten would reduce parameters.\n",
        "- [x] D. GAP avoids dependence on input resolution."
      ],
      "metadata": {
        "id": "pVhvwdyKkYrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "drop = nn.Dropout(p=0.5)\n",
        "x = torch.ones(10, 10)\n",
        "drop.train()\n",
        "y = drop(x)\n",
        "```\n",
        "\n",
        "### 5. Which are correct?\n",
        "\n",
        "- [x] A. About 50% of entries become zero.\n",
        "- [x] B. Remaining entries are scaled by 2.\n",
        "- [ ] C. In eval mode, outputs are half of train mode.\n",
        "- [ ] D. Dropout is applied channel-wise only."
      ],
      "metadata": {
        "id": "ACYA3Ys-kvOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "convT = nn.ConvTranspose2d(16, 8, kernel_size=4, stride=2, padding=1)\n",
        "x = torch.randn(1, 16, 8, 8)\n",
        "y = convT(x)\n",
        "print(y.shape)\n",
        "```\n",
        "\n",
        "### 6. What is the output spatial size? If the answer is in the form `a x a`. Please enter the value of `a`.\n",
        "\n",
        "Ans: 16\n",
        "\n",
        "Formula: (8-1)2 - 2*1 + 4 = 16"
      ],
      "metadata": {
        "id": "xh0X3wUhlM9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "model = nn.Conv2d(3, 8, 3)\n",
        "opt = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "for epoch in range(2):\n",
        "    x = torch.randn(4, 3, 32, 32)\n",
        "    y = torch.randint(0, 8, (4,))\n",
        "    opt.zero_grad()\n",
        "    out = model(x).mean(dim=(2,3))\n",
        "    loss = F.cross_entropy(out, y)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "```\n",
        "\n",
        "### 7. Which statements are correct?\n",
        "\n",
        "- [ ] A. Model trains on raw pixel values without nonlinearities.\n",
        "- [x] B. The loss is computed on spatially averaged logits.\n",
        "- [x] C. `opt.zero_grad()` is necessary to avoid gradient accumulation.\n",
        "- [ ] D. Conv2d auto-applies softmax internally.\n"
      ],
      "metadata": {
        "id": "XTbK29WMl4Uy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN()\n",
        "inp = torch.randn(4, 3, 64, 64)   # batch of 4 images\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 8. What is the correct output shape of `out`?\n",
        "\n",
        "- [ ] A. (4, 32, 16, 16)\n",
        "- [x] B. (4, 32, 16, 16)\n",
        "- [ ] C. (4, 32, 32, 32)\n",
        "- [ ] D. (4, 32, 64, 64)\n",
        "\n",
        "Ans: Conv1 halves size (64→32). Conv2 keeps (32→32). MaxPool halves again (32→16). Channels = 32."
      ],
      "metadata": {
        "id": "5rN7v5rBmW9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class DeeperCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(8, 16, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        return x\n",
        "\n",
        "model = DeeperCNN()\n",
        "inp = torch.randn(1, 3, 32, 32)\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 9. Why does the output retain 32×32 spatial size even after 3 convolutions?\n",
        "\n",
        "- [x] A. Because stride=1 with padding=1 preserves spatial dimensions.\n",
        "- [ ] B. Because filters expand size.\n",
        "- [ ] C. Because PyTorch automatically resizes inputs.\n",
        "- [x] D. Because zero-padding compensates the border effect.\n",
        "\n",
        "Ans: With stride 1 and padding = kernel//2, conv is \"same\" size. PyTorch does not auto-resize; explicit padding ensures no shrinkage."
      ],
      "metadata": {
        "id": "NowhZEkQmrH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "conv = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1, padding=0, bias=False)\n",
        "print(sum(p.numel() for p in conv.parameters()))\n",
        "```\n",
        "\n",
        "### 10. How many parameters are there (ignore bias)?\n",
        "\n",
        "Ans: 450. Each filter: 3×25 params. 6 filters = 450. Bias is off."
      ],
      "metadata": {
        "id": "0C1qTQlrnKXb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class WeirdCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(3, 3, kernel_size=1, stride=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "inp = torch.randn(2, 3, 32, 32)\n",
        "model = WeirdCNN()\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 11. Why does `WeirdCNN` act like a channel-wise linear projection?\n",
        "\n",
        "- [x] A. 1×1 convolution mixes channels without changing spatial size.\n",
        "- [ ] B. 1×1 convolution increases receptive field.\n",
        "- [ ] C. 1×1 convolution discards spatial info completely.\n",
        "- [ ] D. It is identical to pooling."
      ],
      "metadata": {
        "id": "aYqFdYL8nf_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "class MultiConv(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Conv2d(3, 4, 1)\n",
        "        self.branch2 = nn.Conv2d(3, 4, 3, padding=1)\n",
        "        self.branch3 = nn.Conv2d(3, 4, 5, padding=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x)], dim=1)\n",
        "\n",
        "model = MultiConv()\n",
        "inp = torch.randn(1,3,32,32)\n",
        "out = model(inp)\n",
        "print(out.shape)\n",
        "```\n",
        "\n",
        "### 12. What is the output channel count?\n",
        "\n",
        "Ans: 12. Each branch outputs 4 channels. Concatenation along dim=1 → 4+4+4=12."
      ],
      "metadata": {
        "id": "sC2z3AsZoA2q"
      }
    }
  ]
}