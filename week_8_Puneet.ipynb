{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5ggp7DNnMOLn",
        "Ce5Pqo_sMWCd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. In the context of memory-based architectures, which distinction best summarizes the difference between explicit and implicit memory mechanisms in recurrent neural networks, as described for LSTM and GRU.\n",
        "\n",
        "- [x] A. LSTM retains prior cell calculations via explicit memory slots, whereas GRU's memory is internally managed by update/reset gates without separate storage.\n",
        "- [ ] B. Both LSTM and GRU use explicit memory, but LSTM uses more gates.\n",
        "- [ ] C. GRU manages memory with attention layers, whereas LSTM does not use gates for retention.\n",
        "- [ ] D. LSTM relies on weight matrices for state recall, GRU uses activation functions alone.\n",
        "\n",
        "Ans: LSTM features explicit memory cells (the cell state) that persist over time and are controlled by gates. GRU combines gating and state management, lacking a dedicated memory cell, so its memory is considered implicit."
      ],
      "metadata": {
        "id": "2hvCqnhJ2M0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 2. Bidirectional RNNs are introduced to overcome the limitations of standard RNNs in tasks where future context is essential. Which task would most likely highlight the advantage of bidirectional processing?\n",
        "\n",
        "- [x] A. Named entity recognition, where predicting the label for a word may depend on both previous and subsequent words in the sentence.\n",
        "- [ ] B. Naive sentiment aggregation, where only historical input matters.\n",
        "- [ ] C. Time series forecasting for financial data.\n",
        "- [ ] D. Image classification using CNN layers.\n",
        "\n",
        "Ans: Bidirectional RNNs consider information from both left and right context, crucial for tasks like named entity recognition where context isn't limited to previous words."
      ],
      "metadata": {
        "id": "-JfvlYzy2laF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 3. In a typical encoder-decoder architecture for sequence-to-sequence tasks (e.g., translation or summarization), which statements are valid?\n",
        "\n",
        "- [x] A. The encoder compresses the input sequence into a context vector for the decoder.\n",
        "- [ ] B. The decoder utilizes only the final token from the encoder to generate output.\n",
        "- [x] C. The model is trained with teacher forcing, feeding ground-truth tokens during training.\n",
        "- [ ] D. Output sequence and input sequence lengths must match.\n",
        "\n",
        "Ans: The encoder compresses the source to a context vector (A), and teacher forcing supplies ground truths during training (C). Decoder uses the full encoder output, not only the final token (B is incorrect), and input/output sequence lengths need not match (D is incorrect)."
      ],
      "metadata": {
        "id": "FySPDbmI292P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Which mechanism in LSTM helps mitigate the vanishing gradient problem that often plagues vanilla RNNs?\n",
        "\n",
        "- [x] A. The introduction of gates that regulate the flow of information and gradients.\n",
        "- [ ] B. The use of dropout within recurrent units.\n",
        "- [ ] C. Data normalization during pre-processing.\n",
        "- [ ] D. Gradient clipping during training.\n",
        "\n",
        "Ans: LSTM's gating architecture controls the flow of signals and gradients, preserving information and mitigating vanishing gradients."
      ],
      "metadata": {
        "id": "wEDSR_vHbqxM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. BLEU (Bilingual Evaluation Understudy) is described as a metric for evaluating sequence generation. What does it penalize or affect the score with?\n",
        "\n",
        "- [x] A. High n-gram precision between candidate and reference sequence.\n",
        "- [x] B. Overly short candidate sequences via brevity penalties.\n",
        "- [ ] C. Use of rare vocabulary words in the output.\n",
        "- [ ] D. Matching sequence lengths alone.\n",
        "\n",
        "Ans: BLEU checks for n-gram precision (A) and penalizes overly short translations using a brevity penalty (B). Sequence lengths and rare words are not direct scoring components (C, D incorrect)."
      ],
      "metadata": {
        "id": "UwuN3DGncCQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. Teacher forcing in seq2seq training helps stabilize learning. What is a potential risk associated with teacher forcing when performing inference?\n",
        "\n",
        "- [x] A. During inference, the model generates sequences without access to ground-truth tokens, so errors can accumulate step-by-step.\n",
        "- [ ] B. During inference, training and inference loss values remain identical.\n",
        "- [ ] C. The approach delays gradient computation until evaluation.\n",
        "- [ ] D. Teacher forcing skips backpropagation in recurrent layers.\n",
        "\n",
        "Ans: During inference, the model must rely on its own prior predictions, so initial mistakes can propagate without ground-truth corrections."
      ],
      "metadata": {
        "id": "P5T-ObhfcSn1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 7. Which limitations of standard RNNs are specifically addressed by LSTM and GRU architectures?\n",
        "\n",
        "- [x] A. Difficulty with long-term dependency retention.\n",
        "- [ ] B. Fixed input-output lengths for all sequence tasks.\n",
        "- [x] C. Gradient instability due to vanishing and exploding gradients.\n",
        "- [x] D. Lack of selective memory updates.\n",
        "\n",
        "LSTM/GRU address long-term dependency issues (A), mitigate gradient instability (C), and introduce selective memory update mechanisms (D). Fixed length (B) isn't directly addressed by these architectures."
      ],
      "metadata": {
        "id": "1-irPOqtckdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 8. In the preprocessing pipeline described for sentiment analysis, padding is utilized for what purpose in batch-based training?\n",
        "\n",
        "- [x] A. To allow processing of variable-length sequences by making their lengths uniform.\n",
        "- [ ] B. To enhance n-gram scoring.\n",
        "- [ ] C. To increase vocabulary diversity in the batch.\n",
        "- [ ] D. To simplify the labeling process.\n",
        "\n",
        "Ans: Padding makes all input sequences the same length, enabling efficient batch training in frameworks that require consistent tensor shapes."
      ],
      "metadata": {
        "id": "HFrk0gHyc9Ut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. When encoding reviews as integer sequences for the RNN/LSTM model, which role does the integer '0' play in vocabulary mapping?\n",
        "\n",
        "- [x] A. It acts as a reserved index for padding tokens in the encoded sequence.\n",
        "- [ ] B. It is assigned to unknown or out-of-vocabulary words.\n",
        "- [ ] C. It indicates the start of the sequence.\n",
        "- [ ] D. It marks sentiment class boundaries.\n",
        "\n",
        "Ans: Padding is required to standardize sequence lengths, and 0 is typically reserved for this role."
      ],
      "metadata": {
        "id": "AC73JjngdOVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 10. What is the primary class distribution for the labeled sentiment analysis dataset?\n",
        "\n",
        "- [x] A. 50% positive and 50% negative reviews\n",
        "- [ ] B. 75% positive and 25% negative reviews\n",
        "- [ ] C. 60% positive and 40% negative reviews\n",
        "- [ ] D. 90% positive and 10% negative reviews\n",
        "\n",
        "Ans: The dataset is typically balanced in class distribution for effective binary classification."
      ],
      "metadata": {
        "id": "c99Sk-iDdg0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 11. Which common preprocessing step/s is performed before tokenization in the provided code?\n",
        "\n",
        "- [x] A. Case normalization\n",
        "- [x] B. Punctuation removal\n",
        "- [ ] C. Sequence encoding\n",
        "- [ ] D. Vocabulary expansion\n",
        "\n",
        "Ans: Processing steps like lowercasing and removing punctuation help standardize input before tokenization."
      ],
      "metadata": {
        "id": "SCV_KBN2d1r-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Questions (Total: 12)"
      ],
      "metadata": {
        "id": "tZxXjvihqhkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class CustomGRU(nn.Module):\n",
        "    def __init__(self, input_size=10, hidden_size=32, num_layers=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "    def forward(self, x, h0=None):\n",
        "        output, hn = self.gru(x, h0)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out, hn\n",
        "x = torch.randn(8, 20, 10)\n",
        "h0 = torch.zeros(2, 8, 32)\n",
        "model = CustomGRU()\n",
        "y, hn = model(x, h0)\n",
        "```\n",
        "\n",
        "### 1. What is the shape of the hidden state `hn` returned from the forward method?\n",
        "\n",
        "- [x] A. (2, 8, 32)\n",
        "- [ ] B. (8, 32)\n",
        "- [ ] C. (1, 8, 32)\n",
        "- [ ] D. (8, 20, 1)\n",
        "\n",
        "Ans: For GRU, `hn` shape is (num_layers, batch, hidden_size). Here: 2 layers, batch=8, hidden_size=32."
      ],
      "metadata": {
        "id": "UXs3xma7vWFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch import nn\n",
        "lstm = nn.LSTM(input_size=7, hidden_size=16, num_layers=3, bidirectional=True)\n",
        "x = torch.randn(5, 12, 7) # seq_len=12, batch=5, features=7\n",
        "h0 = torch.zeros(6, 5, 16)\n",
        "c0 = torch.zeros(6, 5, 16)\n",
        "out, (hn, cn) = lstm(x, (h0, c0))\n",
        "```\n",
        "\n",
        "### 2. Given the above setup, which of the following statements are correct?\n",
        "\n",
        "- [ ] A. The LSTM output tensor out is shaped (12, 5, 32).\n",
        "- [x] B. The number of layers in hn is 6 because of bidirectional LSTM.\n",
        "- [x] C. The hidden state is doubled due to bidirectionality.\n",
        "- [ ] D. The first axis of hn always matches the batch size.\n",
        "\n",
        "Ans: Bidirectional=True, so layers doubled: 3*2=6. Hidden/cell states are stacked, making hn/cn shape (6,5,16). Output feature dim also doubles."
      ],
      "metadata": {
        "id": "HCIMCv0yvq4F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "class SequenceClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(4, 8, 1, batch_first=True)\n",
        "        self.fc = nn.Linear(8, 2)\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "x = torch.randn(32, 15, 4)\n",
        "model = SequenceClassifier()\n",
        "y = model(x)\n",
        "```\n",
        "\n",
        "### 3. What does `out[:, -1, :]` select from the RNN output?\n",
        "\n",
        "- [x] A. Last time-step output vector for each sequence in the batch.\n",
        "- [ ] B. Random sample from the batch.\n",
        "- [ ] C. All outputs across time steps.\n",
        "- [ ] D. Initial hidden state before sequence starts.\n",
        "\n",
        "Ans:  Index `-1` picks the last element in the time-dimension for each batch sample. This is common before classification."
      ],
      "metadata": {
        "id": "B6MDwiKev_9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class DeepLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(6, 12, num_layers=2, batch_first=True)\n",
        "        self.fc = nn.Linear(12, 1)\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :]), hn, cn\n",
        "x = torch.randn(10, 16, 6)\n",
        "```\n",
        "\n",
        "### 4. Which are true about the shapes of `out`, `hn`, and the output of `fc`?\n",
        "\n",
        "- [ ] A. `out` is (10, 16, 12)\n",
        "- [x] B. `hn` is (2, 10, 12)\n",
        "- [x] C. Output of `fc` is (10, 1)\n",
        "- [ ] D. `hn` has batch size and seq_len swapped.\n",
        "\n",
        "Ans: hn shape is (num_layers, batch, hidden); fc projects last time-step per sample to output (batch, 1)."
      ],
      "metadata": {
        "id": "V1JAz7XIwV_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "x = torch.randn(24, 30, 15)\n",
        "model = nn.LSTM(15, 64, 1, batch_first=True)\n",
        "out, _ = model(x)\n",
        "prob = F.softmax(out[:, -1, :], dim=-1)\n",
        "```\n",
        "\n",
        "### 5. What does `prob` represent?\n",
        "\n",
        "- [x] A. Normalized probabilities computed over features of last time step per batch.\n",
        "- [ ] B. Probabilities for entire sequence length.\n",
        "- [ ] C. Output logits for all time steps.\n",
        "- [ ] D. Raw hidden state values.\n",
        "\n",
        "Ans: Softmax is applied only for last time step features, so `prob` is a batch of distributions over hidden features."
      ],
      "metadata": {
        "id": "b8v9e-xDwufF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "cell = nn.LSTMCell(9, 17)\n",
        "x = torch.randn(5, 9)\n",
        "hx = torch.zeros(5, 17)\n",
        "cx = torch.zeros(5, 17)\n",
        "for i in range(11):\n",
        "    hx, cx = cell(x, (hx, cx))\n",
        "```\n",
        "\n",
        "### 6. Which are true regarding this LSTMCell usage?\n",
        "\n",
        "- [x] A. Each iteration simulates a time step in sequence processing.\n",
        "- [ ] B. Inputs to the cell stay constant here, so output doesn't change meaningfully.\n",
        "- [x] C. This pattern is suitable for custom sequence iteration.\n",
        "- [ ] D. Output batch size would change if cell parameters differed.\n",
        "\n",
        "Ans: Each loop mimics stepping through a sequence, with batch size constant, and custom control over inputs possible."
      ],
      "metadata": {
        "id": "QZ9eITMaxDvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class BidirectionalLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(14, 20, batch_first=True, bidirectional=True)\n",
        "    def forward(self, x):\n",
        "        o, _ = self.lstm(x)\n",
        "        return o\n",
        "x = torch.randn(9, 50, 14)\n",
        "model = BidirectionalLSTM()\n",
        "out = model(x)\n",
        "```\n",
        "\n",
        "### 7. What is the shape of `out`?\n",
        "\n",
        "- [x] A. (9, 50, 40)\n",
        "- [ ] B. (9, 50, 20)\n",
        "- [ ] C. (9, 100, 14)\n",
        "- [ ] D. (9, 50, 14)\n",
        "\n",
        "Ans: Bidirectional doubles the hidden size. The output shape is (batch, seq_len, hidden_size)."
      ],
      "metadata": {
        "id": "AE1jBtO2xuhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class TimeSeriesRNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(3, 4, 1, nonlinearity='relu', batch_first=True)\n",
        "        self.fc = nn.Linear(4, 1)\n",
        "    def forward(self, x):\n",
        "        o, hn = self.rnn(x)\n",
        "        y = self.fc(o[:, -1, :])\n",
        "        return y\n",
        "x = torch.randn(7, 29, 3)\n",
        "```\n",
        "\n",
        "### 8. We have specified `nonlinearity='relu'`. How does it affect the `TimeSeriesRNN` function?\n",
        "\n",
        "- [x] A. Activations inside RNN will use ReLU instead of tanh for hidden state updates.\n",
        "- [ ] B. Output shape changes to account for ReLU.\n",
        "- [ ] C. Model will always converge faster.\n",
        "- [ ] D. RNN input size must match output size.\n",
        "\n",
        "Ans: Relu alters internal activation dynamics in RNN units; shape and convergence depend on other configs."
      ],
      "metadata": {
        "id": "_oKpAXBfyxq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch.nn as nn\n",
        "model = nn.LSTM(input_size=8, hidden_size=16, batch_first=True)\n",
        "x = torch.randn(10, 25, 8)\n",
        "out, (hn, cn) = model(x)\n",
        "```\n",
        "\n",
        "### 9. What happens if you change `batch_first=False` in the LSTM?\n",
        "\n",
        "- [x] A. Input should be (seq_len, batch, input_size).\n",
        "- [ ] B. Input should be (batch, seq_len, input_size).\n",
        "- [ ] C. Hidden state has incorrect shape.\n",
        "- [ ] D. Only bidirectional LSTM is affected.\n",
        "\n",
        "Ans: Setting `batch_first=False` expects input with sequence dimension leading, (seq_len, batch, input_size)."
      ],
      "metadata": {
        "id": "HqLh6wJqzL3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return torch.tensor(xs).float(), torch.tensor(ys).float()\n",
        "series = torch.arange(31)\n",
        "X, y = create_sequences(series, seq_length=5)\n",
        "```\n",
        "\n",
        "### 10. What is the length of the target tensor `y` produced?\n",
        "\n",
        "Ans: 26\n",
        "\n",
        "For a series of length 31 and seq_length=5, you get (31-5) = 26 targets."
      ],
      "metadata": {
        "id": "n1-PkF6j0hf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from torch import nn\n",
        "model = nn.RNN(10, 20, batch_first=True)\n",
        "x = torch.randn(4, 6, 10)\n",
        "out, hn = model(x)\n",
        "```\n",
        "\n",
        "### 11. What will be the shape of `out`?\n",
        "\n",
        "- [x] A. (4, 6, 20)\n",
        "- [ ] B. (6, 4, 20)\n",
        "- [ ] C. (4, 20, 6)\n",
        "- [ ] D. (4, 6)\n",
        "\n",
        "Ans: With `batch_first=True`, output shape matches input batch and sequence dims."
      ],
      "metadata": {
        "id": "KK6PSd9m0tWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "x = torch.randn(32, 10, 5)\n",
        "lstm = nn.LSTM(5, 7, batch_first=True)\n",
        "out, (hn, cn) = lstm(x)\n",
        "```\n",
        "\n",
        "### 12. Which parameter controls the output feature dimension for each time step?\n",
        "\n",
        "- [x] A. hidden_size\n",
        "- [ ] B. batch_first\n",
        "- [ ] C. input_size\n",
        "- [ ] D. num_layers\n",
        "\n",
        "Ans: `hidden_size` sets the dimension of each time step's output for LSTM/RNN/GRU layers."
      ],
      "metadata": {
        "id": "T66oH8QZ1CSG"
      }
    }
  ]
}