{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 15)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 1. You build a 3-layer fully connected neural network in PyTorch using `torch.nn.Sequential`, with ReLU activations in the hidden layers and Sigmoid activation in the output layer. Consider the following statements about training dynamics:\n",
        "\n",
        "- [ ] A. Gradient vanishing is impossible because ReLU completely avoids it.\n",
        "- [x] B. The Sigmoid at the output layer can cause gradients to vanish if the inputs are large in magnitude.\n",
        "- [ ] C. Using ReLU ensures both vanishing and exploding gradients are eliminated.\n",
        "- [x] D. Combining ReLU hidden layers with a Sigmoid output may still result in vanishing gradients in earlier layers."
      ],
      "metadata": {
        "id": "0R1orU2GJsm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. In PyTorch, if you define a `torch.nn.Linear` layer but forget to specify a non-linear activation afterwards:\n",
        "\n",
        "- [ ] A. PyTorch will throw a runtime error.\n",
        "- [x] B. The layer behaves as a purely linear transformation.\n",
        "- [x] C. The overall network may degenerate into a linear function if no other nonlinearities exist.\n",
        "- [x] D. The model still learns, but its expressive capacity is severely limited."
      ],
      "metadata": {
        "id": "s90DCltIKWry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "Consider a **2-layer feedforward neural network** in PyTorch:\n",
        "\n",
        "- Input vector:  \n",
        "  $$\n",
        "  x = \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- First layer: `Linear(2,2)` with weights  \n",
        "  $$\n",
        "  W^{[1]} = \\begin{bmatrix} 1 & -2 \\\\ 0 & 1 \\end{bmatrix}, \\quad b^{[1]} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- Activation: **tanh**\n",
        "\n",
        "- Second layer: `Linear(2,1)` with weights  \n",
        "  $$\n",
        "  W^{[2]} = \\begin{bmatrix} 2 & -1 \\end{bmatrix}, \\quad b^{[2]} = [0]\n",
        "  $$\n",
        "\n",
        "- Output activation: **Sigmoid**\n",
        "\n",
        "- Target: $ y = 1 $, loss = **binary cross-entropy**.\n",
        "\n",
        "### 3. Compute the gradient of the loss with respect to the weights of the second layer, $\\frac{\\partial L}{\\partial W^{[2]}}$. Finally, report the sum of all elements of this gradient vector.\n",
        "\n",
        "Ans: −0.238"
      ],
      "metadata": {
        "id": "859Nz-oLKcZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Which PyTorch optimizer is generally the least sensitive to learning rate tuning?\n",
        "\n",
        "- [ ] A. `torch.optim.SGD`\n",
        "- [ ] B. `torch.optim.RMSprop`\n",
        "- [x] C. `torch.optim.Adam`\n",
        "- [ ] D. `torch.optim.Adagrad`"
      ],
      "metadata": {
        "id": "iiQDKC_OC3lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. Consider two models trained on the same dataset: one with `torch.optim.SGD`, another with `torch.optim.Adam`. Which statements are correct?\n",
        "\n",
        "- [x] A. Adam adapts the learning rate for each parameter based on historical gradients.\n",
        "- [x] B. SGD with momentum can approximate Adam’s performance in some tasks.\n",
        "- [ ] C. Adam always converges faster than SGD in all scenarios.\n",
        "- [x] D. SGD may generalize better than Adam in large-scale classification tasks."
      ],
      "metadata": {
        "id": "00HyWeqwDSmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. You experiment with different hidden-layer activations in PyTorch. Which is most prone to dead neurons?\n",
        "\n",
        "- [ ] A. `Sigmoid`\n",
        "- [ ] B. `Tanh`\n",
        "- [x] C. `ReLU`\n",
        "- [ ] D. `LeakyReLU`"
      ],
      "metadata": {
        "id": "ycGXj6CKDmRL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. Suppose you implement a classifier with `torch.nn.Softmax(dim=1)` at the output. Which of the following are true?\n",
        "\n",
        "- [x] A. The output probabilities always sum to 1.\n",
        "- [x] B. This is suitable for multi-class single-label problems.\n",
        "- [ ] C. This is suitable for multi-class multi-label problems.\n",
        "- [x] D. The most commonly used loss function for such problems is `torch.nn.CrossEntropyLoss`."
      ],
      "metadata": {
        "id": "AJW-oS85EHYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "Consider a **3-hidden-layer PyTorch network** with width 2 and **sigmoid activation** everywhere.\n",
        "\n",
        "- Input:  \n",
        "  $$\n",
        "  x = \\begin{bmatrix} 2 \\\\ -2 \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "- All weights initialized as identity matrices, biases = 0.\n",
        "\n",
        "- Output: a single linear neuron.\n",
        "\n",
        "- Target: $ y = 3 $, loss = **MSE**.\n",
        "\n",
        "### 8. Compute the gradient of the loss with respect to the input, $ \\frac{\\partial L}{\\partial x} $. Finally, report the sum of all elements of this gradient vector.\n",
        "\n",
        "Ans: 0.0003"
      ],
      "metadata": {
        "id": "1QeDFJLvFPnN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 9. Why is proper weight initialization critical for deep PyTorch models?\n",
        "\n",
        "- [x] A. Poor initialization may cause exploding/vanishing gradients.\n",
        "- [ ] B. Initialization only affects convergence speed, not final performance.\n",
        "- [x] C. Xavier/Glorot initialization stabilizes variance of activations across layers.\n",
        "- [ ] D. Random unscaled initialization is sufficient for deep networks."
      ],
      "metadata": {
        "id": "1NVo4MpVF8r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Which of the following directly mitigate vanishing gradients in deep PyTorch networks?\n",
        "\n",
        "- [ ] A. Replace ReLU with Sigmoid.\n",
        "- [x] B. Apply `torch.nn.BatchNorm`.\n",
        "- [x] C. Use Residual Connections (`torch.nn.Sequential`).\n",
        "- [x] D. Use initialization (`torch.nn.init.kaiming_normal_`)."
      ],
      "metadata": {
        "id": "2BNSIiamw5XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. Which of the following do not dynamically adjust the learning rate during PyTorch training?\n",
        "\n",
        "- [ ] A. `torch.optim.lr_scheduler.ReduceLROnPlateau`\n",
        "- [ ] B. `torch.optim.lr_scheduler.StepLR`\n",
        "- [ ] C. `torch.optim.lr_scheduler.ExponentialLR`\n",
        "- [x] D. Fixed `lr` in `torch.optim.Adam`"
      ],
      "metadata": {
        "id": "gM4wD5YQxUVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 12. What is the effect of different activations on optimization geometry?\n",
        "\n",
        "- [x] A. ReLU partitions space into piecewise linear regions, simplifying optimization.\n",
        "- [x] B. Sigmoid compresses outputs into [0,1], often leading to flat gradients.\n",
        "- [x] C. Tanh centers outputs at 0, often helping convergence vs sigmoid.\n",
        "- [ ] D. Softmax in hidden layers improves optimization geometry."
      ],
      "metadata": {
        "id": "Do3OewEpxuGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 13. Two identical PyTorch models are trained, but one uses `torch.nn.BatchNorm1d`. Which is true?\n",
        "\n",
        "- [x] A. BatchNorm reduces internal covariate shift.\n",
        "- [ ] B. BatchNorm eliminates the need for activations.\n",
        "- [x] C. BatchNorm allows training with higher learning rates.\n",
        "- [ ] D. BatchNorm guarantees higher test accuracy."
      ],
      "metadata": {
        "id": "lSJtWScByM30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 14. During training, you notice oscillations in loss values across epochs. Which remedies are valid?\n",
        "\n",
        "- [x] A. Reduce learning rate in the optimizer.\n",
        "- [x] B. Switch from SGD to Adam.\n",
        "- [x] C. Add momentum to SGD.\n",
        "- [ ] D. Remove non-linear activations to simplify optimization."
      ],
      "metadata": {
        "id": "eWWRAGqiyguV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "You train a **3-layer MLP** in PyTorch:\n",
        "\n",
        "- Input dimension = 2\n",
        "- Hidden Layer 1: 2 neurons, activation = ReLU\n",
        "- Hidden Layer 2: 2 neurons, activation = ReLU\n",
        "- Output: 1 neuron, activation = Sigmoid\n",
        "\n",
        "Weights/biases:\n",
        "\n",
        "$$\n",
        "W^{[1]} = \\begin{bmatrix} 1 & -1 \\\\ 2 & 0 \\end{bmatrix}, \\quad b^{[1]} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{[2]} = \\begin{bmatrix} 1 & 0 \\\\ -1 & 2 \\end{bmatrix}, \\quad b^{[2]} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "W^{[3]} = \\begin{bmatrix} 1 & -2 \\end{bmatrix}, \\quad b^{[3]} = [0]\n",
        "$$\n",
        "\n",
        "Input:  \n",
        "$$\n",
        "x = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}, \\quad \\text{Target: } y = 0\n",
        "$$\n",
        "\n",
        "### 15. Compute the final output prediction $\\hat{y}$ of the network after the forward pass.\n",
        "\n",
        "Ans: 0.182"
      ],
      "metadata": {
        "id": "g3-kcnI_y0Yt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding questions (Total: 6)"
      ],
      "metadata": {
        "id": "Ce5Pqo_sMWCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X = torch.tensor([[0.,0.],[0.,1.],[1.,0.],[1.,1.]])\n",
        "y = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2,4),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(4,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.layers(x)\n",
        "\n",
        "model = MLP()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = criterion(output,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n",
        "```\n",
        "\n",
        "### 1. Which of the following are true?\n",
        "\n",
        "- [ ] A. Without non-linear activation, this model can solve XOR.  \n",
        "- [x] B. If we replace Tanh with ReLU, convergence might be faster.\n",
        "- [x] C. If we omit `optimizer.zero_grad()`, gradients accumulate and weights diverge.\n",
        "- [ ] D. Sigmoid in the final layer is necessary for mean squared error loss calculation."
      ],
      "metadata": {
        "id": "SgGDJTSBMb54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(42)\n",
        "\n",
        "X = torch.tensor([[1.,-1.]])\n",
        "y = torch.tensor([[1.]])\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for p in model.parameters():\n",
        "        p.fill_(1.0)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "output = model(X)\n",
        "loss = criterion(output,y)\n",
        "print(output.item(), loss.item())\n",
        "```\n",
        "\n",
        "### 2. What is the loss value printed? (Round to 3 decimal places)\n",
        "\n",
        "Ans: 0.313"
      ],
      "metadata": {
        "id": "Ol59Tf45Mb56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "X = torch.randn(5,10)\n",
        "y = torch.randint(0,3,(5,))\n",
        "\n",
        "model = nn.Linear(10,3)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(50):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X)\n",
        "    loss = F.cross_entropy(logits,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "```\n",
        "\n",
        "### 3. Which of the following statement/s is incorrect?\n",
        "\n",
        "- [ ] A. `cross_entropy` internally applies softmax to logits.\n",
        "- [ ] B. Changing labels `y` to one-hot encoding would break this code.\n",
        "- [ ] C. The final linear layer must have 3 outputs for this setup.\n",
        "- [x] D. Replacing Adam with SGD(lr=0.01) would guarantee identical convergence."
      ],
      "metadata": {
        "id": "q2Xc6yQHMb57"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(0)\n",
        "\n",
        "X = torch.tensor([[2., -3.]])\n",
        "layer = nn.Linear(2,1)\n",
        "with torch.no_grad():\n",
        "    layer.weight[:] = torch.tensor([[1.,-2.]])\n",
        "    layer.bias[:] = torch.tensor([0.5])\n",
        "\n",
        "output = layer(X)\n",
        "print(output.item())\n",
        "```\n",
        "\n",
        "### 4. What is the numerical output value printed?\n",
        "\n",
        "Ans: 8.5"
      ],
      "metadata": {
        "id": "yZvm-HSl2Ggs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "for epoch in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(X)\n",
        "    loss = F.cross_entropy(logits,y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "```\n",
        "\n",
        "### 5. Which statements are true?\n",
        "\n",
        "- [x] A. Learning rate decreases every 10 epochs by a factor of 0.1.\n",
        "- [ ] B. If base LR = 0.01, then LR at epoch 21 is 0.0001.\n",
        "- [ ] C. Scheduler must be stepped before `optimizer.step()` to function correctly.\n",
        "- [x] D. This helps avoid getting stuck in sharp local minima."
      ],
      "metadata": {
        "id": "R8GOC-HV2ig1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "torch.manual_seed(7)\n",
        "\n",
        "X = torch.tensor([[1.,2.,3.]])\n",
        "y = torch.tensor([[1.]])\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(3,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "output = model(X)\n",
        "loss = criterion(output,y)\n",
        "loss.backward()\n",
        "\n",
        "grads = []\n",
        "for p in model.parameters():\n",
        "    grads.append(p.grad.sum().item())\n",
        "\n",
        "print(sum(grads))\n",
        "```\n",
        "\n",
        "### 6. What is the sum of all gradient elements printed? (Round to 3 decimal places)\n",
        "\n",
        "Ans: -0.578"
      ],
      "metadata": {
        "id": "xXG3hT74216F"
      }
    }
  ]
}