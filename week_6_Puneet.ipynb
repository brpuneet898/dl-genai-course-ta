{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. In a Vanilla GAN setup, if the discriminator becomes an optimal classifier too early in training, which of the following outcomes is most theoretically accurate in terms of the generator’s gradient behavior and adversarial learning objective?\n",
        "\n",
        "- [ ] A. The generator receives stronger gradients that accelerate convergence due to discriminator confidence\n",
        "- [x] B. The generator receives vanishing gradients, causing training stagnation even if its outputs are random\n",
        "- [ ] C. The discriminator’s optimality guarantees Nash equilibrium is reached earlier\n",
        "- [ ] D. The generator's updates become similar to maximum likelihood estimation\n",
        "\n",
        "Ans: When the discriminator becomes optimal too early, it assigns values near 1 for real and 0 for fake, causing the gradient from the generator’s loss to vanish. As a result, the generator cannot learn."
      ],
      "metadata": {
        "id": "_UMNhxNvXXOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 2. Consider mode collapse in GANs. Which mechanisms directly contribute to its emergence at an optimization level?\n",
        "\n",
        "- [x] A. Generator exploiting local minima where discriminator response is predictable\n",
        "- [x] B. Discriminator loss saturating due to Jensen-Shannon divergence properties\n",
        "- [ ] C. Incorrect initialization of batch normalization parameters only\n",
        "- [ ] D. Generator learning effectively but discriminator underfitting\n",
        "\n",
        "Ans: Mode collapse occurs due to generator exploiting local predictable responses from the discriminator. The JS divergence saturates when distributions don't overlap, leading to vanishing gradients."
      ],
      "metadata": {
        "id": "QKYMqiroXlBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 3. In the minimax GAN objective using Jensen-Shannon divergence, why is the divergence theoretically unsuitable for disjoint real/generated distributions during early training?\n",
        "\n",
        "- [ ] A. JS divergence becomes negative\n",
        "- [x] B. JS divergence saturates at log(2), resulting in zero gradients\n",
        "- [ ] C. It causes the discriminator to output uniform probabilities\n",
        "- [ ] D. It forces generator to switch to KL divergence automatically\n",
        "\n",
        "Ans: JS divergence becomes constant (log(2)) when distributions are disjoint, resulting in zero gradients, preventing generator learning."
      ],
      "metadata": {
        "id": "3jijDdHsX373"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 4. If the discriminator perfectly distinguishes real and fake samples such that $ D(x)=1 $ and $ D(G(z))=0 $, what numerical value does the generator’s gradient term in the original minimax formulation approach (provide the numerical limit)?\n",
        "\n",
        "Ans: 0.\n",
        "\n",
        "If the discriminator is perfect, the generator's loss term $\n",
        "log(1−D(G(z))) $ saturates, making the gradient approach zero → no learning signal."
      ],
      "metadata": {
        "id": "jUl8HnKJYGj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. Which of the following architectural or training strategies fundamentally alter the divergence measure implicitly optimized by GANs?\n",
        "\n",
        "- [x] A. WGAN replacing sigmoid activation with linear output\n",
        "- [x] B. Using gradient penalty in WGAN-GP\n",
        "- [x] C. Feature matching in the discriminator loss\n",
        "- [ ] D. Increasing batch size during training\n",
        "\n",
        "Ans: A: Linear activation changes the divergence from JS to Wasserstein. B: Gradient penalty enforces Lipschitz constraint, modifying distance measure. C: Feature matching induces generator to optimize toward feature statistics rather than solely fooling discriminator. D has no fundamental effect on divergence."
      ],
      "metadata": {
        "id": "SxIl4j_OYbBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. What is the theoretical reason that Wasserstein distance enables meaningful gradients even when generator and real distributions do not overlap?\n",
        "\n",
        "- [ ] A. It introduces a logarithmic divergence behavior\n",
        "- [ ] B. It computes distance over the support boundary only\n",
        "- [x] C. It measures the cost of optimal transport between distributions\n",
        "- [ ] D. It forces discriminator to approximate entropy\n",
        "\n",
        "Ans: Wasserstein distance measures the minimum cost to transform generated distribution into real distribution, giving meaningful gradients even without distributional overlap."
      ],
      "metadata": {
        "id": "15eLZ42oYtJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 7. Which of the following indicate a failure of the GAN game to converge to a Nash equilibrium, based on training signal behavior?\n",
        "\n",
        "- [x] A. Oscillations in generator loss with no trend\n",
        "- [x] B. Discriminator loss converging to zero permanently\n",
        "- [ ] C. Generator loss reaching negative infinity\n",
        "- [ ] D. Constant discriminator accuracy near 50%\n",
        "\n",
        "Ans: A: Oscillations imply dynamic instability and lack of equilibrium. B: If discriminator loss stays at zero, it overpowers the generator, halting progress. C is theoretically impossible, and D indicates convergence to equilibrium rather than failure."
      ],
      "metadata": {
        "id": "8Vrw7-wGY8Rh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 8. Why is Lipschitz continuity critical in WGAN theory?\n",
        "\n",
        "- [ ] A. It bounds the discriminator weights to prevent saturation\n",
        "- [x] B. It ensures the discriminator approximates a K-Lipschitz function for valid Wasserstein estimation\n",
        "- [ ] C. It increases variance in gradient flow\n",
        "- [ ] D. It allows generator to minimize KL divergence directly\n",
        "\n",
        "Ans: WGAN critic must be 1-Lipschitz to ensure valid Wasserstein distance estimation. This theoretical constraint is essential for stable training."
      ],
      "metadata": {
        "id": "tUiLCKVBZMAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. Batch normalization in GAN generators primarily helps in:\n",
        "\n",
        "- [x] A. Stabilizing gradient magnitudes across layers\n",
        "- [ ] B. Preventing discriminator from overpowering the generator\n",
        "- [ ] C. Changing divergence measure\n",
        "- [ ] D. Reducing Lipschitz constant directly\n",
        "\n",
        "Ans: Batch normalization reduces internal covariate shift, stabilizing training and improving gradient flow in the generator."
      ],
      "metadata": {
        "id": "10dS6mrLZZQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Which regularization techniques help mitigate discriminator overfitting in GANs?\n",
        "\n",
        "- [x] A. Label smoothing\n",
        "- [x] B. Dropout in discriminator\n",
        "- [ ] C. Increasing learning rate exponentially\n",
        "- [x] D. Spectral normalization\n",
        "\n",
        "Ans: Label smoothing reduces discriminator overconfidence. Dropout prevents overfitting. Spectral normalization constrains the Lipschitz constant to maintain stability. Option C destabilizes training."
      ],
      "metadata": {
        "id": "G83ojcupZm-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. What is the core principle behind GANs?\n",
        "\n",
        "- [ ] A. Generator tries to maximize classification accuracy\n",
        "- [x] B. Generator and discriminator play a two-player minimax game\n",
        "- [ ] C. Discriminator generates samples while generator evaluates them\n",
        "- [ ] D. Generator minimizes logistic regression loss directly\n",
        "\n",
        "Ans: GANs are adversarial models where generator and discriminator optimize opposing objectives in a minimax framework."
      ],
      "metadata": {
        "id": "QBBc-JLlZ3oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 12. In a GAN context, if the discriminator outputs 0.5 consistently for both real and fake samples, what is the ideal theoretical interpretation of this output (numerical answer for the discriminator’s confidence level)?\n",
        "\n",
        "Ans: 0.5\n",
        "\n",
        "A discriminator output of 0.5 indicates maximum uncertainty, meaning it cannot distinguish real from fake — the ideal point of equilibrium."
      ],
      "metadata": {
        "id": "G6qv_dZaaIeX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding questions (Total: 12)"
      ],
      "metadata": {
        "id": "Ce5Pqo_sMWCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "netD.train(); netG.train()\n",
        "\n",
        "optimG.zero_grad(set_to_none=True)\n",
        "z = torch.randn(B, 100, device=device)\n",
        "fake = netG(z)\n",
        "pred_fake = netD(fake)\n",
        "g_loss = criterion(pred_fake, torch.ones_like(pred_fake))\n",
        "g_loss.backward()\n",
        "optimG.step()\n",
        "\n",
        "optimD.zero_grad(set_to_none=True)\n",
        "pred_real = netD(x)\n",
        "loss_real = criterion(pred_real, torch.ones_like(pred_real))\n",
        "pred_fake_detached = netD(fake.detach())\n",
        "loss_fake = criterion(pred_fake_detached, torch.zeros_like(pred_fake_detached))\n",
        "d_loss = (loss_real + loss_fake) * 0.5\n",
        "d_loss.backward()\n",
        "optimD.step()\n",
        "```\n",
        "\n",
        "### 1. Which lines/practices are critical to avoid incorrect gradient flow or graph reuse problems in alternating updates?\n",
        "\n",
        "- [x] A. Using `fake.detach()` before the discriminator update\n",
        "- [x] B. Calling `optimG.zero_grad(set_to_none=True)` and `optimD.zero_grad(set_to_none=True)` separately\n",
        "- [x] C. Computing `pred_fake = netD(fake)` again for D instead of reusing `pred_fake` from G step\n",
        "- [x] D. Calling `netD.train(); netG.train()` at the start of the step\n",
        "\n",
        "Ans: A breaks the G graph for D’s step. B prevents gradient accumulation bleed. C avoids backprop through the same graph twice. D ensures layers like Dropout/BN are in train mode for correct stats."
      ],
      "metadata": {
        "id": "NFsiNKcucSI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "for _ in range(5):\n",
        "    netD.train(); netG.train()\n",
        "    for p in netD.parameters(): p.requires_grad = True\n",
        "\n",
        "    optimD.zero_grad(set_to_none=True)\n",
        "    z = torch.randn(B, 128, device=device)\n",
        "    fake = netG(z).detach()\n",
        "    d_real = netD(x)\n",
        "    d_fake = netD(fake)\n",
        "\n",
        "    eps = torch.rand(B, 1, 1, 1, device=device)\n",
        "    x_hat = eps * x + (1 - eps) * fake\n",
        "    x_hat.requires_grad_(True)\n",
        "    d_hat = netD(x_hat)\n",
        "    grad = torch.autograd.grad(\n",
        "        outputs=d_hat.sum(), inputs=x_hat, create_graph=True\n",
        "    )[0].view(B, -1)\n",
        "    gp = ((grad.norm(2, dim=1) - 1.0) ** 2).mean()\n",
        "\n",
        "    d_loss = (d_fake.mean() - d_real.mean()) + 10.0 * gp\n",
        "    d_loss.backward()\n",
        "    optimD.step()\n",
        "\n",
        "for p in netD.parameters(): p.requires_grad = False\n",
        "optimG.zero_grad(set_to_none=True)\n",
        "z = torch.randn(B, 128, device=device)\n",
        "g_loss = -netD(netG(z)).mean()\n",
        "g_loss.backward()\n",
        "optimG.step()\n",
        "```\n",
        "\n",
        "### 2. What is the primary reason we set `requires_grad=False` on the critic parameters before the G step?\n",
        "\n",
        "- [ ] A. Reduces memory overhead only\n",
        "- [x] B. Prevents accidental gradient flow into critic during G update\n",
        "- [ ] C. Avoids weight clipping\n",
        "- [ ] D. Enables batch norm statistics to freeze\n",
        "\n",
        "Ans: Freezes critic so only G receives gradients when optimizing `g_loss`."
      ],
      "metadata": {
        "id": "PTlaDhcWcu7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "for p in netD1.parameters():\n",
        "    p.data.clamp_(-0.01, 0.01)\n",
        "\n",
        "from torch.nn.utils import spectral_norm\n",
        "netD2.conv1 = spectral_norm(netD2.conv1)\n",
        "netD2.conv2 = spectral_norm(netD2.conv2)\n",
        "netD2.fc = spectral_norm(netD2.fc)\n",
        "```\n",
        "\n",
        "### 3. Which statements about training behavior are correct?\n",
        "\n",
        "- [x] A. Spectral norm enforces a tighter Lipschitz control than naive clipping in practice\n",
        "- [x] B. Weight clipping can reduce critic capacity and lead to underfitting\n",
        "- [x] C. Spectral norm typically stabilizes gradients for both WGAN and non-WGAN losses\n",
        "- [ ] D. Spectral norm guarantees convergence for any generator\n",
        "\n",
        "Ans: SN provides smoother Lipschitz control and stabilizes training; clipping is crude and harms capacity. No method guarantees convergence universally."
      ],
      "metadata": {
        "id": "mP1eSAStdEEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "for it in range(T):\n",
        "    optimD.zero_grad(set_to_none=True)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        d_real = netD(x)\n",
        "        d_fake = netD(netG(z).detach())\n",
        "        d_loss = (d_fake.mean() - d_real.mean())\n",
        "\n",
        "    scaler.scale(d_loss).backward()\n",
        "    scaler.step(optimD)\n",
        "\n",
        "    optimG.zero_grad(set_to_none=True)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        g_loss = -netD(netG(z)).mean()\n",
        "    scaler.scale(g_loss).backward()\n",
        "    scaler.step(optimG)\n",
        "    scaler.update()\n",
        "```\n",
        "\n",
        "### 4. After many iterations, you observe unstable/plateaued critic updates due to a repeated AMP bug. What single integer count of missing calls per iteration causes this?\n",
        "\n",
        "Ans: 1\n",
        "\n",
        "One missing `scaler.update()` after the D step prevents proper AMP scaling state progression before the G step."
      ],
      "metadata": {
        "id": "2OuScI4fdWDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "# EMA - Exponential Moving Average\n",
        "ema_decay = 0.999\n",
        "for it in range(T):\n",
        "    with torch.no_grad():\n",
        "        for p, p_ema in zip(netG.parameters(), netG_EMA.parameters()):\n",
        "            p_ema.copy_(p_ema * ema_decay + (1 - ema_decay) * p)\n",
        "```\n",
        "\n",
        "### 5. Why is evaluating FID/IS with `netG_EMA` often preferred?\n",
        "\n",
        "- [x] A. EMA reduces instantaneous noise in weights, giving more stable sample quality\n",
        "- [ ] B. EMA changes the loss to Wasserstein distance\n",
        "- [ ] C. EMA regularizes the discriminator\n",
        "- [ ] D. EMA guarantees lower FID\n",
        "\n",
        "Ans: EMA smooths parameter trajectories, yielding stabler generations; no guarantees on metrics.\n"
      ],
      "metadata": {
        "id": "jX09s7t6dqkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "# This is happening after each epoch.\n",
        "netG.eval()\n",
        "z_fixed = torch.randn(64, 100, device=device)\n",
        "with torch.no_grad():\n",
        "    samples = netG(z_fixed)\n",
        "\n",
        "# Training resumes:\n",
        "netG.train()\n",
        "```\n",
        "\n",
        "### 6. Pick all correct statements.\n",
        "\n",
        "- [x] A. Using `.eval()` avoids updating BatchNorm running stats during sampling\n",
        "- [x] B. `torch.no_grad()` prevents building graphs and saves memory during sampling\n",
        "- [x] C. Omitting `.eval()` risks sampling with training-time BN/Dropout behavior\n",
        "- [ ] D. Sampling without `no_grad()` can incorrectly update EMA\n",
        "\n",
        "Ans: `.eval()` and `no_grad()` are both needed for correct, efficient evaluation. EMA isn’t updated here (we didn’t change EMA logic)."
      ],
      "metadata": {
        "id": "sQ6wqm5sejvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "logits_real, feat_real = netD(x, return_features=True)\n",
        "logits_fake, feat_fake = netD(netG(z), return_features=True)\n",
        "\n",
        "# Standard adversarial portion\n",
        "g_adv = F.binary_cross_entropy_with_logits(logits_fake, torch.ones_like(logits_fake))\n",
        "\n",
        "# Feature matching\n",
        "g_fm = F.l1_loss(feat_fake.mean(dim=0), feat_real.mean(dim=0))\n",
        "\n",
        "g_total = g_adv + 10.0 * g_fm\n",
        "g_total.backward()\n",
        "optimG.step()\n",
        "```\n",
        "\n",
        "### 7. What’s the main purpose of adding `g_fm` here?\n",
        "\n",
        "- [ ] A. To change divergence from JS to KL\n",
        "- [x] B. To discourage mode collapse by matching higher-level statistics\n",
        "- [ ] C. To regularize the discriminator’s weights\n",
        "- [ ] D. To enforce Lipschitz constraint\n",
        "\n",
        "Ans: Feature matching shapes generator outputs toward real feature statistics, reducing collapse."
      ],
      "metadata": {
        "id": "NU5GPV_JfSN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "def seed_all(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_all(123)\n",
        "\n",
        "loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=64, shuffle=True,\n",
        "    num_workers=4, pin_memory=True, drop_last=True,\n",
        "    worker_init_fn=lambda wid: np.random.seed(123 + wid)\n",
        ")\n",
        "```\n",
        "\n",
        "### 8. Which choices best ensure run-to-run determinism given this setup?\n",
        "\n",
        "- [x] A. Keep `benchmark=False` and `deterministic=True` for cuDNN\n",
        "- [x] B. Fix `worker_init_fn` seeds per worker\n",
        "- [ ] C. Set `shuffle=False`\n",
        "- [x] D. Use `torch.use_deterministic_algorithms(True)` when available\n",
        "\n",
        "Ans: Deterministic cuDNN, deterministic algos, and seeded workers help reproducibility. `shuffle=False` is unnecessary (and often undesirable) for training."
      ],
      "metadata": {
        "id": "NAn1zcv8flJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "real_labels = torch.empty_like(pred_real).uniform_(0.8, 1.0)\n",
        "fake_labels = torch.zeros_like(pred_fake)\n",
        "\n",
        "loss_real = F.binary_cross_entropy_with_logits(pred_real, real_labels)\n",
        "loss_fake = F.binary_cross_entropy_with_logits(pred_fake, fake_labels)\n",
        "d_loss = 0.5 * (loss_real + loss_fake)\n",
        "```\n",
        "\n",
        "### 9. What is the expected training effect of one-sided smoothing for real labels?\n",
        "\n",
        "- [x] A. Reduces D overconfidence, providing better gradients for G\n",
        "- [ ] B. Forces D to underfit by construction\n",
        "- [ ] C. Changes the GAN objective to WGAN\n",
        "- [ ] D. Eliminates mode collapse\n",
        "\n",
        "Ans: Softer targets regularize D and prevent saturated, unhelpful gradients."
      ],
      "metadata": {
        "id": "cOc_6ybqgKiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "# Consider the following two options to clear the grads -\n",
        "optimD.zero_grad()                   # (1)\n",
        "# or\n",
        "optimD.zero_grad(set_to_none=True)   # (2)\n",
        "```\n",
        "\n",
        "### 10. Which statements are true?\n",
        "\n",
        "- [x] A. (2) can reduce memory writes and may be faster\n",
        "- [x] B. (2) sets grads to None, which can be treated differently by some optimizers\n",
        "- [ ] C. (1) and (2) are always identical in performance\n",
        "- [x] D. (2) is commonly recommended for large models\n",
        "\n",
        "Ans: `set_to_none=True` avoids explicit zeroing of tensors, often improving performance/memory."
      ],
      "metadata": {
        "id": "-R3Aw4JqgZvg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "netG.to(device); netD.to(device)\n",
        "z = torch.randn(64, 100)         \n",
        "x = x.to(device)\n",
        "\n",
        "fake = netG(z)    \n",
        "```\n",
        "\n",
        "### 11. What is the bug and fix?\n",
        "\n",
        "- [x] A. `z` is on CPU; move it to device → `z = torch.randn(64, 100, device=device)`\n",
        "- [ ] B. `x` must be on CPU; move it back\n",
        "- [ ] C. `netG` must be on CPU\n",
        "- [ ] D. Use `float16` by default\n",
        "\n",
        "Ans: Inputs must be on the same device as the model."
      ],
      "metadata": {
        "id": "s6OcT_PegyhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(32, 100, device=device)\n",
        "    fake = netG(z)\n",
        "# later we compute a metric requiring CPU numpy\n",
        "m = fake.mean().item()\n",
        "```\n",
        "\n",
        "### 12. What is the scalar dtype of `m` returned by `.item()` (write the Python type name)?\n",
        "\n",
        "Ans: float\n",
        "\n",
        "`.item()` converts single-element tensor to a native Python `float`."
      ],
      "metadata": {
        "id": "jTWRwMqthhNn"
      }
    }
  ]
}