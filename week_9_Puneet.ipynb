{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5ggp7DNnMOLn",
        "Ce5Pqo_sMWCd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Theory Questions (Total: 12)"
      ],
      "metadata": {
        "id": "5ggp7DNnMOLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 1. In static word embeddings (e.g., Word2Vec or GloVe), consider the phrase \"bank deposit near the river bank.\" Which of the following statements best explains the inadequacy of static embeddings in this scenario?\n",
        "\n",
        "- [x] A. Static embeddings treat both meanings of \"bank\" as identical, failing to account for context.\n",
        "- [ ] B. Static embeddings assign different vectors for identical words in different sentences.\n",
        "- [ ] C. Static embeddings use randomly assigned vectors for each occurrence of a word.\n",
        "- [ ] D. Static embeddings rely solely on syntactic information when representing word meaning.\n",
        "\n",
        "Ans: Static (non-contextual) embeddings map each unique word to a single fixed vector, regardless of the sentence context. This means both senses of \"bank\" (financial and river side) share the same vector, so models can't distinguish their meanings within different contexts."
      ],
      "metadata": {
        "id": "3Rktq7lw3OF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 2. A neural network has been trained to learn embeddings for a medical vocabulary. What would you observe in the vector space, if the training data frequently contained \"hypertension\" and \"high blood pressure\" together?\n",
        "\n",
        "- [ ] A. The vectors representing \"hypertension\" and \"high blood pressure\" will diverge significantly.\n",
        "- [x] B. The vectors will become closely clustered, capturing semantic similarity.\n",
        "- [ ] C. The vectors will collapse to a zero vector.\n",
        "- [ ] D. The learning process will fail for infrequent words.\n",
        "\n",
        "Ans: Word embeddings capture semantic relationships based on co-occurrence patterns in data. If \"hypertension\" and \"high blood pressure\" often appear together, their vectors become similar, reflecting their related meanings."
      ],
      "metadata": {
        "id": "W0T4yJo03dmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 3. Which of the following are true about bottlenecks in RNN in the encoder-decoder architecture for machine translation?\n",
        "\n",
        "- [x] A. Information from early tokens can be diluted or lost in the fixed-size context vector.\n",
        "- [ ] B. The decoder generates every output token using attention over all encoder outputs.\n",
        "- [x] C. Vanishing gradients may limit long-term information retention.\n",
        "- [x] D. The final hidden state of the encoder summarizes the whole input sequence.\n",
        "\n",
        "Ans: Early token information can be diluted or lost in the fixed-size context vector, which is a core limitation. RNNs are susceptible to vanishing gradients, making it harder to capture long-range dependencies. The encoder's final hidden state is a compressed summary of the whole sequence, causing information loss."
      ],
      "metadata": {
        "id": "dPqaL-Yw3tBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 4. Consider the chain rule in backpropagation over the unrolled computation graph of a simple RNN. Which mathematical problem primarily arises as the sequence length increases there?\n",
        "\n",
        "- [ ] A. Numerical instability in softmax computation for large vocabularies.\n",
        "- [x] B. Gradients either vanish or explode due to repeated multiplications.\n",
        "- [ ] C. Convergence of embeddings to trivial solutions.\n",
        "- [ ] D. Redundant representations of context in each hidden state.\n",
        "\n",
        "Ans: The chain rule applied through many time steps can cause gradients to shrink (vanish) or grow (explode) exponentially, leading to unstable or ineffective training in long sequences."
      ],
      "metadata": {
        "id": "A8eZB5Qo39n-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 5. In the word embeddings, which observation can typically be captured through vector arithmetic in the embedding space?\n",
        "\n",
        "- [x] A. Gender relationships (e.g., King - Man + Woman â‰ˆ Queen)\n",
        "- [x] B. Syntactic roles (e.g., verb vs. noun)\n",
        "- [ ] C. Context-specific meaning in static embeddings\n",
        "- [x] D. Analogical relationships between pairs of words\n",
        "\n",
        "Ans: Gender relationships can often be represented as consistent vector offsets. Some syntactic roles also show structure in embedding spaces. Analogical relationships (like \"Paris is to France as Tokyo is to Japan\") are modeled via vector arithmetic. C is incorrect because static embeddings do not capture context-specific meanings."
      ],
      "metadata": {
        "id": "liyZ3U0e4Nxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 6. Suppose you have a DL model trained to generate word embeddings solely using co-occurrence frequencies in text corpora. Which of the following limitations might it exhibit for rare words?\n",
        "\n",
        "- [ ] A. Rare words will have highly robust and distinct representations.\n",
        "- [x] B. Embeddings for rare words may be poorly estimated, limiting downstream performance.\n",
        "- [ ] C. Rare words are ignored completely in static models.\n",
        "- [ ] D. Rare words always receive identical vectors.\n",
        "\n",
        "Ans: Rare words have fewer examples, so their vectors are less reliable, harming performance in downstream tasks."
      ],
      "metadata": {
        "id": "9H9AXvp_4eJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 7. When training an RNN on long sequences for language modeling, what architectural modification specifically addresses the problem of long-term dependency retention?\n",
        "\n",
        "- [ ] A. Transitioning to feed-forward networks.\n",
        "- [x] B. Incorporating LSTM or GRU cells for improved memory persistence.\n",
        "- [ ] C. Increasing the vocabulary size.\n",
        "- [ ] D. Using static word embeddings.\n",
        "\n",
        "Ans: LSTM and GRU units have gating mechanisms designed to retain information over long sequences, directly addressing the limitations of vanilla RNNs in modeling long-term dependencies."
      ],
      "metadata": {
        "id": "_htkJ40m4sGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 8. Regarding the computation of contextualized embeddings, which statements are accurate?\n",
        "\n",
        "- [x] A. The representation of a word depends on its surrounding words.\n",
        "- [x] B. The embedding of \"bank\" in \"river bank\" differs from \"bank account.\"\n",
        "- [ ] C. Contextualized embeddings are always static after training.\n",
        "- [x] D. Models like BERT generate embeddings by considering the whole sentence.\n",
        "\n",
        "Ans: Contextualized embeddings depend on surrounding words (this phenomenon is referred to as context awareness). The vector for \"bank\" adapts in \"river bank\" vs. \"bank account\". Models such as BERT dynamically generate embeddings based on sentence context."
      ],
      "metadata": {
        "id": "PTAt7X7jNzIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 9. Suppose you are using an encoder-decoder RNN model for translating paragraphs instead of sentences. Which of the following factors would most affect translation quality of long inputs?\n",
        "\n",
        "- [ ] A. Fixed vocabulary during tokenization\n",
        "- [x] B. Compression of the entire input in the final encoder state\n",
        "- [ ] C. Masking padded tokens during training\n",
        "- [ ] D. Increasing batch size without changing model depth\n",
        "\n",
        "Ans: The limitation arises because the encoder squashes all input information into a single fixed vector, leading to significant loss of details, especially for long paragraphs."
      ],
      "metadata": {
        "id": "k35BOuooOB9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "### 10. Which mechanisms help alleviate vanishing gradient problems in sequence models?\n",
        "\n",
        "- [x] A. Use of LSTM cells\n",
        "- [x] B. Gradient clipping during optimizer updates\n",
        "- [ ] C. Increasing the depth of vanilla RNN layers\n",
        "- [x] D. Incorporation of gating functions in recurrent units\n",
        "\n",
        "Ans: LSTM cells have internal memory and gates to preserve gradients. Gradient clipping directly bounds gradients. Gating functions (as in GRUs or LSTMs) help mitigate vanishing gradients."
      ],
      "metadata": {
        "id": "0InFJ738OL3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. While generating embeddings from scratch, what is the typical first step before meaningful learning occurs?\n",
        "\n",
        "- [ ] A. Assigning fixed vectors based on human intuition\n",
        "- [x] B. Initializing vectors randomly and updating during network training\n",
        "- [ ] C. Averaging co-occurrence counts\n",
        "- [ ] D. Clustering words by frequency\n",
        "\n",
        "Ans: Embedding vectors are usually initialized randomly and then refined as part of the network's training process through backpropagation."
      ],
      "metadata": {
        "id": "WMb0LDT0OXsk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[NAT]\n",
        "\n",
        "### 12. If a neural network uses an attention mechanism with the softmax function to normalize attention scores across 10 input tokens, what is the sum of all attention weights produced for this input? [Enter a single numerical value]\n",
        "\n",
        "Ans: 1\n",
        "\n",
        "The softmax function outputs a probability distribution that sums to 1 over all inputs; thus, the sum of all attention weights for 10 tokens is 1."
      ],
      "metadata": {
        "id": "QF8Zzbr8Ojf5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Questions (Total: 12)"
      ],
      "metadata": {
        "id": "tZxXjvihqhkB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class CustomEmbedder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids)\n",
        "        return self.linear(x)\n",
        "emb = CustomEmbedder(1000, 128)\n",
        "input_ids = torch.randint(0, 1000, (32, 10))\n",
        "output = emb(input_ids)\n",
        "print(output.shape)\n",
        "```\n",
        "\n",
        "### 1. Which is the correct shape of `output`?\n",
        "\n",
        "- [ ] A. `(32, 128)`\n",
        "- [ ] B. `(128, 32)`\n",
        "- [x] C. `(32, 10, 128)`\n",
        "- [ ] D. `(32, 10)`\n",
        "\n",
        "Ans: The embedding looks up a 128-dimensional vector for each of the 10 tokens per sequence, for 32 sequences, so the output is `(batch, seq_len, dim)`."
      ],
      "metadata": {
        "id": "8BSwxzwqQlDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "batch = [{'text': [2, 5, 7, 8]}, {'text': [4, 2]}]\n",
        "padded = nn.utils.rnn.pad_sequence(\n",
        "    [torch.tensor(x['text']) for x in batch], padding_value=0)\n",
        "```\n",
        "\n",
        "### 2. Which statements are true after execution?\n",
        "\n",
        "- [x] A. The padded tensor's `shape[0]` equals the length of the longest sequence.\n",
        "- [ ] B. Padding is added at the start of sequences.\n",
        "- [x] C. By default, sequences are stacked along the first dimension.\n",
        "- [x] D. Padding value 0 is used for shorter sequences.\n",
        "\n",
        "Ans: Padding in PyTorch is added at the end, the tensor is stacked first along the max sequence dimension, and the chosen padding value is 0."
      ],
      "metadata": {
        "id": "H1LCt8qqRZK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(64, 64)\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "model = SimpleModel()\n",
        "model.apply(lambda m: nn.init.constant_(m.weight, 0.5) if hasattr(m, 'weight') else None)\n",
        "```\n",
        "\n",
        "### 3. How are the weights of model parameters after this initialization?\n",
        "\n",
        "- [x] A. All weights become exactly 0.5 where possible.\n",
        "- [ ] B. Only the last layer receives constant weights, others are untouched.\n",
        "- [ ] C. All weights are zeros.\n",
        "- [ ] D. Initialization fails if no 'weight' attribute exists.\n",
        "\n",
        "Ans: `apply` traverses all modules. When `weight` exists, it sets all entries to 0.5 using `nn.init.constant_`."
      ],
      "metadata": {
        "id": "FYZSki5vR-fL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "def my_collate(batch):\n",
        "    xs = [torch.tensor(item) for item in batch]\n",
        "    return torch.stack(xs, dim=1)\n",
        "loader = torch.utils.data.DataLoader([[1,2], [3,4]], batch_size=2, collate_fn=my_collate)\n",
        "for batch in loader:\n",
        "    print(batch.shape)\n",
        "```\n",
        "\n",
        "### 4. What will be printed?\n",
        "\n",
        "- [x] A. `torch.Size([2, 2])`\n",
        "- [ ] B. `torch.Size([2])`\n",
        "- [ ] C. `torch.Size([2, 2, 1])`\n",
        "- [ ] D. `torch.Size([2, 1])`\n",
        "\n",
        "Ans: Stacking 2 tensors of shape `(2,)` along `dim=1` gives `(2, 2)` shape."
      ],
      "metadata": {
        "id": "OBH7ZzMLSuxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
        "for epoch in range(5):\n",
        "    train_loss = ... # returns a float\n",
        "    scheduler.step(train_loss)\n",
        "```\n",
        "\n",
        "### 5. Which statements accurately describe what can occur during training?\n",
        "\n",
        "- [x] A. Learning rate reduces if `train_loss` plateaus for 2 epochs.\n",
        "- [x] B. The optimizer's learning rate may decrease multiple times.\n",
        "- [ ] C. The scheduler is unaffected by validation loss.\n",
        "- [x] D. If loss decreases every epoch, learning rate stays the same.\n",
        "\n",
        "Ans: The scheduler observes the provided metric, reduces LR if no improvement for 'patience' epochs, and does nothing if loss keeps improving."
      ],
      "metadata": {
        "id": "FJTQI1YxTj75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "rnn = nn.GRU(256, 512, bidirectional=True)\n",
        "x = torch.randn(15, 32, 256)\n",
        "out, hidden = rnn(x)\n",
        "```\n",
        "\n",
        "### 6. What is the shape of `hidden`?\n",
        "\n",
        "- [x] A. (2, 32, 512)\n",
        "- [ ] B. (4, 32, 512)\n",
        "- [ ] C. (6, 32, 512)\n",
        "- [ ] D. (8, 32, 512)\n",
        "\n",
        "Ans: For a single-layer, bidirectional GRU, the hidden state has shape (2, batch, hidden_size)."
      ],
      "metadata": {
        "id": "AuRlwhTYUJMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "def numericalize(tokens, vocab):\n",
        "    return [vocab.get(t, 0) for t in tokens]\n",
        "tokens = ['the', 'cat', 'sat']\n",
        "vocab = {'the':3, 'cat':6, 'dog':8}\n",
        "print(numericalize(tokens, vocab))\n",
        "```\n",
        "\n",
        "### 7. What will be printed?\n",
        "\n",
        "- [ ] A. `[3, 6, 8]`\n",
        "- [x] B. `[3, 6, 0]`\n",
        "- [ ] C. `[0, 0, 0]`\n",
        "- [ ] D. An error will be raised.\n",
        "\n",
        "Ans: 'sat' is not in vocab, so 0 is substituted. Others are mapped correctly to their values."
      ],
      "metadata": {
        "id": "MPDP5NcUUgQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MSQ]\n",
        "\n",
        "```python\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.GRU(hidden_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "    def forward(self, x, hidden):\n",
        "        out, hidden_new = self.rnn(x, hidden)\n",
        "        pred = self.fc(out)\n",
        "        return pred, hidden_new\n",
        "```\n",
        "\n",
        "### 8. Which statements are always true?\n",
        "\n",
        "- [x] A. Output shape of `pred` depends on input sequence length.\n",
        "- [x] B. Decoder's hidden state must be updated at every step.\n",
        "- [x] C. Output and hidden have same batch dimension.\n",
        "- [x] D. The forward pass can run for multiple time-steps.\n",
        "\n",
        "Ans: All statements are true for general RNNs handling sequence decoding."
      ],
      "metadata": {
        "id": "vXKFIhS2VWfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "output = model(src, trg, teacher_forcing_ratio)\n",
        "output_dim = output.shape[-1]\n",
        "preds = output[1:].view(-1, output_dim)\n",
        "print(preds.shape)\n",
        "```\n",
        "\n",
        "### 9. Which statement is true if `output` has shape `(max_len, batch, vocab_size)` and `max_len=20`, `batch=32`, `vocab_size=10000`?\n",
        "\n",
        "- [ ] A. (19, 32, 10000)\n",
        "- [x] B. (608, 10000)\n",
        "- [ ] C. (32, 10000)\n",
        "- [ ] D. (20, 32, 10000)\n",
        "\n",
        "Ans: Removing the first time-step gives shape (19, 32, 10000), then flattening over (19, 32) gives (608, 10000)."
      ],
      "metadata": {
        "id": "aeKJ3zf0VxKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "a = torch.softmax(torch.randn(32, 10), dim=1)\n",
        "b = torch.randn(32, 10, 64)\n",
        "weighted = torch.bmm(a.unsqueeze(1), b)\n",
        "print(weighted.shape)\n",
        "```\n",
        "\n",
        "### 10. What is the resulting shape of `weighted`?\n",
        "\n",
        "- [ ] A. (32, 10, 64)\n",
        "- [x] B. (32, 1, 64)\n",
        "- [ ] C. (1, 10, 64)\n",
        "- [ ] D. (32, 64)\n",
        "\n",
        "Ans: `torch.bmm` with (32,1,10) times (32,10,64) => (32,1,64) (batch matrix multiplication)."
      ],
      "metadata": {
        "id": "XiUEe46fWPej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "### 11. A PyTorch DataLoader uses a batch size of 64 and a dataset of 1000 examples. With `drop_last=True`, how many batches are created?\n",
        "\n",
        "- [x] A. 15\n",
        "- [ ] B. 16\n",
        "- [ ] C. 14\n",
        "- [ ] D. 10\n",
        "\n",
        "Ans: 1000 // 64 = 15 full batches; remainder is dropped."
      ],
      "metadata": {
        "id": "V34w7uC4Wqt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[MCQ]\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class SelfAttentionLayer(nn.Module):\n",
        "    def __init__(self, feature_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(feature_size, feature_size)\n",
        "        self.query = nn.Linear(feature_size, feature_size)\n",
        "        self.value = nn.Linear(feature_size, feature_size)\n",
        "    def forward(self, x, mask=None):\n",
        "        keys = self.key(x)\n",
        "        queries = self.query(x)\n",
        "        values = self.value(x)\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.key.out_features, dtype=torch.float32))\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attention_weights, values)\n",
        "        return output, attention_weights\n",
        "sa = SelfAttentionLayer(64)\n",
        "x = torch.randn(8, 20, 64)  \n",
        "out, attention_weights = sa(x)\n",
        "print(out.shape, attention_weights.shape)\n",
        "```\n",
        "\n",
        "### 12. What are the shapes of `out` and `attention_weights`?\n",
        "\n",
        "- [x] A. (8, 20, 64), (8, 20, 20)\n",
        "- [ ] B. (8, 64), (8, 20, 64)\n",
        "- [ ] C. (8, 20, 64), (8, 64, 20)\n",
        "- [ ] D. (8, 20, 20), (8, 20, 64)\n",
        "\n",
        "Ans: The output retains (batch, seq, feature), attention weights (batch, seq, seq) indicating attention scores for every sequence position."
      ],
      "metadata": {
        "id": "wQ53m1iyXkcC"
      }
    }
  ]
}